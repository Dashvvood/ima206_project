{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b7bf59",
   "metadata": {},
   "source": [
    "# 25 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676fb24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Epoch 0, Batch 0, Loss: 2.318887233734131\n",
      "Epoch 0, Batch 10, Loss: 2.0775582790374756\n",
      "Epoch 0, Train Loss: 2.17932923634847, Validation Accuracy: 14.8140743702519%\n",
      "Epoch 1, Batch 0, Loss: 2.0209834575653076\n",
      "Epoch 1, Batch 10, Loss: 1.9749492406845093\n",
      "Epoch 1, Train Loss: 2.0538632313410443, Validation Accuracy: 23.970411835265892%\n",
      "Epoch 2, Batch 0, Loss: 2.0560190677642822\n",
      "Epoch 2, Batch 10, Loss: 1.8476451635360718\n",
      "Epoch 2, Train Loss: 1.9483352820078532, Validation Accuracy: 33.496601359456214%\n",
      "Epoch 3, Batch 0, Loss: 1.9819947481155396\n",
      "Epoch 3, Batch 10, Loss: 1.8649293184280396\n",
      "Epoch 3, Train Loss: 1.9310323079427083, Validation Accuracy: 35.715713714514195%\n",
      "Epoch 4, Batch 0, Loss: 1.7696908712387085\n",
      "Epoch 4, Batch 10, Loss: 1.9500973224639893\n",
      "Epoch 4, Train Loss: 1.862119181950887, Validation Accuracy: 34.23630547780888%\n",
      "Epoch 5, Batch 0, Loss: 1.7699947357177734\n",
      "Epoch 5, Batch 10, Loss: 1.6749879121780396\n",
      "Epoch 5, Train Loss: 1.7784127553304037, Validation Accuracy: 36.00559776089564%\n",
      "Epoch 6, Batch 0, Loss: 1.8756965398788452\n",
      "Epoch 6, Batch 10, Loss: 1.659355640411377\n",
      "Epoch 6, Train Loss: 1.830856720606486, Validation Accuracy: 38.4046381447421%\n",
      "Epoch 7, Batch 0, Loss: 1.8756011724472046\n",
      "Epoch 7, Batch 10, Loss: 1.7012670040130615\n",
      "Epoch 7, Train Loss: 1.725114099184672, Validation Accuracy: 38.18472610955618%\n",
      "Epoch 8, Batch 0, Loss: 1.52226984500885\n",
      "Epoch 8, Batch 10, Loss: 1.725603461265564\n",
      "Epoch 8, Train Loss: 1.6943399588267007, Validation Accuracy: 40.543782487005195%\n",
      "Epoch 9, Batch 0, Loss: 1.6435825824737549\n",
      "Epoch 9, Batch 10, Loss: 1.6302851438522339\n",
      "Epoch 9, Train Loss: 1.668621039390564, Validation Accuracy: 38.69452219112355%\n",
      "Best Validation Accuracy: 40.543782487005195%\n",
      "Test Accuracy: 48.871866295264624%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from medmnist import PathMNIST\n",
    "import numpy as np\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = self.get_resnet(base_model)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def get_resnet(self, base_model):\n",
    "        model = models.__dict__[base_model](weights=None)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        z = self.projector(h)\n",
    "        return h, z\n",
    "\n",
    "\n",
    "model = SimCLR(base_model='resnet18', out_dim=128).cuda()\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./simclr_pathmnist_pretrained_epoch_25.pth'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, base_encoder):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.classifier = nn.Linear(512, 9)  # PathMNIST 有 9 个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        logits = self.classifier(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "linear_probe_model = LinearProbe(model.encoder).cuda()\n",
    "\n",
    "PathMNIST_MEAN = [0.73765225, 0.53090023, 0.70307171]\n",
    "PathMNIST_STD = [0.12319908, 0.17607205, 0.12394462]\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=PathMNIST_MEAN, std=PathMNIST_STD)\n",
    "])\n",
    "\n",
    "train_dataset = PathMNIST(split='train', transform=data_transform, download=True, size=64)\n",
    "val_dataset = PathMNIST(split='val', transform=data_transform, download=True, size=64)\n",
    "test_dataset = PathMNIST(split='test', transform=data_transform, download=True, size=64)\n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "subset_indices = list(range(num_samples))\n",
    "np.random.shuffle(subset_indices)\n",
    "subset_indices = subset_indices[:int(0.01 * num_samples)]\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe_model.classifier.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "def train_linear_probe(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.cuda(), labels.squeeze().long().cuda() \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.squeeze().long().cuda()  # 确保 labels 是 1D 张量\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_linear_probe(linear_probe_model, train_loader, criterion, optimizer, epoch)\n",
    "    val_accuracy = evaluate(linear_probe_model, val_loader)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = linear_probe_model.state_dict()\n",
    "\n",
    "linear_probe_model.load_state_dict(best_model_state)\n",
    "\n",
    "test_accuracy = evaluate(linear_probe_model, test_loader)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}%')\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5c99dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Epoch 0, Batch 0, Loss: 2.325059175491333\n",
      "Epoch 0, Batch 10, Loss: 2.0261573791503906\n",
      "Epoch 0, Batch 20, Loss: 1.869328260421753\n",
      "Epoch 0, Batch 30, Loss: 1.9303045272827148\n",
      "Epoch 0, Batch 40, Loss: 1.8603856563568115\n",
      "Epoch 0, Batch 50, Loss: 1.9464701414108276\n",
      "Epoch 0, Batch 60, Loss: 1.8822803497314453\n",
      "Epoch 0, Batch 70, Loss: 1.9040511846542358\n",
      "Epoch 0, Batch 80, Loss: 1.8852758407592773\n",
      "Epoch 0, Batch 90, Loss: 1.744970679283142\n",
      "Epoch 0, Batch 100, Loss: 1.6991333961486816\n",
      "Epoch 0, Batch 110, Loss: 1.7160401344299316\n",
      "Epoch 0, Batch 120, Loss: 1.7872779369354248\n",
      "Epoch 0, Batch 130, Loss: 1.664961338043213\n",
      "Epoch 0, Batch 140, Loss: 1.7176120281219482\n",
      "Epoch 0, Train Loss: 1.8457414503638625, Validation Accuracy: 40.903638544582165%\n",
      "Epoch 1, Batch 0, Loss: 1.5696567296981812\n",
      "Epoch 1, Batch 10, Loss: 1.7764440774917603\n",
      "Epoch 1, Batch 20, Loss: 1.4673632383346558\n",
      "Epoch 1, Batch 30, Loss: 1.795087218284607\n",
      "Epoch 1, Batch 40, Loss: 1.563140630722046\n",
      "Epoch 1, Batch 50, Loss: 1.5867583751678467\n",
      "Epoch 1, Batch 60, Loss: 1.717596173286438\n",
      "Epoch 1, Batch 70, Loss: 1.5594861507415771\n",
      "Epoch 1, Batch 80, Loss: 1.6739294528961182\n",
      "Epoch 1, Batch 90, Loss: 1.5655972957611084\n",
      "Epoch 1, Batch 100, Loss: 1.5025708675384521\n",
      "Epoch 1, Batch 110, Loss: 1.4174327850341797\n",
      "Epoch 1, Batch 120, Loss: 1.4919402599334717\n",
      "Epoch 1, Batch 130, Loss: 1.3811472654342651\n",
      "Epoch 1, Batch 140, Loss: 1.48970627784729\n",
      "Epoch 1, Train Loss: 1.5865212509818112, Validation Accuracy: 46.75129948020792%\n",
      "Epoch 2, Batch 0, Loss: 1.6412627696990967\n",
      "Epoch 2, Batch 10, Loss: 1.5166637897491455\n",
      "Epoch 2, Batch 20, Loss: 1.3564530611038208\n",
      "Epoch 2, Batch 30, Loss: 1.5361231565475464\n",
      "Epoch 2, Batch 40, Loss: 1.517110824584961\n",
      "Epoch 2, Batch 50, Loss: 1.5728288888931274\n",
      "Epoch 2, Batch 60, Loss: 1.582047462463379\n",
      "Epoch 2, Batch 70, Loss: 1.4315348863601685\n",
      "Epoch 2, Batch 80, Loss: 1.3726444244384766\n",
      "Epoch 2, Batch 90, Loss: 1.61917245388031\n",
      "Epoch 2, Batch 100, Loss: 1.540920615196228\n",
      "Epoch 2, Batch 110, Loss: 1.620095133781433\n",
      "Epoch 2, Batch 120, Loss: 1.4085696935653687\n",
      "Epoch 2, Batch 130, Loss: 1.3821910619735718\n",
      "Epoch 2, Batch 140, Loss: 1.3849844932556152\n",
      "Epoch 2, Train Loss: 1.4786399780435766, Validation Accuracy: 49.120351859256296%\n",
      "Epoch 3, Batch 0, Loss: 1.4556514024734497\n",
      "Epoch 3, Batch 10, Loss: 1.4668102264404297\n",
      "Epoch 3, Batch 20, Loss: 1.4841159582138062\n",
      "Epoch 3, Batch 30, Loss: 1.4907203912734985\n",
      "Epoch 3, Batch 40, Loss: 1.5891704559326172\n",
      "Epoch 3, Batch 50, Loss: 1.3643271923065186\n",
      "Epoch 3, Batch 60, Loss: 1.4982587099075317\n",
      "Epoch 3, Batch 70, Loss: 1.428160309791565\n",
      "Epoch 3, Batch 80, Loss: 1.5667794942855835\n",
      "Epoch 3, Batch 90, Loss: 1.4116331338882446\n",
      "Epoch 3, Batch 100, Loss: 1.3343369960784912\n",
      "Epoch 3, Batch 110, Loss: 1.3320441246032715\n",
      "Epoch 3, Batch 120, Loss: 1.3098082542419434\n",
      "Epoch 3, Batch 130, Loss: 1.324905514717102\n",
      "Epoch 3, Batch 140, Loss: 1.349735975265503\n",
      "Epoch 3, Train Loss: 1.404086227958084, Validation Accuracy: 49.800079968012795%\n",
      "Epoch 4, Batch 0, Loss: 1.3682806491851807\n",
      "Epoch 4, Batch 10, Loss: 1.2420859336853027\n",
      "Epoch 4, Batch 20, Loss: 1.424777626991272\n",
      "Epoch 4, Batch 30, Loss: 1.321277141571045\n",
      "Epoch 4, Batch 40, Loss: 1.437815546989441\n",
      "Epoch 4, Batch 50, Loss: 1.5008288621902466\n",
      "Epoch 4, Batch 60, Loss: 1.1653797626495361\n",
      "Epoch 4, Batch 70, Loss: 1.557770848274231\n",
      "Epoch 4, Batch 80, Loss: 1.3929035663604736\n",
      "Epoch 4, Batch 90, Loss: 1.369852900505066\n",
      "Epoch 4, Batch 100, Loss: 1.4433685541152954\n",
      "Epoch 4, Batch 110, Loss: 1.3121538162231445\n",
      "Epoch 4, Batch 120, Loss: 1.2135865688323975\n",
      "Epoch 4, Batch 130, Loss: 1.3202662467956543\n",
      "Epoch 4, Batch 140, Loss: 1.2243268489837646\n",
      "Epoch 4, Train Loss: 1.350764163842438, Validation Accuracy: 50.79968012794882%\n",
      "Epoch 5, Batch 0, Loss: 1.2518025636672974\n",
      "Epoch 5, Batch 10, Loss: 1.6988757848739624\n",
      "Epoch 5, Batch 20, Loss: 1.536182165145874\n",
      "Epoch 5, Batch 30, Loss: 1.222938895225525\n",
      "Epoch 5, Batch 40, Loss: 1.415273904800415\n",
      "Epoch 5, Batch 50, Loss: 1.2320406436920166\n",
      "Epoch 5, Batch 60, Loss: 1.3081626892089844\n",
      "Epoch 5, Batch 70, Loss: 1.4834243059158325\n",
      "Epoch 5, Batch 80, Loss: 1.3182834386825562\n",
      "Epoch 5, Batch 90, Loss: 1.2779229879379272\n",
      "Epoch 5, Batch 100, Loss: 1.2987874746322632\n",
      "Epoch 5, Batch 110, Loss: 1.2642477750778198\n",
      "Epoch 5, Batch 120, Loss: 1.2973185777664185\n",
      "Epoch 5, Batch 130, Loss: 1.3750998973846436\n",
      "Epoch 5, Batch 140, Loss: 1.12004554271698\n",
      "Epoch 5, Train Loss: 1.3275603504045634, Validation Accuracy: 49.46021591363455%\n",
      "Epoch 6, Batch 0, Loss: 1.3600291013717651\n",
      "Epoch 6, Batch 10, Loss: 1.2322529554367065\n",
      "Epoch 6, Batch 20, Loss: 1.343205451965332\n",
      "Epoch 6, Batch 30, Loss: 1.1362806558609009\n",
      "Epoch 6, Batch 40, Loss: 1.2135322093963623\n",
      "Epoch 6, Batch 50, Loss: 1.469697117805481\n",
      "Epoch 6, Batch 60, Loss: 1.1376373767852783\n",
      "Epoch 6, Batch 70, Loss: 1.2258461713790894\n",
      "Epoch 6, Batch 80, Loss: 1.1748120784759521\n",
      "Epoch 6, Batch 90, Loss: 1.4615020751953125\n",
      "Epoch 6, Batch 100, Loss: 1.4356557130813599\n",
      "Epoch 6, Batch 110, Loss: 1.2356066703796387\n",
      "Epoch 6, Batch 120, Loss: 1.2281711101531982\n",
      "Epoch 6, Batch 130, Loss: 1.196765661239624\n",
      "Epoch 6, Batch 140, Loss: 1.27125084400177\n",
      "Epoch 6, Train Loss: 1.287573147327342, Validation Accuracy: 51.11955217912835%\n",
      "Epoch 7, Batch 0, Loss: 1.2721900939941406\n",
      "Epoch 7, Batch 10, Loss: 1.3973013162612915\n",
      "Epoch 7, Batch 20, Loss: 1.1217976808547974\n",
      "Epoch 7, Batch 30, Loss: 1.3474234342575073\n",
      "Epoch 7, Batch 40, Loss: 1.2274738550186157\n",
      "Epoch 7, Batch 50, Loss: 1.3768646717071533\n",
      "Epoch 7, Batch 60, Loss: 1.2096668481826782\n",
      "Epoch 7, Batch 70, Loss: 1.2994678020477295\n",
      "Epoch 7, Batch 80, Loss: 1.322965383529663\n",
      "Epoch 7, Batch 90, Loss: 1.1931266784667969\n",
      "Epoch 7, Batch 100, Loss: 1.2371299266815186\n",
      "Epoch 7, Batch 110, Loss: 1.28859281539917\n",
      "Epoch 7, Batch 120, Loss: 1.4926880598068237\n",
      "Epoch 7, Batch 130, Loss: 1.400978684425354\n",
      "Epoch 7, Batch 140, Loss: 1.2587820291519165\n",
      "Epoch 7, Train Loss: 1.2763007414256426, Validation Accuracy: 51.789284286285486%\n",
      "Epoch 8, Batch 0, Loss: 1.1703871488571167\n",
      "Epoch 8, Batch 10, Loss: 1.145031452178955\n",
      "Epoch 8, Batch 20, Loss: 1.142805576324463\n",
      "Epoch 8, Batch 30, Loss: 1.2113206386566162\n",
      "Epoch 8, Batch 40, Loss: 1.086745262145996\n",
      "Epoch 8, Batch 50, Loss: 1.3445570468902588\n",
      "Epoch 8, Batch 60, Loss: 1.3006951808929443\n",
      "Epoch 8, Batch 70, Loss: 1.0408860445022583\n",
      "Epoch 8, Batch 80, Loss: 1.1340125799179077\n",
      "Epoch 8, Batch 90, Loss: 1.314681053161621\n",
      "Epoch 8, Batch 100, Loss: 1.41933274269104\n",
      "Epoch 8, Batch 110, Loss: 1.0924662351608276\n",
      "Epoch 8, Batch 120, Loss: 1.336837887763977\n",
      "Epoch 8, Batch 130, Loss: 1.2599886655807495\n",
      "Epoch 8, Batch 140, Loss: 1.0931763648986816\n",
      "Epoch 8, Train Loss: 1.2491822555555518, Validation Accuracy: 52.07916833266693%\n",
      "Epoch 9, Batch 0, Loss: 1.2805516719818115\n",
      "Epoch 9, Batch 10, Loss: 1.2622257471084595\n",
      "Epoch 9, Batch 20, Loss: 1.254238486289978\n",
      "Epoch 9, Batch 30, Loss: 1.2445831298828125\n",
      "Epoch 9, Batch 40, Loss: 1.180242896080017\n",
      "Epoch 9, Batch 50, Loss: 1.280112862586975\n",
      "Epoch 9, Batch 60, Loss: 1.301761507987976\n",
      "Epoch 9, Batch 70, Loss: 1.2083144187927246\n",
      "Epoch 9, Batch 80, Loss: 1.2289140224456787\n",
      "Epoch 9, Batch 90, Loss: 1.1540782451629639\n",
      "Epoch 9, Batch 100, Loss: 1.3083971738815308\n",
      "Epoch 9, Batch 110, Loss: 1.0414044857025146\n",
      "Epoch 9, Batch 120, Loss: 1.3269591331481934\n",
      "Epoch 9, Batch 130, Loss: 1.264149785041809\n",
      "Epoch 9, Batch 140, Loss: 1.0188714265823364\n",
      "Epoch 9, Train Loss: 1.2328602921032736, Validation Accuracy: 50.479808076769295%\n",
      "Best Validation Accuracy: 52.07916833266693%\n",
      "Test Accuracy: 57.50696378830084%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from medmnist import PathMNIST\n",
    "import numpy as np\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = self.get_resnet(base_model)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def get_resnet(self, base_model):\n",
    "        model = models.__dict__[base_model](weights=None)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        z = self.projector(h)\n",
    "        return h, z\n",
    "\n",
    "model = SimCLR(base_model='resnet18', out_dim=128).cuda()\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./simclr_pathmnist_pretrained_epoch_25.pth'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, base_encoder):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.classifier = nn.Linear(512, 9)  # PathMNIST 有 9 个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        logits = self.classifier(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "linear_probe_model = LinearProbe(model.encoder).cuda()\n",
    "\n",
    "PathMNIST_MEAN = [0.73765225, 0.53090023, 0.70307171]\n",
    "PathMNIST_STD = [0.12319908, 0.17607205, 0.12394462]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=PathMNIST_MEAN, std=PathMNIST_STD)\n",
    "])\n",
    "\n",
    "train_dataset = PathMNIST(split='train', transform=data_transform, download=True, size=64)\n",
    "val_dataset = PathMNIST(split='val', transform=data_transform, download=True, size=64)\n",
    "test_dataset = PathMNIST(split='test', transform=data_transform, download=True, size=64)\n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "subset_indices = list(range(num_samples))\n",
    "np.random.shuffle(subset_indices)\n",
    "subset_indices = subset_indices[:int(0.1 * num_samples)]\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe_model.classifier.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "def train_linear_probe(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.cuda(), labels.squeeze().long().cuda()  # 确保 labels 是 1D 张量\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.squeeze().long().cuda()  # 确保 labels 是 1D 张量\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_linear_probe(linear_probe_model, train_loader, criterion, optimizer, epoch)\n",
    "    val_accuracy = evaluate(linear_probe_model, val_loader)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    \n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = linear_probe_model.state_dict()\n",
    "\n",
    "linear_probe_model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "test_accuracy = evaluate(linear_probe_model, test_loader)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}%')\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b6de75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Epoch 0, Batch 0, Loss: 2.4099528789520264\n",
      "Epoch 0, Batch 10, Loss: 2.114763021469116\n",
      "Epoch 0, Batch 20, Loss: 2.169755697250366\n",
      "Epoch 0, Batch 30, Loss: 2.00824236869812\n",
      "Epoch 0, Batch 40, Loss: 1.9558193683624268\n",
      "Epoch 0, Batch 50, Loss: 1.9336069822311401\n",
      "Epoch 0, Batch 60, Loss: 1.9814732074737549\n",
      "Epoch 0, Batch 70, Loss: 1.798601508140564\n",
      "Epoch 0, Batch 80, Loss: 1.7699127197265625\n",
      "Epoch 0, Batch 90, Loss: 1.7611044645309448\n",
      "Epoch 0, Batch 100, Loss: 1.7327957153320312\n",
      "Epoch 0, Batch 110, Loss: 1.7308229207992554\n",
      "Epoch 0, Batch 120, Loss: 1.6507103443145752\n",
      "Epoch 0, Batch 130, Loss: 1.6986031532287598\n",
      "Epoch 0, Batch 140, Loss: 1.60010826587677\n",
      "Epoch 0, Batch 150, Loss: 1.6408694982528687\n",
      "Epoch 0, Batch 160, Loss: 1.6401801109313965\n",
      "Epoch 0, Batch 170, Loss: 1.8218110799789429\n",
      "Epoch 0, Batch 180, Loss: 1.5239169597625732\n",
      "Epoch 0, Batch 190, Loss: 1.4193837642669678\n",
      "Epoch 0, Batch 200, Loss: 1.5647618770599365\n",
      "Epoch 0, Batch 210, Loss: 1.4917222261428833\n",
      "Epoch 0, Batch 220, Loss: 1.4758144617080688\n",
      "Epoch 0, Batch 230, Loss: 1.7279456853866577\n",
      "Epoch 0, Batch 240, Loss: 1.5280207395553589\n",
      "Epoch 0, Batch 250, Loss: 1.665341854095459\n",
      "Epoch 0, Batch 260, Loss: 1.4599616527557373\n",
      "Epoch 0, Batch 270, Loss: 1.4925947189331055\n",
      "Epoch 0, Batch 280, Loss: 1.4225738048553467\n",
      "Epoch 0, Batch 290, Loss: 1.5941001176834106\n",
      "Epoch 0, Batch 300, Loss: 1.372496485710144\n",
      "Epoch 0, Batch 310, Loss: 1.3487799167633057\n",
      "Epoch 0, Batch 320, Loss: 1.414111852645874\n",
      "Epoch 0, Batch 330, Loss: 1.4743717908859253\n",
      "Epoch 0, Batch 340, Loss: 1.5443094968795776\n",
      "Epoch 0, Batch 350, Loss: 1.4041067361831665\n",
      "Epoch 0, Batch 360, Loss: 1.5655637979507446\n",
      "Epoch 0, Batch 370, Loss: 1.5953203439712524\n",
      "Epoch 0, Batch 380, Loss: 1.4973833560943604\n",
      "Epoch 0, Batch 390, Loss: 1.4860169887542725\n",
      "Epoch 0, Batch 400, Loss: 1.2669944763183594\n",
      "Epoch 0, Batch 410, Loss: 1.5190677642822266\n",
      "Epoch 0, Batch 420, Loss: 1.4768402576446533\n",
      "Epoch 0, Batch 430, Loss: 1.5312093496322632\n",
      "Epoch 0, Batch 440, Loss: 1.39345383644104\n",
      "Epoch 0, Batch 450, Loss: 1.6476306915283203\n",
      "Epoch 0, Batch 460, Loss: 1.334822177886963\n",
      "Epoch 0, Batch 470, Loss: 1.454467535018921\n",
      "Epoch 0, Batch 480, Loss: 1.2198505401611328\n",
      "Epoch 0, Batch 490, Loss: 1.3088427782058716\n",
      "Epoch 0, Batch 500, Loss: 1.3837543725967407\n",
      "Epoch 0, Batch 510, Loss: 1.4172029495239258\n",
      "Epoch 0, Batch 520, Loss: 1.233642339706421\n",
      "Epoch 0, Batch 530, Loss: 1.5033912658691406\n",
      "Epoch 0, Batch 540, Loss: 1.4061980247497559\n",
      "Epoch 0, Batch 550, Loss: 1.3093581199645996\n",
      "Epoch 0, Batch 560, Loss: 1.4285752773284912\n",
      "Epoch 0, Batch 570, Loss: 1.2760354280471802\n",
      "Epoch 0, Batch 580, Loss: 1.5629403591156006\n",
      "Epoch 0, Batch 590, Loss: 1.5070528984069824\n",
      "Epoch 0, Batch 600, Loss: 1.608215093612671\n",
      "Epoch 0, Batch 610, Loss: 1.3276188373565674\n",
      "Epoch 0, Batch 620, Loss: 1.2778972387313843\n",
      "Epoch 0, Batch 630, Loss: 1.3216794729232788\n",
      "Epoch 0, Batch 640, Loss: 1.4541186094284058\n",
      "Epoch 0, Batch 650, Loss: 1.307205319404602\n",
      "Epoch 0, Batch 660, Loss: 1.2228764295578003\n",
      "Epoch 0, Batch 670, Loss: 1.3776700496673584\n",
      "Epoch 0, Batch 680, Loss: 1.3121294975280762\n",
      "Epoch 0, Batch 690, Loss: 1.3056612014770508\n",
      "Epoch 0, Batch 700, Loss: 1.5103470087051392\n",
      "Epoch 0, Batch 710, Loss: 1.2409942150115967\n",
      "Epoch 0, Batch 720, Loss: 1.2577465772628784\n",
      "Epoch 0, Batch 730, Loss: 1.359140396118164\n",
      "Epoch 0, Batch 740, Loss: 1.4207037687301636\n",
      "Epoch 0, Batch 750, Loss: 1.2549877166748047\n",
      "Epoch 0, Batch 760, Loss: 1.24838125705719\n",
      "Epoch 0, Batch 770, Loss: 1.5143393278121948\n",
      "Epoch 0, Batch 780, Loss: 1.222695231437683\n",
      "Epoch 0, Batch 790, Loss: 1.5996497869491577\n",
      "Epoch 0, Batch 800, Loss: 1.5105197429656982\n",
      "Epoch 0, Batch 810, Loss: 1.519291877746582\n",
      "Epoch 0, Batch 820, Loss: 1.4181053638458252\n",
      "Epoch 0, Batch 830, Loss: 1.3249189853668213\n",
      "Epoch 0, Batch 840, Loss: 1.3082536458969116\n",
      "Epoch 0, Batch 850, Loss: 1.2852767705917358\n",
      "Epoch 0, Batch 860, Loss: 1.1359559297561646\n",
      "Epoch 0, Batch 870, Loss: 1.4574170112609863\n",
      "Epoch 0, Batch 880, Loss: 1.1791576147079468\n",
      "Epoch 0, Batch 890, Loss: 1.375124454498291\n",
      "Epoch 0, Batch 900, Loss: 1.4029687643051147\n",
      "Epoch 0, Batch 910, Loss: 1.4079382419586182\n",
      "Epoch 0, Batch 920, Loss: 1.4241642951965332\n",
      "Epoch 0, Batch 930, Loss: 1.2371635437011719\n",
      "Epoch 0, Batch 940, Loss: 1.2394347190856934\n",
      "Epoch 0, Batch 950, Loss: 1.4833989143371582\n",
      "Epoch 0, Batch 960, Loss: 1.244652509689331\n",
      "Epoch 0, Batch 970, Loss: 1.4031175374984741\n",
      "Epoch 0, Batch 980, Loss: 1.0824577808380127\n",
      "Epoch 0, Batch 990, Loss: 1.1632130146026611\n",
      "Epoch 0, Batch 1000, Loss: 1.2515571117401123\n",
      "Epoch 0, Batch 1010, Loss: 1.356601357460022\n",
      "Epoch 0, Batch 1020, Loss: 1.1601694822311401\n",
      "Epoch 0, Batch 1030, Loss: 1.3398751020431519\n",
      "Epoch 0, Batch 1040, Loss: 1.2198996543884277\n",
      "Epoch 0, Batch 1050, Loss: 1.291587233543396\n",
      "Epoch 0, Batch 1060, Loss: 1.4093717336654663\n",
      "Epoch 0, Batch 1070, Loss: 1.0865592956542969\n",
      "Epoch 0, Batch 1080, Loss: 1.2985432147979736\n",
      "Epoch 0, Batch 1090, Loss: 1.3428289890289307\n",
      "Epoch 0, Batch 1100, Loss: 1.246659517288208\n",
      "Epoch 0, Batch 1110, Loss: 1.4556080102920532\n",
      "Epoch 0, Batch 1120, Loss: 1.1545345783233643\n",
      "Epoch 0, Batch 1130, Loss: 1.3205835819244385\n",
      "Epoch 0, Batch 1140, Loss: 1.3608320951461792\n",
      "Epoch 0, Batch 1150, Loss: 1.2999954223632812\n",
      "Epoch 0, Batch 1160, Loss: 1.46760094165802\n",
      "Epoch 0, Batch 1170, Loss: 1.387125849723816\n",
      "Epoch 0, Batch 1180, Loss: 1.482690691947937\n",
      "Epoch 0, Batch 1190, Loss: 1.5594310760498047\n",
      "Epoch 0, Batch 1200, Loss: 1.2594422101974487\n",
      "Epoch 0, Batch 1210, Loss: 1.3920162916183472\n",
      "Epoch 0, Batch 1220, Loss: 1.4449793100357056\n",
      "Epoch 0, Batch 1230, Loss: 1.3628424406051636\n",
      "Epoch 0, Batch 1240, Loss: 1.2366939783096313\n",
      "Epoch 0, Batch 1250, Loss: 1.2798782587051392\n",
      "Epoch 0, Batch 1260, Loss: 1.2000665664672852\n",
      "Epoch 0, Batch 1270, Loss: 1.3249980211257935\n",
      "Epoch 0, Batch 1280, Loss: 1.4173800945281982\n",
      "Epoch 0, Batch 1290, Loss: 1.203636884689331\n",
      "Epoch 0, Batch 1300, Loss: 1.2433758974075317\n",
      "Epoch 0, Batch 1310, Loss: 1.4739911556243896\n",
      "Epoch 0, Batch 1320, Loss: 1.272282361984253\n",
      "Epoch 0, Batch 1330, Loss: 1.1521697044372559\n",
      "Epoch 0, Batch 1340, Loss: 1.2030137777328491\n",
      "Epoch 0, Batch 1350, Loss: 1.3057576417922974\n",
      "Epoch 0, Batch 1360, Loss: 1.1788911819458008\n",
      "Epoch 0, Batch 1370, Loss: 1.237510323524475\n",
      "Epoch 0, Batch 1380, Loss: 1.3641870021820068\n",
      "Epoch 0, Batch 1390, Loss: 1.3210397958755493\n",
      "Epoch 0, Batch 1400, Loss: 1.1169096231460571\n",
      "Epoch 0, Train Loss: 1.4323447033201624, Validation Accuracy: 54.37824870051979%\n",
      "Epoch 1, Batch 0, Loss: 1.2526174783706665\n",
      "Epoch 1, Batch 10, Loss: 1.328790545463562\n",
      "Epoch 1, Batch 20, Loss: 1.0271073579788208\n",
      "Epoch 1, Batch 30, Loss: 1.3878133296966553\n",
      "Epoch 1, Batch 40, Loss: 1.368107795715332\n",
      "Epoch 1, Batch 50, Loss: 1.2844423055648804\n",
      "Epoch 1, Batch 60, Loss: 1.2585545778274536\n",
      "Epoch 1, Batch 70, Loss: 1.2497740983963013\n",
      "Epoch 1, Batch 80, Loss: 1.2398139238357544\n",
      "Epoch 1, Batch 90, Loss: 1.3437020778656006\n",
      "Epoch 1, Batch 100, Loss: 1.3316912651062012\n",
      "Epoch 1, Batch 110, Loss: 1.2722160816192627\n",
      "Epoch 1, Batch 120, Loss: 1.3303840160369873\n",
      "Epoch 1, Batch 130, Loss: 1.2069365978240967\n",
      "Epoch 1, Batch 140, Loss: 1.2700343132019043\n",
      "Epoch 1, Batch 150, Loss: 1.178518533706665\n",
      "Epoch 1, Batch 160, Loss: 1.1293882131576538\n",
      "Epoch 1, Batch 170, Loss: 1.3115651607513428\n",
      "Epoch 1, Batch 180, Loss: 1.2861828804016113\n",
      "Epoch 1, Batch 190, Loss: 1.345463752746582\n",
      "Epoch 1, Batch 200, Loss: 1.3766292333602905\n",
      "Epoch 1, Batch 210, Loss: 1.4233191013336182\n",
      "Epoch 1, Batch 220, Loss: 1.2959007024765015\n",
      "Epoch 1, Batch 230, Loss: 1.2157171964645386\n",
      "Epoch 1, Batch 240, Loss: 1.2826772928237915\n",
      "Epoch 1, Batch 250, Loss: 1.1623950004577637\n",
      "Epoch 1, Batch 260, Loss: 1.3434576988220215\n",
      "Epoch 1, Batch 270, Loss: 1.228365182876587\n",
      "Epoch 1, Batch 280, Loss: 1.3143389225006104\n",
      "Epoch 1, Batch 290, Loss: 1.4451875686645508\n",
      "Epoch 1, Batch 300, Loss: 1.2653261423110962\n",
      "Epoch 1, Batch 310, Loss: 1.3322525024414062\n",
      "Epoch 1, Batch 320, Loss: 1.3408805131912231\n",
      "Epoch 1, Batch 330, Loss: 1.1980445384979248\n",
      "Epoch 1, Batch 340, Loss: 1.4534283876419067\n",
      "Epoch 1, Batch 350, Loss: 1.150405764579773\n",
      "Epoch 1, Batch 360, Loss: 1.1449602842330933\n",
      "Epoch 1, Batch 370, Loss: 1.4879024028778076\n",
      "Epoch 1, Batch 380, Loss: 1.276340365409851\n",
      "Epoch 1, Batch 390, Loss: 1.2555266618728638\n",
      "Epoch 1, Batch 400, Loss: 1.1513909101486206\n",
      "Epoch 1, Batch 410, Loss: 1.0708926916122437\n",
      "Epoch 1, Batch 420, Loss: 1.2623392343521118\n",
      "Epoch 1, Batch 430, Loss: 1.3163034915924072\n",
      "Epoch 1, Batch 440, Loss: 1.1572335958480835\n",
      "Epoch 1, Batch 450, Loss: 1.4529987573623657\n",
      "Epoch 1, Batch 460, Loss: 1.1187705993652344\n",
      "Epoch 1, Batch 470, Loss: 1.3224717378616333\n",
      "Epoch 1, Batch 480, Loss: 1.2728995084762573\n",
      "Epoch 1, Batch 490, Loss: 1.3559404611587524\n",
      "Epoch 1, Batch 500, Loss: 1.1610218286514282\n",
      "Epoch 1, Batch 510, Loss: 1.3712464570999146\n",
      "Epoch 1, Batch 520, Loss: 1.0773714780807495\n",
      "Epoch 1, Batch 530, Loss: 1.330770492553711\n",
      "Epoch 1, Batch 540, Loss: 1.192647099494934\n",
      "Epoch 1, Batch 550, Loss: 1.2251439094543457\n",
      "Epoch 1, Batch 560, Loss: 1.477452278137207\n",
      "Epoch 1, Batch 570, Loss: 1.1946970224380493\n",
      "Epoch 1, Batch 580, Loss: 1.2438856363296509\n",
      "Epoch 1, Batch 590, Loss: 1.252270221710205\n",
      "Epoch 1, Batch 600, Loss: 1.3546600341796875\n",
      "Epoch 1, Batch 610, Loss: 1.4877419471740723\n",
      "Epoch 1, Batch 620, Loss: 1.2032629251480103\n",
      "Epoch 1, Batch 630, Loss: 1.4373183250427246\n",
      "Epoch 1, Batch 640, Loss: 1.290537714958191\n",
      "Epoch 1, Batch 650, Loss: 1.104826807975769\n",
      "Epoch 1, Batch 660, Loss: 1.3249435424804688\n",
      "Epoch 1, Batch 670, Loss: 1.2081269025802612\n",
      "Epoch 1, Batch 680, Loss: 1.294642686843872\n",
      "Epoch 1, Batch 690, Loss: 1.2602171897888184\n",
      "Epoch 1, Batch 700, Loss: 1.2969287633895874\n",
      "Epoch 1, Batch 710, Loss: 1.2133240699768066\n",
      "Epoch 1, Batch 720, Loss: 1.018012285232544\n",
      "Epoch 1, Batch 730, Loss: 1.2281399965286255\n",
      "Epoch 1, Batch 740, Loss: 1.1348891258239746\n",
      "Epoch 1, Batch 750, Loss: 1.3244588375091553\n",
      "Epoch 1, Batch 760, Loss: 1.478352427482605\n",
      "Epoch 1, Batch 770, Loss: 1.1749842166900635\n",
      "Epoch 1, Batch 780, Loss: 1.381327509880066\n",
      "Epoch 1, Batch 790, Loss: 1.3483556509017944\n",
      "Epoch 1, Batch 800, Loss: 1.1663423776626587\n",
      "Epoch 1, Batch 810, Loss: 1.2812002897262573\n",
      "Epoch 1, Batch 820, Loss: 1.390953779220581\n",
      "Epoch 1, Batch 830, Loss: 1.2881969213485718\n",
      "Epoch 1, Batch 840, Loss: 1.318138837814331\n",
      "Epoch 1, Batch 850, Loss: 1.4472473859786987\n",
      "Epoch 1, Batch 860, Loss: 1.0558198690414429\n",
      "Epoch 1, Batch 870, Loss: 1.0965745449066162\n",
      "Epoch 1, Batch 880, Loss: 1.3230564594268799\n",
      "Epoch 1, Batch 890, Loss: 1.1928125619888306\n",
      "Epoch 1, Batch 900, Loss: 1.3355419635772705\n",
      "Epoch 1, Batch 910, Loss: 1.2243502140045166\n",
      "Epoch 1, Batch 920, Loss: 1.184386968612671\n",
      "Epoch 1, Batch 930, Loss: 1.2605276107788086\n",
      "Epoch 1, Batch 940, Loss: 1.38789963722229\n",
      "Epoch 1, Batch 950, Loss: 1.076033353805542\n",
      "Epoch 1, Batch 960, Loss: 1.2508423328399658\n",
      "Epoch 1, Batch 970, Loss: 1.2780351638793945\n",
      "Epoch 1, Batch 980, Loss: 1.5947333574295044\n",
      "Epoch 1, Batch 990, Loss: 1.5148957967758179\n",
      "Epoch 1, Batch 1000, Loss: 0.980846643447876\n",
      "Epoch 1, Batch 1010, Loss: 1.1091278791427612\n",
      "Epoch 1, Batch 1020, Loss: 1.334635853767395\n",
      "Epoch 1, Batch 1030, Loss: 1.2931876182556152\n",
      "Epoch 1, Batch 1040, Loss: 1.2280648946762085\n",
      "Epoch 1, Batch 1050, Loss: 1.155274748802185\n",
      "Epoch 1, Batch 1060, Loss: 1.168117642402649\n",
      "Epoch 1, Batch 1070, Loss: 1.344794750213623\n",
      "Epoch 1, Batch 1080, Loss: 1.240358591079712\n",
      "Epoch 1, Batch 1090, Loss: 0.9922930598258972\n",
      "Epoch 1, Batch 1100, Loss: 1.2247915267944336\n",
      "Epoch 1, Batch 1110, Loss: 1.4715816974639893\n",
      "Epoch 1, Batch 1120, Loss: 1.2443832159042358\n",
      "Epoch 1, Batch 1130, Loss: 1.350169062614441\n",
      "Epoch 1, Batch 1140, Loss: 1.1578642129898071\n",
      "Epoch 1, Batch 1150, Loss: 1.4698883295059204\n",
      "Epoch 1, Batch 1160, Loss: 1.2279200553894043\n",
      "Epoch 1, Batch 1170, Loss: 1.1453379392623901\n",
      "Epoch 1, Batch 1180, Loss: 1.208775520324707\n",
      "Epoch 1, Batch 1190, Loss: 1.1389607191085815\n",
      "Epoch 1, Batch 1200, Loss: 1.4045283794403076\n",
      "Epoch 1, Batch 1210, Loss: 1.4174574613571167\n",
      "Epoch 1, Batch 1220, Loss: 1.145429253578186\n",
      "Epoch 1, Batch 1230, Loss: 1.248523473739624\n",
      "Epoch 1, Batch 1240, Loss: 1.385034203529358\n",
      "Epoch 1, Batch 1250, Loss: 1.3263206481933594\n",
      "Epoch 1, Batch 1260, Loss: 1.3480968475341797\n",
      "Epoch 1, Batch 1270, Loss: 1.330392837524414\n",
      "Epoch 1, Batch 1280, Loss: 1.3570059537887573\n",
      "Epoch 1, Batch 1290, Loss: 1.0436091423034668\n",
      "Epoch 1, Batch 1300, Loss: 1.2119684219360352\n",
      "Epoch 1, Batch 1310, Loss: 1.3514736890792847\n",
      "Epoch 1, Batch 1320, Loss: 1.0854822397232056\n",
      "Epoch 1, Batch 1330, Loss: 1.4979937076568604\n",
      "Epoch 1, Batch 1340, Loss: 1.3501498699188232\n",
      "Epoch 1, Batch 1350, Loss: 1.196460247039795\n",
      "Epoch 1, Batch 1360, Loss: 1.4384691715240479\n",
      "Epoch 1, Batch 1370, Loss: 1.0474305152893066\n",
      "Epoch 1, Batch 1380, Loss: 1.4189889430999756\n",
      "Epoch 1, Batch 1390, Loss: 1.1383172273635864\n",
      "Epoch 1, Batch 1400, Loss: 1.2481791973114014\n",
      "Epoch 1, Train Loss: 1.2604091984630903, Validation Accuracy: 55.92762894842063%\n",
      "Epoch 2, Batch 0, Loss: 1.1517891883850098\n",
      "Epoch 2, Batch 10, Loss: 1.1992263793945312\n",
      "Epoch 2, Batch 20, Loss: 1.2007662057876587\n",
      "Epoch 2, Batch 30, Loss: 1.2921910285949707\n",
      "Epoch 2, Batch 40, Loss: 1.0862349271774292\n",
      "Epoch 2, Batch 50, Loss: 1.2275471687316895\n",
      "Epoch 2, Batch 60, Loss: 1.1908661127090454\n",
      "Epoch 2, Batch 70, Loss: 1.3063273429870605\n",
      "Epoch 2, Batch 80, Loss: 1.0887736082077026\n",
      "Epoch 2, Batch 90, Loss: 1.4105888605117798\n",
      "Epoch 2, Batch 100, Loss: 1.2870303392410278\n",
      "Epoch 2, Batch 110, Loss: 1.1254830360412598\n",
      "Epoch 2, Batch 120, Loss: 1.2178336381912231\n",
      "Epoch 2, Batch 130, Loss: 1.3806779384613037\n",
      "Epoch 2, Batch 140, Loss: 1.4224218130111694\n",
      "Epoch 2, Batch 150, Loss: 1.18141770362854\n",
      "Epoch 2, Batch 160, Loss: 1.2119132280349731\n",
      "Epoch 2, Batch 170, Loss: 1.2473305463790894\n",
      "Epoch 2, Batch 180, Loss: 1.2901599407196045\n",
      "Epoch 2, Batch 190, Loss: 1.267941951751709\n",
      "Epoch 2, Batch 200, Loss: 1.1568762063980103\n",
      "Epoch 2, Batch 210, Loss: 1.0589852333068848\n",
      "Epoch 2, Batch 220, Loss: 1.247538685798645\n",
      "Epoch 2, Batch 230, Loss: 1.1279399394989014\n",
      "Epoch 2, Batch 240, Loss: 1.385646939277649\n",
      "Epoch 2, Batch 250, Loss: 1.1870901584625244\n",
      "Epoch 2, Batch 260, Loss: 1.1451466083526611\n",
      "Epoch 2, Batch 270, Loss: 1.2122315168380737\n",
      "Epoch 2, Batch 280, Loss: 1.213257074356079\n",
      "Epoch 2, Batch 290, Loss: 1.1627253293991089\n",
      "Epoch 2, Batch 300, Loss: 0.9681355357170105\n",
      "Epoch 2, Batch 310, Loss: 1.3796390295028687\n",
      "Epoch 2, Batch 320, Loss: 1.3895751237869263\n",
      "Epoch 2, Batch 330, Loss: 1.0876785516738892\n",
      "Epoch 2, Batch 340, Loss: 1.6005744934082031\n",
      "Epoch 2, Batch 350, Loss: 1.357010841369629\n",
      "Epoch 2, Batch 360, Loss: 1.1015212535858154\n",
      "Epoch 2, Batch 370, Loss: 1.0972996950149536\n",
      "Epoch 2, Batch 380, Loss: 1.268571138381958\n",
      "Epoch 2, Batch 390, Loss: 1.0651636123657227\n",
      "Epoch 2, Batch 400, Loss: 1.196703553199768\n",
      "Epoch 2, Batch 410, Loss: 1.3268200159072876\n",
      "Epoch 2, Batch 420, Loss: 1.010817289352417\n",
      "Epoch 2, Batch 430, Loss: 1.0204838514328003\n",
      "Epoch 2, Batch 440, Loss: 1.265681266784668\n",
      "Epoch 2, Batch 450, Loss: 0.9937256574630737\n",
      "Epoch 2, Batch 460, Loss: 1.1367785930633545\n",
      "Epoch 2, Batch 470, Loss: 1.1606085300445557\n",
      "Epoch 2, Batch 480, Loss: 1.2157857418060303\n",
      "Epoch 2, Batch 490, Loss: 1.3240853548049927\n",
      "Epoch 2, Batch 500, Loss: 1.2218244075775146\n",
      "Epoch 2, Batch 510, Loss: 1.6201839447021484\n",
      "Epoch 2, Batch 520, Loss: 1.381348729133606\n",
      "Epoch 2, Batch 530, Loss: 1.1877574920654297\n",
      "Epoch 2, Batch 540, Loss: 1.446656584739685\n",
      "Epoch 2, Batch 550, Loss: 1.1589610576629639\n",
      "Epoch 2, Batch 560, Loss: 1.2851225137710571\n",
      "Epoch 2, Batch 570, Loss: 1.1434756517410278\n",
      "Epoch 2, Batch 580, Loss: 1.1098672151565552\n",
      "Epoch 2, Batch 590, Loss: 1.0396281480789185\n",
      "Epoch 2, Batch 600, Loss: 1.250197410583496\n",
      "Epoch 2, Batch 610, Loss: 1.253220558166504\n",
      "Epoch 2, Batch 620, Loss: 1.2642203569412231\n",
      "Epoch 2, Batch 630, Loss: 1.2230616807937622\n",
      "Epoch 2, Batch 640, Loss: 1.2434449195861816\n",
      "Epoch 2, Batch 650, Loss: 1.297202706336975\n",
      "Epoch 2, Batch 660, Loss: 1.385141134262085\n",
      "Epoch 2, Batch 670, Loss: 1.1160765886306763\n",
      "Epoch 2, Batch 680, Loss: 1.2663497924804688\n",
      "Epoch 2, Batch 690, Loss: 1.1381803750991821\n",
      "Epoch 2, Batch 700, Loss: 1.2234691381454468\n",
      "Epoch 2, Batch 710, Loss: 1.2770602703094482\n",
      "Epoch 2, Batch 720, Loss: 1.1228746175765991\n",
      "Epoch 2, Batch 730, Loss: 1.2668603658676147\n",
      "Epoch 2, Batch 740, Loss: 1.1242992877960205\n",
      "Epoch 2, Batch 750, Loss: 1.295973777770996\n",
      "Epoch 2, Batch 760, Loss: 1.3231571912765503\n",
      "Epoch 2, Batch 770, Loss: 1.1721785068511963\n",
      "Epoch 2, Batch 780, Loss: 1.1823277473449707\n",
      "Epoch 2, Batch 790, Loss: 1.2664659023284912\n",
      "Epoch 2, Batch 800, Loss: 1.5927965641021729\n",
      "Epoch 2, Batch 810, Loss: 1.1577637195587158\n",
      "Epoch 2, Batch 820, Loss: 1.2357196807861328\n",
      "Epoch 2, Batch 830, Loss: 1.0374789237976074\n",
      "Epoch 2, Batch 840, Loss: 1.345502257347107\n",
      "Epoch 2, Batch 850, Loss: 1.2997783422470093\n",
      "Epoch 2, Batch 860, Loss: 1.3052291870117188\n",
      "Epoch 2, Batch 870, Loss: 1.1718090772628784\n",
      "Epoch 2, Batch 880, Loss: 1.141579270362854\n",
      "Epoch 2, Batch 890, Loss: 1.2883508205413818\n",
      "Epoch 2, Batch 900, Loss: 1.3850072622299194\n",
      "Epoch 2, Batch 910, Loss: 1.1856414079666138\n",
      "Epoch 2, Batch 920, Loss: 1.4048588275909424\n",
      "Epoch 2, Batch 930, Loss: 1.370193362236023\n",
      "Epoch 2, Batch 940, Loss: 1.3417260646820068\n",
      "Epoch 2, Batch 950, Loss: 1.2547109127044678\n",
      "Epoch 2, Batch 960, Loss: 1.226953148841858\n",
      "Epoch 2, Batch 970, Loss: 1.4470634460449219\n",
      "Epoch 2, Batch 980, Loss: 1.1596667766571045\n",
      "Epoch 2, Batch 990, Loss: 1.4551082849502563\n",
      "Epoch 2, Batch 1000, Loss: 1.2885088920593262\n",
      "Epoch 2, Batch 1010, Loss: 1.2393062114715576\n",
      "Epoch 2, Batch 1020, Loss: 1.392960786819458\n",
      "Epoch 2, Batch 1030, Loss: 1.2282099723815918\n",
      "Epoch 2, Batch 1040, Loss: 1.0848342180252075\n",
      "Epoch 2, Batch 1050, Loss: 1.2726573944091797\n",
      "Epoch 2, Batch 1060, Loss: 1.370278000831604\n",
      "Epoch 2, Batch 1070, Loss: 1.2903159856796265\n",
      "Epoch 2, Batch 1080, Loss: 1.306617259979248\n",
      "Epoch 2, Batch 1090, Loss: 1.1695342063903809\n",
      "Epoch 2, Batch 1100, Loss: 1.2908275127410889\n",
      "Epoch 2, Batch 1110, Loss: 1.0377345085144043\n",
      "Epoch 2, Batch 1120, Loss: 1.2368685007095337\n",
      "Epoch 2, Batch 1130, Loss: 1.1398541927337646\n",
      "Epoch 2, Batch 1140, Loss: 1.2735651731491089\n",
      "Epoch 2, Batch 1150, Loss: 1.3544071912765503\n",
      "Epoch 2, Batch 1160, Loss: 1.3772367238998413\n",
      "Epoch 2, Batch 1170, Loss: 1.2179930210113525\n",
      "Epoch 2, Batch 1180, Loss: 1.154083490371704\n",
      "Epoch 2, Batch 1190, Loss: 1.1975905895233154\n",
      "Epoch 2, Batch 1200, Loss: 1.2900129556655884\n",
      "Epoch 2, Batch 1210, Loss: 1.0243029594421387\n",
      "Epoch 2, Batch 1220, Loss: 1.0264557600021362\n",
      "Epoch 2, Batch 1230, Loss: 1.2339445352554321\n",
      "Epoch 2, Batch 1240, Loss: 1.2357838153839111\n",
      "Epoch 2, Batch 1250, Loss: 1.4368516206741333\n",
      "Epoch 2, Batch 1260, Loss: 1.352452278137207\n",
      "Epoch 2, Batch 1270, Loss: 1.2188971042633057\n",
      "Epoch 2, Batch 1280, Loss: 1.1436662673950195\n",
      "Epoch 2, Batch 1290, Loss: 1.2515021562576294\n",
      "Epoch 2, Batch 1300, Loss: 1.4662507772445679\n",
      "Epoch 2, Batch 1310, Loss: 1.4465446472167969\n",
      "Epoch 2, Batch 1320, Loss: 1.189529299736023\n",
      "Epoch 2, Batch 1330, Loss: 1.3090983629226685\n",
      "Epoch 2, Batch 1340, Loss: 1.190102219581604\n",
      "Epoch 2, Batch 1350, Loss: 1.1863269805908203\n",
      "Epoch 2, Batch 1360, Loss: 1.360653281211853\n",
      "Epoch 2, Batch 1370, Loss: 1.1692949533462524\n",
      "Epoch 2, Batch 1380, Loss: 1.3343653678894043\n",
      "Epoch 2, Batch 1390, Loss: 1.1303021907806396\n",
      "Epoch 2, Batch 1400, Loss: 1.285461664199829\n",
      "Epoch 2, Train Loss: 1.232684829723098, Validation Accuracy: 56.17752898840464%\n",
      "Epoch 3, Batch 0, Loss: 1.3399451971054077\n",
      "Epoch 3, Batch 10, Loss: 1.199764609336853\n",
      "Epoch 3, Batch 20, Loss: 1.1143079996109009\n",
      "Epoch 3, Batch 30, Loss: 1.1924318075180054\n",
      "Epoch 3, Batch 40, Loss: 1.4448540210723877\n",
      "Epoch 3, Batch 50, Loss: 1.2003642320632935\n",
      "Epoch 3, Batch 60, Loss: 1.2107560634613037\n",
      "Epoch 3, Batch 70, Loss: 1.4012690782546997\n",
      "Epoch 3, Batch 80, Loss: 1.2947437763214111\n",
      "Epoch 3, Batch 90, Loss: 1.2767150402069092\n",
      "Epoch 3, Batch 100, Loss: 1.3257936239242554\n",
      "Epoch 3, Batch 110, Loss: 1.157332181930542\n",
      "Epoch 3, Batch 120, Loss: 1.327109932899475\n",
      "Epoch 3, Batch 130, Loss: 1.273368000984192\n",
      "Epoch 3, Batch 140, Loss: 1.5203624963760376\n",
      "Epoch 3, Batch 150, Loss: 1.3999302387237549\n",
      "Epoch 3, Batch 160, Loss: 1.3645310401916504\n",
      "Epoch 3, Batch 170, Loss: 1.335970163345337\n",
      "Epoch 3, Batch 180, Loss: 1.2702422142028809\n",
      "Epoch 3, Batch 190, Loss: 1.161515712738037\n",
      "Epoch 3, Batch 200, Loss: 1.1676077842712402\n",
      "Epoch 3, Batch 210, Loss: 1.3392192125320435\n",
      "Epoch 3, Batch 220, Loss: 1.3822503089904785\n",
      "Epoch 3, Batch 230, Loss: 1.4670425653457642\n",
      "Epoch 3, Batch 240, Loss: 1.3818700313568115\n",
      "Epoch 3, Batch 250, Loss: 1.0593042373657227\n",
      "Epoch 3, Batch 260, Loss: 1.0688508749008179\n",
      "Epoch 3, Batch 270, Loss: 1.3568109273910522\n",
      "Epoch 3, Batch 280, Loss: 1.2072230577468872\n",
      "Epoch 3, Batch 290, Loss: 1.4091964960098267\n",
      "Epoch 3, Batch 300, Loss: 1.1860649585723877\n",
      "Epoch 3, Batch 310, Loss: 1.2124545574188232\n",
      "Epoch 3, Batch 320, Loss: 1.1316255331039429\n",
      "Epoch 3, Batch 330, Loss: 1.1890496015548706\n",
      "Epoch 3, Batch 340, Loss: 1.3132368326187134\n",
      "Epoch 3, Batch 350, Loss: 1.2945103645324707\n",
      "Epoch 3, Batch 360, Loss: 1.1742007732391357\n",
      "Epoch 3, Batch 370, Loss: 1.4025094509124756\n",
      "Epoch 3, Batch 380, Loss: 1.3844810724258423\n",
      "Epoch 3, Batch 390, Loss: 1.4747276306152344\n",
      "Epoch 3, Batch 400, Loss: 1.181689739227295\n",
      "Epoch 3, Batch 410, Loss: 1.2445197105407715\n",
      "Epoch 3, Batch 420, Loss: 1.040776252746582\n",
      "Epoch 3, Batch 430, Loss: 0.982424259185791\n",
      "Epoch 3, Batch 440, Loss: 1.1905030012130737\n",
      "Epoch 3, Batch 450, Loss: 1.1880335807800293\n",
      "Epoch 3, Batch 460, Loss: 1.2751221656799316\n",
      "Epoch 3, Batch 470, Loss: 1.161569356918335\n",
      "Epoch 3, Batch 480, Loss: 1.1933505535125732\n",
      "Epoch 3, Batch 490, Loss: 1.2729036808013916\n",
      "Epoch 3, Batch 500, Loss: 0.9821694493293762\n",
      "Epoch 3, Batch 510, Loss: 1.0991681814193726\n",
      "Epoch 3, Batch 520, Loss: 1.1409071683883667\n",
      "Epoch 3, Batch 530, Loss: 1.1390981674194336\n",
      "Epoch 3, Batch 540, Loss: 1.307003140449524\n",
      "Epoch 3, Batch 550, Loss: 1.2054945230484009\n",
      "Epoch 3, Batch 560, Loss: 1.3235621452331543\n",
      "Epoch 3, Batch 570, Loss: 1.1167521476745605\n",
      "Epoch 3, Batch 580, Loss: 1.461094856262207\n",
      "Epoch 3, Batch 590, Loss: 1.3134726285934448\n",
      "Epoch 3, Batch 600, Loss: 1.1783626079559326\n",
      "Epoch 3, Batch 610, Loss: 1.2206604480743408\n",
      "Epoch 3, Batch 620, Loss: 1.5540095567703247\n",
      "Epoch 3, Batch 630, Loss: 1.3143047094345093\n",
      "Epoch 3, Batch 640, Loss: 1.132757306098938\n",
      "Epoch 3, Batch 650, Loss: 1.1300649642944336\n",
      "Epoch 3, Batch 660, Loss: 1.1306824684143066\n",
      "Epoch 3, Batch 670, Loss: 1.0704153776168823\n",
      "Epoch 3, Batch 680, Loss: 1.322332739830017\n",
      "Epoch 3, Batch 690, Loss: 1.484201431274414\n",
      "Epoch 3, Batch 700, Loss: 1.137342095375061\n",
      "Epoch 3, Batch 710, Loss: 1.140020728111267\n",
      "Epoch 3, Batch 720, Loss: 1.2004300355911255\n",
      "Epoch 3, Batch 730, Loss: 1.530958652496338\n",
      "Epoch 3, Batch 740, Loss: 1.4828788042068481\n",
      "Epoch 3, Batch 750, Loss: 1.3330105543136597\n",
      "Epoch 3, Batch 760, Loss: 1.2144182920455933\n",
      "Epoch 3, Batch 770, Loss: 1.1467832326889038\n",
      "Epoch 3, Batch 780, Loss: 1.2279542684555054\n",
      "Epoch 3, Batch 790, Loss: 1.184051752090454\n",
      "Epoch 3, Batch 800, Loss: 1.141148567199707\n",
      "Epoch 3, Batch 810, Loss: 1.0740985870361328\n",
      "Epoch 3, Batch 820, Loss: 1.1810451745986938\n",
      "Epoch 3, Batch 830, Loss: 1.1640501022338867\n",
      "Epoch 3, Batch 840, Loss: 1.1627295017242432\n",
      "Epoch 3, Batch 850, Loss: 1.1380434036254883\n",
      "Epoch 3, Batch 860, Loss: 0.9960893392562866\n",
      "Epoch 3, Batch 870, Loss: 1.0884015560150146\n",
      "Epoch 3, Batch 880, Loss: 1.219671368598938\n",
      "Epoch 3, Batch 890, Loss: 1.0469532012939453\n",
      "Epoch 3, Batch 900, Loss: 1.078528881072998\n",
      "Epoch 3, Batch 910, Loss: 1.0402745008468628\n",
      "Epoch 3, Batch 920, Loss: 1.3988442420959473\n",
      "Epoch 3, Batch 930, Loss: 1.1317272186279297\n",
      "Epoch 3, Batch 940, Loss: 1.1125012636184692\n",
      "Epoch 3, Batch 950, Loss: 1.2584415674209595\n",
      "Epoch 3, Batch 960, Loss: 1.206965684890747\n",
      "Epoch 3, Batch 970, Loss: 1.1903008222579956\n",
      "Epoch 3, Batch 980, Loss: 1.2108347415924072\n",
      "Epoch 3, Batch 990, Loss: 1.4249035120010376\n",
      "Epoch 3, Batch 1000, Loss: 1.3281968832015991\n",
      "Epoch 3, Batch 1010, Loss: 1.748674750328064\n",
      "Epoch 3, Batch 1020, Loss: 1.174049735069275\n",
      "Epoch 3, Batch 1030, Loss: 1.2425040006637573\n",
      "Epoch 3, Batch 1040, Loss: 1.2672197818756104\n",
      "Epoch 3, Batch 1050, Loss: 1.4205983877182007\n",
      "Epoch 3, Batch 1060, Loss: 1.1069560050964355\n",
      "Epoch 3, Batch 1070, Loss: 1.0425938367843628\n",
      "Epoch 3, Batch 1080, Loss: 0.9688564538955688\n",
      "Epoch 3, Batch 1090, Loss: 0.9898587465286255\n",
      "Epoch 3, Batch 1100, Loss: 1.2077381610870361\n",
      "Epoch 3, Batch 1110, Loss: 1.1559869050979614\n",
      "Epoch 3, Batch 1120, Loss: 1.224544644355774\n",
      "Epoch 3, Batch 1130, Loss: 1.3650672435760498\n",
      "Epoch 3, Batch 1140, Loss: 1.3738038539886475\n",
      "Epoch 3, Batch 1150, Loss: 1.0512723922729492\n",
      "Epoch 3, Batch 1160, Loss: 1.2859805822372437\n",
      "Epoch 3, Batch 1170, Loss: 1.150904655456543\n",
      "Epoch 3, Batch 1180, Loss: 1.290156364440918\n",
      "Epoch 3, Batch 1190, Loss: 1.184961199760437\n",
      "Epoch 3, Batch 1200, Loss: 1.1300798654556274\n",
      "Epoch 3, Batch 1210, Loss: 1.3332409858703613\n",
      "Epoch 3, Batch 1220, Loss: 1.4199835062026978\n",
      "Epoch 3, Batch 1230, Loss: 1.1121089458465576\n",
      "Epoch 3, Batch 1240, Loss: 0.9514814019203186\n",
      "Epoch 3, Batch 1250, Loss: 1.2296581268310547\n",
      "Epoch 3, Batch 1260, Loss: 1.264129400253296\n",
      "Epoch 3, Batch 1270, Loss: 1.4614390134811401\n",
      "Epoch 3, Batch 1280, Loss: 1.0056085586547852\n",
      "Epoch 3, Batch 1290, Loss: 1.3554103374481201\n",
      "Epoch 3, Batch 1300, Loss: 1.2681154012680054\n",
      "Epoch 3, Batch 1310, Loss: 1.248908281326294\n",
      "Epoch 3, Batch 1320, Loss: 1.1891250610351562\n",
      "Epoch 3, Batch 1330, Loss: 1.16243314743042\n",
      "Epoch 3, Batch 1340, Loss: 1.2075005769729614\n",
      "Epoch 3, Batch 1350, Loss: 1.2741214036941528\n",
      "Epoch 3, Batch 1360, Loss: 1.3382000923156738\n",
      "Epoch 3, Batch 1370, Loss: 1.2698954343795776\n",
      "Epoch 3, Batch 1380, Loss: 1.1979960203170776\n",
      "Epoch 3, Batch 1390, Loss: 1.1608693599700928\n",
      "Epoch 3, Batch 1400, Loss: 1.4467321634292603\n",
      "Epoch 3, Train Loss: 1.2198307947139835, Validation Accuracy: 56.94722111155538%\n",
      "Epoch 4, Batch 0, Loss: 1.288145899772644\n",
      "Epoch 4, Batch 10, Loss: 1.2683448791503906\n",
      "Epoch 4, Batch 20, Loss: 1.183730959892273\n",
      "Epoch 4, Batch 30, Loss: 1.2531267404556274\n",
      "Epoch 4, Batch 40, Loss: 1.1283680200576782\n",
      "Epoch 4, Batch 50, Loss: 1.1144613027572632\n",
      "Epoch 4, Batch 60, Loss: 1.432486653327942\n",
      "Epoch 4, Batch 70, Loss: 1.3500837087631226\n",
      "Epoch 4, Batch 80, Loss: 1.2917484045028687\n",
      "Epoch 4, Batch 90, Loss: 1.1882585287094116\n",
      "Epoch 4, Batch 100, Loss: 1.1764192581176758\n",
      "Epoch 4, Batch 110, Loss: 1.1032580137252808\n",
      "Epoch 4, Batch 120, Loss: 1.1662174463272095\n",
      "Epoch 4, Batch 130, Loss: 1.2516051530838013\n",
      "Epoch 4, Batch 140, Loss: 1.308516025543213\n",
      "Epoch 4, Batch 150, Loss: 1.2041560411453247\n",
      "Epoch 4, Batch 160, Loss: 1.06536865234375\n",
      "Epoch 4, Batch 170, Loss: 1.2639079093933105\n",
      "Epoch 4, Batch 180, Loss: 1.0394703149795532\n",
      "Epoch 4, Batch 190, Loss: 1.053516149520874\n",
      "Epoch 4, Batch 200, Loss: 1.1027764081954956\n",
      "Epoch 4, Batch 210, Loss: 1.1819261312484741\n",
      "Epoch 4, Batch 220, Loss: 1.3125231266021729\n",
      "Epoch 4, Batch 230, Loss: 1.1269736289978027\n",
      "Epoch 4, Batch 240, Loss: 1.0490614175796509\n",
      "Epoch 4, Batch 250, Loss: 1.3176581859588623\n",
      "Epoch 4, Batch 260, Loss: 1.2152130603790283\n",
      "Epoch 4, Batch 270, Loss: 1.066581130027771\n",
      "Epoch 4, Batch 280, Loss: 1.243494987487793\n",
      "Epoch 4, Batch 290, Loss: 1.3570160865783691\n",
      "Epoch 4, Batch 300, Loss: 1.267774224281311\n",
      "Epoch 4, Batch 310, Loss: 1.3194650411605835\n",
      "Epoch 4, Batch 320, Loss: 1.0865757465362549\n",
      "Epoch 4, Batch 330, Loss: 1.3787585496902466\n",
      "Epoch 4, Batch 340, Loss: 1.2578370571136475\n",
      "Epoch 4, Batch 350, Loss: 1.067204236984253\n",
      "Epoch 4, Batch 360, Loss: 0.9423249959945679\n",
      "Epoch 4, Batch 370, Loss: 1.375573992729187\n",
      "Epoch 4, Batch 380, Loss: 1.2819442749023438\n",
      "Epoch 4, Batch 390, Loss: 1.3838791847229004\n",
      "Epoch 4, Batch 400, Loss: 1.2258273363113403\n",
      "Epoch 4, Batch 410, Loss: 1.0553176403045654\n",
      "Epoch 4, Batch 420, Loss: 1.4080913066864014\n",
      "Epoch 4, Batch 430, Loss: 1.0792055130004883\n",
      "Epoch 4, Batch 440, Loss: 1.3473814725875854\n",
      "Epoch 4, Batch 450, Loss: 1.1516658067703247\n",
      "Epoch 4, Batch 460, Loss: 1.2977931499481201\n",
      "Epoch 4, Batch 470, Loss: 1.112134337425232\n",
      "Epoch 4, Batch 480, Loss: 1.1184324026107788\n",
      "Epoch 4, Batch 490, Loss: 1.4202667474746704\n",
      "Epoch 4, Batch 500, Loss: 1.309749960899353\n",
      "Epoch 4, Batch 510, Loss: 1.331878900527954\n",
      "Epoch 4, Batch 520, Loss: 1.3525253534317017\n",
      "Epoch 4, Batch 530, Loss: 1.200739860534668\n",
      "Epoch 4, Batch 540, Loss: 1.001143217086792\n",
      "Epoch 4, Batch 550, Loss: 1.3315784931182861\n",
      "Epoch 4, Batch 560, Loss: 1.0916492938995361\n",
      "Epoch 4, Batch 570, Loss: 1.1557576656341553\n",
      "Epoch 4, Batch 580, Loss: 1.2785894870758057\n",
      "Epoch 4, Batch 590, Loss: 1.1465954780578613\n",
      "Epoch 4, Batch 600, Loss: 1.4237030744552612\n",
      "Epoch 4, Batch 610, Loss: 1.0887696743011475\n",
      "Epoch 4, Batch 620, Loss: 1.2957611083984375\n",
      "Epoch 4, Batch 630, Loss: 0.9478306770324707\n",
      "Epoch 4, Batch 640, Loss: 1.3533592224121094\n",
      "Epoch 4, Batch 650, Loss: 1.2402788400650024\n",
      "Epoch 4, Batch 660, Loss: 1.268584132194519\n",
      "Epoch 4, Batch 670, Loss: 1.135923981666565\n",
      "Epoch 4, Batch 680, Loss: 1.0221517086029053\n",
      "Epoch 4, Batch 690, Loss: 1.1037858724594116\n",
      "Epoch 4, Batch 700, Loss: 1.171434760093689\n",
      "Epoch 4, Batch 710, Loss: 1.2504619359970093\n",
      "Epoch 4, Batch 720, Loss: 1.23808753490448\n",
      "Epoch 4, Batch 730, Loss: 1.2094790935516357\n",
      "Epoch 4, Batch 740, Loss: 1.2118126153945923\n",
      "Epoch 4, Batch 750, Loss: 1.1198158264160156\n",
      "Epoch 4, Batch 760, Loss: 1.1425763368606567\n",
      "Epoch 4, Batch 770, Loss: 1.1039773225784302\n",
      "Epoch 4, Batch 780, Loss: 1.0957565307617188\n",
      "Epoch 4, Batch 790, Loss: 1.2400038242340088\n",
      "Epoch 4, Batch 800, Loss: 0.9905135631561279\n",
      "Epoch 4, Batch 810, Loss: 1.3186354637145996\n",
      "Epoch 4, Batch 820, Loss: 1.4676399230957031\n",
      "Epoch 4, Batch 830, Loss: 1.0741899013519287\n",
      "Epoch 4, Batch 840, Loss: 1.2969059944152832\n",
      "Epoch 4, Batch 850, Loss: 1.2281891107559204\n",
      "Epoch 4, Batch 860, Loss: 1.1654329299926758\n",
      "Epoch 4, Batch 870, Loss: 1.2724027633666992\n",
      "Epoch 4, Batch 880, Loss: 1.1110255718231201\n",
      "Epoch 4, Batch 890, Loss: 0.9908640384674072\n",
      "Epoch 4, Batch 900, Loss: 1.2173019647598267\n",
      "Epoch 4, Batch 910, Loss: 1.0487596988677979\n",
      "Epoch 4, Batch 920, Loss: 1.270766019821167\n",
      "Epoch 4, Batch 930, Loss: 1.1291794776916504\n",
      "Epoch 4, Batch 940, Loss: 1.102399468421936\n",
      "Epoch 4, Batch 950, Loss: 1.1488996744155884\n",
      "Epoch 4, Batch 960, Loss: 1.3569608926773071\n",
      "Epoch 4, Batch 970, Loss: 1.5141642093658447\n",
      "Epoch 4, Batch 980, Loss: 1.0422173738479614\n",
      "Epoch 4, Batch 990, Loss: 1.1864292621612549\n",
      "Epoch 4, Batch 1000, Loss: 1.236599087715149\n",
      "Epoch 4, Batch 1010, Loss: 1.348621129989624\n",
      "Epoch 4, Batch 1020, Loss: 1.1772927045822144\n",
      "Epoch 4, Batch 1030, Loss: 1.345823049545288\n",
      "Epoch 4, Batch 1040, Loss: 1.0724060535430908\n",
      "Epoch 4, Batch 1050, Loss: 1.1993852853775024\n",
      "Epoch 4, Batch 1060, Loss: 1.2651057243347168\n",
      "Epoch 4, Batch 1070, Loss: 1.175125002861023\n",
      "Epoch 4, Batch 1080, Loss: 1.4899309873580933\n",
      "Epoch 4, Batch 1090, Loss: 1.273317813873291\n",
      "Epoch 4, Batch 1100, Loss: 1.2155797481536865\n",
      "Epoch 4, Batch 1110, Loss: 1.0624573230743408\n",
      "Epoch 4, Batch 1120, Loss: 1.1690140962600708\n",
      "Epoch 4, Batch 1130, Loss: 1.1204246282577515\n",
      "Epoch 4, Batch 1140, Loss: 1.1983270645141602\n",
      "Epoch 4, Batch 1150, Loss: 1.1454859972000122\n",
      "Epoch 4, Batch 1160, Loss: 1.0983715057373047\n",
      "Epoch 4, Batch 1170, Loss: 1.1762787103652954\n",
      "Epoch 4, Batch 1180, Loss: 1.282067060470581\n",
      "Epoch 4, Batch 1190, Loss: 1.5212929248809814\n",
      "Epoch 4, Batch 1200, Loss: 1.2424075603485107\n",
      "Epoch 4, Batch 1210, Loss: 1.1583521366119385\n",
      "Epoch 4, Batch 1220, Loss: 1.1788057088851929\n",
      "Epoch 4, Batch 1230, Loss: 1.2068984508514404\n",
      "Epoch 4, Batch 1240, Loss: 1.1317416429519653\n",
      "Epoch 4, Batch 1250, Loss: 1.4776405096054077\n",
      "Epoch 4, Batch 1260, Loss: 1.0989900827407837\n",
      "Epoch 4, Batch 1270, Loss: 1.1498900651931763\n",
      "Epoch 4, Batch 1280, Loss: 1.3710381984710693\n",
      "Epoch 4, Batch 1290, Loss: 1.2916748523712158\n",
      "Epoch 4, Batch 1300, Loss: 1.2767820358276367\n",
      "Epoch 4, Batch 1310, Loss: 1.1819653511047363\n",
      "Epoch 4, Batch 1320, Loss: 1.3204885721206665\n",
      "Epoch 4, Batch 1330, Loss: 1.5035258531570435\n",
      "Epoch 4, Batch 1340, Loss: 1.048632264137268\n",
      "Epoch 4, Batch 1350, Loss: 1.4378089904785156\n",
      "Epoch 4, Batch 1360, Loss: 1.207465648651123\n",
      "Epoch 4, Batch 1370, Loss: 1.4238237142562866\n",
      "Epoch 4, Batch 1380, Loss: 1.2495815753936768\n",
      "Epoch 4, Batch 1390, Loss: 1.2779531478881836\n",
      "Epoch 4, Batch 1400, Loss: 0.9939911365509033\n",
      "Epoch 4, Train Loss: 1.2147100135292157, Validation Accuracy: 56.68732506997201%\n",
      "Epoch 5, Batch 0, Loss: 1.2781906127929688\n",
      "Epoch 5, Batch 10, Loss: 1.287681221961975\n",
      "Epoch 5, Batch 20, Loss: 1.509110689163208\n",
      "Epoch 5, Batch 30, Loss: 1.1639230251312256\n",
      "Epoch 5, Batch 40, Loss: 1.14323091506958\n",
      "Epoch 5, Batch 50, Loss: 1.168544888496399\n",
      "Epoch 5, Batch 60, Loss: 1.2112245559692383\n",
      "Epoch 5, Batch 70, Loss: 1.543303370475769\n",
      "Epoch 5, Batch 80, Loss: 1.2037837505340576\n",
      "Epoch 5, Batch 90, Loss: 1.284947156906128\n",
      "Epoch 5, Batch 100, Loss: 1.1294710636138916\n",
      "Epoch 5, Batch 110, Loss: 1.2740635871887207\n",
      "Epoch 5, Batch 120, Loss: 1.2861874103546143\n",
      "Epoch 5, Batch 130, Loss: 0.9931680560112\n",
      "Epoch 5, Batch 140, Loss: 1.2783472537994385\n",
      "Epoch 5, Batch 150, Loss: 1.1941224336624146\n",
      "Epoch 5, Batch 160, Loss: 1.2414662837982178\n",
      "Epoch 5, Batch 170, Loss: 1.1162028312683105\n",
      "Epoch 5, Batch 180, Loss: 1.06173574924469\n",
      "Epoch 5, Batch 190, Loss: 1.227009654045105\n",
      "Epoch 5, Batch 200, Loss: 1.4946328401565552\n",
      "Epoch 5, Batch 210, Loss: 1.1208515167236328\n",
      "Epoch 5, Batch 220, Loss: 1.2606163024902344\n",
      "Epoch 5, Batch 230, Loss: 1.183118224143982\n",
      "Epoch 5, Batch 240, Loss: 1.242362380027771\n",
      "Epoch 5, Batch 250, Loss: 1.2791409492492676\n",
      "Epoch 5, Batch 260, Loss: 1.2715790271759033\n",
      "Epoch 5, Batch 270, Loss: 1.1889994144439697\n",
      "Epoch 5, Batch 280, Loss: 1.0939903259277344\n",
      "Epoch 5, Batch 290, Loss: 1.2196054458618164\n",
      "Epoch 5, Batch 300, Loss: 1.1877373456954956\n",
      "Epoch 5, Batch 310, Loss: 1.2111907005310059\n",
      "Epoch 5, Batch 320, Loss: 1.284570336341858\n",
      "Epoch 5, Batch 330, Loss: 1.1850016117095947\n",
      "Epoch 5, Batch 340, Loss: 1.3844757080078125\n",
      "Epoch 5, Batch 350, Loss: 1.364180564880371\n",
      "Epoch 5, Batch 360, Loss: 1.1933009624481201\n",
      "Epoch 5, Batch 370, Loss: 1.0698456764221191\n",
      "Epoch 5, Batch 380, Loss: 1.2311406135559082\n",
      "Epoch 5, Batch 390, Loss: 1.25987708568573\n",
      "Epoch 5, Batch 400, Loss: 1.1972354650497437\n",
      "Epoch 5, Batch 410, Loss: 1.2302043437957764\n",
      "Epoch 5, Batch 420, Loss: 1.402577519416809\n",
      "Epoch 5, Batch 430, Loss: 1.3474905490875244\n",
      "Epoch 5, Batch 440, Loss: 1.1600148677825928\n",
      "Epoch 5, Batch 450, Loss: 1.2724639177322388\n",
      "Epoch 5, Batch 460, Loss: 1.4224275350570679\n",
      "Epoch 5, Batch 470, Loss: 1.0434017181396484\n",
      "Epoch 5, Batch 480, Loss: 1.0671675205230713\n",
      "Epoch 5, Batch 490, Loss: 1.1530612707138062\n",
      "Epoch 5, Batch 500, Loss: 1.204230785369873\n",
      "Epoch 5, Batch 510, Loss: 1.471971035003662\n",
      "Epoch 5, Batch 520, Loss: 1.027910590171814\n",
      "Epoch 5, Batch 530, Loss: 1.375035285949707\n",
      "Epoch 5, Batch 540, Loss: 1.5991995334625244\n",
      "Epoch 5, Batch 550, Loss: 1.3458294868469238\n",
      "Epoch 5, Batch 560, Loss: 1.2209910154342651\n",
      "Epoch 5, Batch 570, Loss: 1.203075647354126\n",
      "Epoch 5, Batch 580, Loss: 1.4087636470794678\n",
      "Epoch 5, Batch 590, Loss: 1.0053629875183105\n",
      "Epoch 5, Batch 600, Loss: 1.0947470664978027\n",
      "Epoch 5, Batch 610, Loss: 1.379714846611023\n",
      "Epoch 5, Batch 620, Loss: 1.1996322870254517\n",
      "Epoch 5, Batch 630, Loss: 1.160410761833191\n",
      "Epoch 5, Batch 640, Loss: 1.246748685836792\n",
      "Epoch 5, Batch 650, Loss: 1.2241981029510498\n",
      "Epoch 5, Batch 660, Loss: 1.5117688179016113\n",
      "Epoch 5, Batch 670, Loss: 1.1179516315460205\n",
      "Epoch 5, Batch 680, Loss: 1.222182273864746\n",
      "Epoch 5, Batch 690, Loss: 1.3587895631790161\n",
      "Epoch 5, Batch 700, Loss: 1.0391860008239746\n",
      "Epoch 5, Batch 710, Loss: 1.2036277055740356\n",
      "Epoch 5, Batch 720, Loss: 1.0941177606582642\n",
      "Epoch 5, Batch 730, Loss: 1.0578174591064453\n",
      "Epoch 5, Batch 740, Loss: 1.320962905883789\n",
      "Epoch 5, Batch 750, Loss: 1.3295096158981323\n",
      "Epoch 5, Batch 760, Loss: 1.179040789604187\n",
      "Epoch 5, Batch 770, Loss: 1.1629499197006226\n",
      "Epoch 5, Batch 780, Loss: 1.2107365131378174\n",
      "Epoch 5, Batch 790, Loss: 1.1862115859985352\n",
      "Epoch 5, Batch 800, Loss: 1.2565770149230957\n",
      "Epoch 5, Batch 810, Loss: 1.2894269227981567\n",
      "Epoch 5, Batch 820, Loss: 1.0519607067108154\n",
      "Epoch 5, Batch 830, Loss: 1.298469066619873\n",
      "Epoch 5, Batch 840, Loss: 1.212891936302185\n",
      "Epoch 5, Batch 850, Loss: 1.0092116594314575\n",
      "Epoch 5, Batch 860, Loss: 1.3580455780029297\n",
      "Epoch 5, Batch 870, Loss: 1.11958646774292\n",
      "Epoch 5, Batch 880, Loss: 1.0819960832595825\n",
      "Epoch 5, Batch 890, Loss: 1.1086891889572144\n",
      "Epoch 5, Batch 900, Loss: 0.9260991811752319\n",
      "Epoch 5, Batch 910, Loss: 1.1988681554794312\n",
      "Epoch 5, Batch 920, Loss: 0.971260130405426\n",
      "Epoch 5, Batch 930, Loss: 1.1579843759536743\n",
      "Epoch 5, Batch 940, Loss: 1.3548483848571777\n",
      "Epoch 5, Batch 950, Loss: 1.3890397548675537\n",
      "Epoch 5, Batch 960, Loss: 1.2998830080032349\n",
      "Epoch 5, Batch 970, Loss: 1.1333489418029785\n",
      "Epoch 5, Batch 980, Loss: 1.1399848461151123\n",
      "Epoch 5, Batch 990, Loss: 1.2097774744033813\n",
      "Epoch 5, Batch 1000, Loss: 1.1056169271469116\n",
      "Epoch 5, Batch 1010, Loss: 1.2356226444244385\n",
      "Epoch 5, Batch 1020, Loss: 1.2693232297897339\n",
      "Epoch 5, Batch 1030, Loss: 1.177817702293396\n",
      "Epoch 5, Batch 1040, Loss: 0.8899661898612976\n",
      "Epoch 5, Batch 1050, Loss: 1.308650255203247\n",
      "Epoch 5, Batch 1060, Loss: 1.2575997114181519\n",
      "Epoch 5, Batch 1070, Loss: 1.153804898262024\n",
      "Epoch 5, Batch 1080, Loss: 1.25985848903656\n",
      "Epoch 5, Batch 1090, Loss: 1.1232798099517822\n",
      "Epoch 5, Batch 1100, Loss: 1.2150405645370483\n",
      "Epoch 5, Batch 1110, Loss: 0.9884248375892639\n",
      "Epoch 5, Batch 1120, Loss: 1.4090523719787598\n",
      "Epoch 5, Batch 1130, Loss: 1.2902936935424805\n",
      "Epoch 5, Batch 1140, Loss: 1.0351500511169434\n",
      "Epoch 5, Batch 1150, Loss: 1.1687968969345093\n",
      "Epoch 5, Batch 1160, Loss: 1.1808573007583618\n",
      "Epoch 5, Batch 1170, Loss: 1.1756235361099243\n",
      "Epoch 5, Batch 1180, Loss: 1.2744288444519043\n",
      "Epoch 5, Batch 1190, Loss: 1.0775779485702515\n",
      "Epoch 5, Batch 1200, Loss: 1.3564698696136475\n",
      "Epoch 5, Batch 1210, Loss: 1.083174467086792\n",
      "Epoch 5, Batch 1220, Loss: 1.3104422092437744\n",
      "Epoch 5, Batch 1230, Loss: 1.3135576248168945\n",
      "Epoch 5, Batch 1240, Loss: 1.175797939300537\n",
      "Epoch 5, Batch 1250, Loss: 1.2526131868362427\n",
      "Epoch 5, Batch 1260, Loss: 1.1287020444869995\n",
      "Epoch 5, Batch 1270, Loss: 1.418466567993164\n",
      "Epoch 5, Batch 1280, Loss: 1.3582555055618286\n",
      "Epoch 5, Batch 1290, Loss: 1.1491094827651978\n",
      "Epoch 5, Batch 1300, Loss: 1.488759160041809\n",
      "Epoch 5, Batch 1310, Loss: 1.1197820901870728\n",
      "Epoch 5, Batch 1320, Loss: 1.042478084564209\n",
      "Epoch 5, Batch 1330, Loss: 0.9549488425254822\n",
      "Epoch 5, Batch 1340, Loss: 1.6712632179260254\n",
      "Epoch 5, Batch 1350, Loss: 1.198007583618164\n",
      "Epoch 5, Batch 1360, Loss: 1.1537156105041504\n",
      "Epoch 5, Batch 1370, Loss: 1.268633484840393\n",
      "Epoch 5, Batch 1380, Loss: 1.2023648023605347\n",
      "Epoch 5, Batch 1390, Loss: 1.1994599103927612\n",
      "Epoch 5, Batch 1400, Loss: 1.1291245222091675\n",
      "Epoch 5, Train Loss: 1.2117633989879064, Validation Accuracy: 56.84726109556178%\n",
      "Epoch 6, Batch 0, Loss: 1.4796628952026367\n",
      "Epoch 6, Batch 10, Loss: 1.1040611267089844\n",
      "Epoch 6, Batch 20, Loss: 1.1065828800201416\n",
      "Epoch 6, Batch 30, Loss: 1.466059923171997\n",
      "Epoch 6, Batch 40, Loss: 0.9758141040802002\n",
      "Epoch 6, Batch 50, Loss: 1.2346612215042114\n",
      "Epoch 6, Batch 60, Loss: 1.454542636871338\n",
      "Epoch 6, Batch 70, Loss: 1.3867608308792114\n",
      "Epoch 6, Batch 80, Loss: 1.1213096380233765\n",
      "Epoch 6, Batch 90, Loss: 1.2565990686416626\n",
      "Epoch 6, Batch 100, Loss: 1.261291265487671\n",
      "Epoch 6, Batch 110, Loss: 1.2563061714172363\n",
      "Epoch 6, Batch 120, Loss: 1.2764010429382324\n",
      "Epoch 6, Batch 130, Loss: 1.1054389476776123\n",
      "Epoch 6, Batch 140, Loss: 1.5745816230773926\n",
      "Epoch 6, Batch 150, Loss: 1.2595444917678833\n",
      "Epoch 6, Batch 160, Loss: 1.3438714742660522\n",
      "Epoch 6, Batch 170, Loss: 1.4312541484832764\n",
      "Epoch 6, Batch 180, Loss: 1.2218060493469238\n",
      "Epoch 6, Batch 190, Loss: 0.9587757587432861\n",
      "Epoch 6, Batch 200, Loss: 1.2991032600402832\n",
      "Epoch 6, Batch 210, Loss: 1.1878126859664917\n",
      "Epoch 6, Batch 220, Loss: 1.1678519248962402\n",
      "Epoch 6, Batch 230, Loss: 1.1408045291900635\n",
      "Epoch 6, Batch 240, Loss: 1.375933289527893\n",
      "Epoch 6, Batch 250, Loss: 1.3712897300720215\n",
      "Epoch 6, Batch 260, Loss: 1.1030548810958862\n",
      "Epoch 6, Batch 270, Loss: 1.2176638841629028\n",
      "Epoch 6, Batch 280, Loss: 1.1225898265838623\n",
      "Epoch 6, Batch 290, Loss: 1.0042988061904907\n",
      "Epoch 6, Batch 300, Loss: 1.178453803062439\n",
      "Epoch 6, Batch 310, Loss: 1.153114676475525\n",
      "Epoch 6, Batch 320, Loss: 1.3337876796722412\n",
      "Epoch 6, Batch 330, Loss: 1.053387999534607\n",
      "Epoch 6, Batch 340, Loss: 1.2396976947784424\n",
      "Epoch 6, Batch 350, Loss: 1.0396839380264282\n",
      "Epoch 6, Batch 360, Loss: 1.127256989479065\n",
      "Epoch 6, Batch 370, Loss: 1.1867847442626953\n",
      "Epoch 6, Batch 380, Loss: 1.2569284439086914\n",
      "Epoch 6, Batch 390, Loss: 1.0613232851028442\n",
      "Epoch 6, Batch 400, Loss: 1.38033926486969\n",
      "Epoch 6, Batch 410, Loss: 1.375125527381897\n",
      "Epoch 6, Batch 420, Loss: 1.2144542932510376\n",
      "Epoch 6, Batch 430, Loss: 1.1143258810043335\n",
      "Epoch 6, Batch 440, Loss: 1.2261407375335693\n",
      "Epoch 6, Batch 450, Loss: 0.8194739818572998\n",
      "Epoch 6, Batch 460, Loss: 1.3153597116470337\n",
      "Epoch 6, Batch 470, Loss: 1.0232285261154175\n",
      "Epoch 6, Batch 480, Loss: 1.517368197441101\n",
      "Epoch 6, Batch 490, Loss: 0.9733690619468689\n",
      "Epoch 6, Batch 500, Loss: 1.2436940670013428\n",
      "Epoch 6, Batch 510, Loss: 1.004204273223877\n",
      "Epoch 6, Batch 520, Loss: 1.0661660432815552\n",
      "Epoch 6, Batch 530, Loss: 1.3042529821395874\n",
      "Epoch 6, Batch 540, Loss: 1.1811234951019287\n",
      "Epoch 6, Batch 550, Loss: 1.2615933418273926\n",
      "Epoch 6, Batch 560, Loss: 1.2775970697402954\n",
      "Epoch 6, Batch 570, Loss: 1.276597261428833\n",
      "Epoch 6, Batch 580, Loss: 1.0119532346725464\n",
      "Epoch 6, Batch 590, Loss: 1.2369366884231567\n",
      "Epoch 6, Batch 600, Loss: 1.1668336391448975\n",
      "Epoch 6, Batch 610, Loss: 1.48126220703125\n",
      "Epoch 6, Batch 620, Loss: 1.0567649602890015\n",
      "Epoch 6, Batch 630, Loss: 1.2668719291687012\n",
      "Epoch 6, Batch 640, Loss: 1.2211754322052002\n",
      "Epoch 6, Batch 650, Loss: 1.2616580724716187\n",
      "Epoch 6, Batch 660, Loss: 1.3706427812576294\n",
      "Epoch 6, Batch 670, Loss: 1.322476863861084\n",
      "Epoch 6, Batch 680, Loss: 1.1504915952682495\n",
      "Epoch 6, Batch 690, Loss: 1.136304259300232\n",
      "Epoch 6, Batch 700, Loss: 1.1752983331680298\n",
      "Epoch 6, Batch 710, Loss: 1.2994848489761353\n",
      "Epoch 6, Batch 720, Loss: 1.4109915494918823\n",
      "Epoch 6, Batch 730, Loss: 1.1950623989105225\n",
      "Epoch 6, Batch 740, Loss: 1.2853354215621948\n",
      "Epoch 6, Batch 750, Loss: 1.0507657527923584\n",
      "Epoch 6, Batch 760, Loss: 1.1910902261734009\n",
      "Epoch 6, Batch 770, Loss: 1.3153566122055054\n",
      "Epoch 6, Batch 780, Loss: 1.0474895238876343\n",
      "Epoch 6, Batch 790, Loss: 1.2346811294555664\n",
      "Epoch 6, Batch 800, Loss: 1.1457072496414185\n",
      "Epoch 6, Batch 810, Loss: 1.3132719993591309\n",
      "Epoch 6, Batch 820, Loss: 1.2125657796859741\n",
      "Epoch 6, Batch 830, Loss: 1.2658182382583618\n",
      "Epoch 6, Batch 840, Loss: 1.2146728038787842\n",
      "Epoch 6, Batch 850, Loss: 1.3925293684005737\n",
      "Epoch 6, Batch 860, Loss: 1.1510800123214722\n",
      "Epoch 6, Batch 870, Loss: 1.3494703769683838\n",
      "Epoch 6, Batch 880, Loss: 1.1816033124923706\n",
      "Epoch 6, Batch 890, Loss: 1.278243899345398\n",
      "Epoch 6, Batch 900, Loss: 1.1306216716766357\n",
      "Epoch 6, Batch 910, Loss: 1.2973471879959106\n",
      "Epoch 6, Batch 920, Loss: 1.1270588636398315\n",
      "Epoch 6, Batch 930, Loss: 1.3950144052505493\n",
      "Epoch 6, Batch 940, Loss: 1.1488780975341797\n",
      "Epoch 6, Batch 950, Loss: 1.114277720451355\n",
      "Epoch 6, Batch 960, Loss: 1.346820592880249\n",
      "Epoch 6, Batch 970, Loss: 1.3814105987548828\n",
      "Epoch 6, Batch 980, Loss: 1.013588547706604\n",
      "Epoch 6, Batch 990, Loss: 1.1134051084518433\n",
      "Epoch 6, Batch 1000, Loss: 1.2101213932037354\n",
      "Epoch 6, Batch 1010, Loss: 1.2952244281768799\n",
      "Epoch 6, Batch 1020, Loss: 1.2883193492889404\n",
      "Epoch 6, Batch 1030, Loss: 1.0159412622451782\n",
      "Epoch 6, Batch 1040, Loss: 1.0782246589660645\n",
      "Epoch 6, Batch 1050, Loss: 1.160153865814209\n",
      "Epoch 6, Batch 1060, Loss: 1.080865740776062\n",
      "Epoch 6, Batch 1070, Loss: 1.212451696395874\n",
      "Epoch 6, Batch 1080, Loss: 1.241011142730713\n",
      "Epoch 6, Batch 1090, Loss: 1.3528664112091064\n",
      "Epoch 6, Batch 1100, Loss: 1.1296579837799072\n",
      "Epoch 6, Batch 1110, Loss: 1.2628051042556763\n",
      "Epoch 6, Batch 1120, Loss: 1.2150553464889526\n",
      "Epoch 6, Batch 1130, Loss: 1.3417290449142456\n",
      "Epoch 6, Batch 1140, Loss: 1.2750107049942017\n",
      "Epoch 6, Batch 1150, Loss: 1.2500163316726685\n",
      "Epoch 6, Batch 1160, Loss: 1.1839110851287842\n",
      "Epoch 6, Batch 1170, Loss: 1.1768031120300293\n",
      "Epoch 6, Batch 1180, Loss: 1.1733886003494263\n",
      "Epoch 6, Batch 1190, Loss: 1.3426226377487183\n",
      "Epoch 6, Batch 1200, Loss: 1.1318618059158325\n",
      "Epoch 6, Batch 1210, Loss: 1.1716374158859253\n",
      "Epoch 6, Batch 1220, Loss: 1.2879406213760376\n",
      "Epoch 6, Batch 1230, Loss: 1.0869412422180176\n",
      "Epoch 6, Batch 1240, Loss: 1.0385797023773193\n",
      "Epoch 6, Batch 1250, Loss: 1.1492892503738403\n",
      "Epoch 6, Batch 1260, Loss: 1.1585032939910889\n",
      "Epoch 6, Batch 1270, Loss: 1.1222574710845947\n",
      "Epoch 6, Batch 1280, Loss: 1.3673498630523682\n",
      "Epoch 6, Batch 1290, Loss: 1.31735098361969\n",
      "Epoch 6, Batch 1300, Loss: 1.0297096967697144\n",
      "Epoch 6, Batch 1310, Loss: 1.293867588043213\n",
      "Epoch 6, Batch 1320, Loss: 1.036988615989685\n",
      "Epoch 6, Batch 1330, Loss: 1.1026036739349365\n",
      "Epoch 6, Batch 1340, Loss: 1.1954762935638428\n",
      "Epoch 6, Batch 1350, Loss: 1.2959206104278564\n",
      "Epoch 6, Batch 1360, Loss: 1.4296835660934448\n",
      "Epoch 6, Batch 1370, Loss: 1.3260473012924194\n",
      "Epoch 6, Batch 1380, Loss: 1.3663394451141357\n",
      "Epoch 6, Batch 1390, Loss: 1.2390694618225098\n",
      "Epoch 6, Batch 1400, Loss: 1.2421406507492065\n",
      "Epoch 6, Train Loss: 1.2109017065423722, Validation Accuracy: 56.69732107157137%\n",
      "Epoch 7, Batch 0, Loss: 1.2086765766143799\n",
      "Epoch 7, Batch 10, Loss: 1.4573330879211426\n",
      "Epoch 7, Batch 20, Loss: 1.240979790687561\n",
      "Epoch 7, Batch 30, Loss: 1.2169114351272583\n",
      "Epoch 7, Batch 40, Loss: 1.225937008857727\n",
      "Epoch 7, Batch 50, Loss: 1.2296435832977295\n",
      "Epoch 7, Batch 60, Loss: 1.1510568857192993\n",
      "Epoch 7, Batch 70, Loss: 1.172187089920044\n",
      "Epoch 7, Batch 80, Loss: 1.0715699195861816\n",
      "Epoch 7, Batch 90, Loss: 1.1032905578613281\n",
      "Epoch 7, Batch 100, Loss: 1.2446300983428955\n",
      "Epoch 7, Batch 110, Loss: 1.343666911125183\n",
      "Epoch 7, Batch 120, Loss: 1.2456246614456177\n",
      "Epoch 7, Batch 130, Loss: 1.257036805152893\n",
      "Epoch 7, Batch 140, Loss: 1.127814769744873\n",
      "Epoch 7, Batch 150, Loss: 1.2118202447891235\n",
      "Epoch 7, Batch 160, Loss: 1.0368560552597046\n",
      "Epoch 7, Batch 170, Loss: 1.3152307271957397\n",
      "Epoch 7, Batch 180, Loss: 1.2314956188201904\n",
      "Epoch 7, Batch 190, Loss: 1.1335768699645996\n",
      "Epoch 7, Batch 200, Loss: 1.0819908380508423\n",
      "Epoch 7, Batch 210, Loss: 1.1449612379074097\n",
      "Epoch 7, Batch 220, Loss: 1.4888232946395874\n",
      "Epoch 7, Batch 230, Loss: 1.147271752357483\n",
      "Epoch 7, Batch 240, Loss: 1.2139434814453125\n",
      "Epoch 7, Batch 250, Loss: 1.2309186458587646\n",
      "Epoch 7, Batch 260, Loss: 0.9990656971931458\n",
      "Epoch 7, Batch 270, Loss: 1.0660704374313354\n",
      "Epoch 7, Batch 280, Loss: 1.0752763748168945\n",
      "Epoch 7, Batch 290, Loss: 1.1963714361190796\n",
      "Epoch 7, Batch 300, Loss: 1.0757335424423218\n",
      "Epoch 7, Batch 310, Loss: 1.1266759634017944\n",
      "Epoch 7, Batch 320, Loss: 1.3588407039642334\n",
      "Epoch 7, Batch 330, Loss: 0.9434998035430908\n",
      "Epoch 7, Batch 340, Loss: 1.3973000049591064\n",
      "Epoch 7, Batch 350, Loss: 1.333520770072937\n",
      "Epoch 7, Batch 360, Loss: 1.1369147300720215\n",
      "Epoch 7, Batch 370, Loss: 1.2158772945404053\n",
      "Epoch 7, Batch 380, Loss: 1.1892725229263306\n",
      "Epoch 7, Batch 390, Loss: 1.173828363418579\n",
      "Epoch 7, Batch 400, Loss: 1.0949000120162964\n",
      "Epoch 7, Batch 410, Loss: 1.2937206029891968\n",
      "Epoch 7, Batch 420, Loss: 1.195634365081787\n",
      "Epoch 7, Batch 430, Loss: 1.0540990829467773\n",
      "Epoch 7, Batch 440, Loss: 1.2923074960708618\n",
      "Epoch 7, Batch 450, Loss: 1.2422353029251099\n",
      "Epoch 7, Batch 460, Loss: 1.1860346794128418\n",
      "Epoch 7, Batch 470, Loss: 1.0182080268859863\n",
      "Epoch 7, Batch 480, Loss: 1.2782690525054932\n",
      "Epoch 7, Batch 490, Loss: 1.1820204257965088\n",
      "Epoch 7, Batch 500, Loss: 0.9683274626731873\n",
      "Epoch 7, Batch 510, Loss: 1.008225440979004\n",
      "Epoch 7, Batch 520, Loss: 1.1163908243179321\n",
      "Epoch 7, Batch 530, Loss: 1.0303350687026978\n",
      "Epoch 7, Batch 540, Loss: 1.250704050064087\n",
      "Epoch 7, Batch 550, Loss: 1.1806704998016357\n",
      "Epoch 7, Batch 560, Loss: 1.3123629093170166\n",
      "Epoch 7, Batch 570, Loss: 1.1071559190750122\n",
      "Epoch 7, Batch 580, Loss: 1.1205536127090454\n",
      "Epoch 7, Batch 590, Loss: 1.314351201057434\n",
      "Epoch 7, Batch 600, Loss: 1.15547513961792\n",
      "Epoch 7, Batch 610, Loss: 1.1957697868347168\n",
      "Epoch 7, Batch 620, Loss: 1.2031549215316772\n",
      "Epoch 7, Batch 630, Loss: 1.0530822277069092\n",
      "Epoch 7, Batch 640, Loss: 1.1081390380859375\n",
      "Epoch 7, Batch 650, Loss: 1.140536904335022\n",
      "Epoch 7, Batch 660, Loss: 0.9510577917098999\n",
      "Epoch 7, Batch 670, Loss: 1.064253807067871\n",
      "Epoch 7, Batch 680, Loss: 1.3969577550888062\n",
      "Epoch 7, Batch 690, Loss: 1.3341403007507324\n",
      "Epoch 7, Batch 700, Loss: 1.2581276893615723\n",
      "Epoch 7, Batch 710, Loss: 1.1728408336639404\n",
      "Epoch 7, Batch 720, Loss: 1.2903141975402832\n",
      "Epoch 7, Batch 730, Loss: 1.2532106637954712\n",
      "Epoch 7, Batch 740, Loss: 1.0855432748794556\n",
      "Epoch 7, Batch 750, Loss: 1.3096667528152466\n",
      "Epoch 7, Batch 760, Loss: 1.4047757387161255\n",
      "Epoch 7, Batch 770, Loss: 1.1201989650726318\n",
      "Epoch 7, Batch 780, Loss: 1.4513604640960693\n",
      "Epoch 7, Batch 790, Loss: 1.3559776544570923\n",
      "Epoch 7, Batch 800, Loss: 1.2006860971450806\n",
      "Epoch 7, Batch 810, Loss: 1.2653915882110596\n",
      "Epoch 7, Batch 820, Loss: 1.3303329944610596\n",
      "Epoch 7, Batch 830, Loss: 1.2721283435821533\n",
      "Epoch 7, Batch 840, Loss: 1.164784550666809\n",
      "Epoch 7, Batch 850, Loss: 1.2298696041107178\n",
      "Epoch 7, Batch 860, Loss: 1.1353908777236938\n",
      "Epoch 7, Batch 870, Loss: 1.1572164297103882\n",
      "Epoch 7, Batch 880, Loss: 1.0961724519729614\n",
      "Epoch 7, Batch 890, Loss: 1.146679162979126\n",
      "Epoch 7, Batch 900, Loss: 1.043291449546814\n",
      "Epoch 7, Batch 910, Loss: 1.085834264755249\n",
      "Epoch 7, Batch 920, Loss: 1.2910499572753906\n",
      "Epoch 7, Batch 930, Loss: 0.9857465624809265\n",
      "Epoch 7, Batch 940, Loss: 1.362265944480896\n",
      "Epoch 7, Batch 950, Loss: 1.3315798044204712\n",
      "Epoch 7, Batch 960, Loss: 1.2224324941635132\n",
      "Epoch 7, Batch 970, Loss: 1.3948702812194824\n",
      "Epoch 7, Batch 980, Loss: 1.3786810636520386\n",
      "Epoch 7, Batch 990, Loss: 1.0997610092163086\n",
      "Epoch 7, Batch 1000, Loss: 1.2233307361602783\n",
      "Epoch 7, Batch 1010, Loss: 1.3540390729904175\n",
      "Epoch 7, Batch 1020, Loss: 1.1674782037734985\n",
      "Epoch 7, Batch 1030, Loss: 1.2912662029266357\n",
      "Epoch 7, Batch 1040, Loss: 1.267095685005188\n",
      "Epoch 7, Batch 1050, Loss: 1.2775620222091675\n",
      "Epoch 7, Batch 1060, Loss: 1.2919878959655762\n",
      "Epoch 7, Batch 1070, Loss: 1.1622459888458252\n",
      "Epoch 7, Batch 1080, Loss: 1.2496857643127441\n",
      "Epoch 7, Batch 1090, Loss: 0.9696031808853149\n",
      "Epoch 7, Batch 1100, Loss: 1.1090307235717773\n",
      "Epoch 7, Batch 1110, Loss: 1.072434902191162\n",
      "Epoch 7, Batch 1120, Loss: 1.2558144330978394\n",
      "Epoch 7, Batch 1130, Loss: 1.271788477897644\n",
      "Epoch 7, Batch 1140, Loss: 1.047407865524292\n",
      "Epoch 7, Batch 1150, Loss: 1.0348401069641113\n",
      "Epoch 7, Batch 1160, Loss: 0.9440520405769348\n",
      "Epoch 7, Batch 1170, Loss: 1.275881290435791\n",
      "Epoch 7, Batch 1180, Loss: 0.8519983887672424\n",
      "Epoch 7, Batch 1190, Loss: 1.2738028764724731\n",
      "Epoch 7, Batch 1200, Loss: 1.357069492340088\n",
      "Epoch 7, Batch 1210, Loss: 1.0211988687515259\n",
      "Epoch 7, Batch 1220, Loss: 1.043503761291504\n",
      "Epoch 7, Batch 1230, Loss: 1.187569260597229\n",
      "Epoch 7, Batch 1240, Loss: 1.338133692741394\n",
      "Epoch 7, Batch 1250, Loss: 1.259185791015625\n",
      "Epoch 7, Batch 1260, Loss: 1.1985102891921997\n",
      "Epoch 7, Batch 1270, Loss: 1.279125452041626\n",
      "Epoch 7, Batch 1280, Loss: 1.4575573205947876\n",
      "Epoch 7, Batch 1290, Loss: 1.2695006132125854\n",
      "Epoch 7, Batch 1300, Loss: 1.2149720191955566\n",
      "Epoch 7, Batch 1310, Loss: 1.4365289211273193\n",
      "Epoch 7, Batch 1320, Loss: 1.1079028844833374\n",
      "Epoch 7, Batch 1330, Loss: 1.0332673788070679\n",
      "Epoch 7, Batch 1340, Loss: 0.9885343313217163\n",
      "Epoch 7, Batch 1350, Loss: 1.31195068359375\n",
      "Epoch 7, Batch 1360, Loss: 1.3131884336471558\n",
      "Epoch 7, Batch 1370, Loss: 1.0798275470733643\n",
      "Epoch 7, Batch 1380, Loss: 1.0818144083023071\n",
      "Epoch 7, Batch 1390, Loss: 1.2043907642364502\n",
      "Epoch 7, Batch 1400, Loss: 1.2679224014282227\n",
      "Epoch 7, Train Loss: 1.207955261706967, Validation Accuracy: 56.32746901239504%\n",
      "Epoch 8, Batch 0, Loss: 1.130958080291748\n",
      "Epoch 8, Batch 10, Loss: 1.0450310707092285\n",
      "Epoch 8, Batch 20, Loss: 1.2338827848434448\n",
      "Epoch 8, Batch 30, Loss: 0.9083669185638428\n",
      "Epoch 8, Batch 40, Loss: 1.3075097799301147\n",
      "Epoch 8, Batch 50, Loss: 1.301047921180725\n",
      "Epoch 8, Batch 60, Loss: 1.210378646850586\n",
      "Epoch 8, Batch 70, Loss: 1.1229232549667358\n",
      "Epoch 8, Batch 80, Loss: 1.360474705696106\n",
      "Epoch 8, Batch 90, Loss: 1.1667248010635376\n",
      "Epoch 8, Batch 100, Loss: 1.2076778411865234\n",
      "Epoch 8, Batch 110, Loss: 1.423232913017273\n",
      "Epoch 8, Batch 120, Loss: 1.1540414094924927\n",
      "Epoch 8, Batch 130, Loss: 1.255295991897583\n",
      "Epoch 8, Batch 140, Loss: 1.2646280527114868\n",
      "Epoch 8, Batch 150, Loss: 1.1722686290740967\n",
      "Epoch 8, Batch 160, Loss: 0.9979270696640015\n",
      "Epoch 8, Batch 170, Loss: 1.246884822845459\n",
      "Epoch 8, Batch 180, Loss: 1.0824131965637207\n",
      "Epoch 8, Batch 190, Loss: 1.3858329057693481\n",
      "Epoch 8, Batch 200, Loss: 1.080474853515625\n",
      "Epoch 8, Batch 210, Loss: 1.0744067430496216\n",
      "Epoch 8, Batch 220, Loss: 1.0637620687484741\n",
      "Epoch 8, Batch 230, Loss: 1.0996564626693726\n",
      "Epoch 8, Batch 240, Loss: 1.3442262411117554\n",
      "Epoch 8, Batch 250, Loss: 1.2817203998565674\n",
      "Epoch 8, Batch 260, Loss: 1.2122693061828613\n",
      "Epoch 8, Batch 270, Loss: 1.2055394649505615\n",
      "Epoch 8, Batch 280, Loss: 1.4509971141815186\n",
      "Epoch 8, Batch 290, Loss: 1.1621474027633667\n",
      "Epoch 8, Batch 300, Loss: 1.3313424587249756\n",
      "Epoch 8, Batch 310, Loss: 1.3707751035690308\n",
      "Epoch 8, Batch 320, Loss: 1.2349305152893066\n",
      "Epoch 8, Batch 330, Loss: 1.2125890254974365\n",
      "Epoch 8, Batch 340, Loss: 1.1163614988327026\n",
      "Epoch 8, Batch 350, Loss: 1.4679003953933716\n",
      "Epoch 8, Batch 360, Loss: 1.3205664157867432\n",
      "Epoch 8, Batch 370, Loss: 1.2917035818099976\n",
      "Epoch 8, Batch 380, Loss: 1.1671515703201294\n",
      "Epoch 8, Batch 390, Loss: 1.1883291006088257\n",
      "Epoch 8, Batch 400, Loss: 1.2797446250915527\n",
      "Epoch 8, Batch 410, Loss: 1.0875763893127441\n",
      "Epoch 8, Batch 420, Loss: 1.21053147315979\n",
      "Epoch 8, Batch 430, Loss: 1.0949394702911377\n",
      "Epoch 8, Batch 440, Loss: 1.4240617752075195\n",
      "Epoch 8, Batch 450, Loss: 0.9755862951278687\n",
      "Epoch 8, Batch 460, Loss: 1.2247248888015747\n",
      "Epoch 8, Batch 470, Loss: 1.5122606754302979\n",
      "Epoch 8, Batch 480, Loss: 1.273471713066101\n",
      "Epoch 8, Batch 490, Loss: 1.2434005737304688\n",
      "Epoch 8, Batch 500, Loss: 1.3233082294464111\n",
      "Epoch 8, Batch 510, Loss: 1.1519534587860107\n",
      "Epoch 8, Batch 520, Loss: 1.2626832723617554\n",
      "Epoch 8, Batch 530, Loss: 1.0100178718566895\n",
      "Epoch 8, Batch 540, Loss: 1.2608509063720703\n",
      "Epoch 8, Batch 550, Loss: 1.4572229385375977\n",
      "Epoch 8, Batch 560, Loss: 1.1001789569854736\n",
      "Epoch 8, Batch 570, Loss: 1.0334715843200684\n",
      "Epoch 8, Batch 580, Loss: 1.344379186630249\n",
      "Epoch 8, Batch 590, Loss: 1.1146467924118042\n",
      "Epoch 8, Batch 600, Loss: 1.4912999868392944\n",
      "Epoch 8, Batch 610, Loss: 1.1774687767028809\n",
      "Epoch 8, Batch 620, Loss: 1.2700592279434204\n",
      "Epoch 8, Batch 630, Loss: 1.3547805547714233\n",
      "Epoch 8, Batch 640, Loss: 1.1689093112945557\n",
      "Epoch 8, Batch 650, Loss: 0.9314470887184143\n",
      "Epoch 8, Batch 660, Loss: 1.2010360956192017\n",
      "Epoch 8, Batch 670, Loss: 0.9624965786933899\n",
      "Epoch 8, Batch 680, Loss: 1.179131031036377\n",
      "Epoch 8, Batch 690, Loss: 1.3110716342926025\n",
      "Epoch 8, Batch 700, Loss: 1.0097206830978394\n",
      "Epoch 8, Batch 710, Loss: 1.5732215642929077\n",
      "Epoch 8, Batch 720, Loss: 1.3305675983428955\n",
      "Epoch 8, Batch 730, Loss: 1.2959566116333008\n",
      "Epoch 8, Batch 740, Loss: 1.1688610315322876\n",
      "Epoch 8, Batch 750, Loss: 1.3490277528762817\n",
      "Epoch 8, Batch 760, Loss: 1.2161294221878052\n",
      "Epoch 8, Batch 770, Loss: 1.232279658317566\n",
      "Epoch 8, Batch 780, Loss: 1.4072084426879883\n",
      "Epoch 8, Batch 790, Loss: 1.5133697986602783\n",
      "Epoch 8, Batch 800, Loss: 1.129807472229004\n",
      "Epoch 8, Batch 810, Loss: 1.1505625247955322\n",
      "Epoch 8, Batch 820, Loss: 1.390147089958191\n",
      "Epoch 8, Batch 830, Loss: 1.1292604207992554\n",
      "Epoch 8, Batch 840, Loss: 1.1262965202331543\n",
      "Epoch 8, Batch 850, Loss: 1.3039171695709229\n",
      "Epoch 8, Batch 860, Loss: 1.3257312774658203\n",
      "Epoch 8, Batch 870, Loss: 1.3254389762878418\n",
      "Epoch 8, Batch 880, Loss: 1.0309075117111206\n",
      "Epoch 8, Batch 890, Loss: 1.0430715084075928\n",
      "Epoch 8, Batch 900, Loss: 0.978202760219574\n",
      "Epoch 8, Batch 910, Loss: 1.2571097612380981\n",
      "Epoch 8, Batch 920, Loss: 1.0181726217269897\n",
      "Epoch 8, Batch 930, Loss: 1.0868885517120361\n",
      "Epoch 8, Batch 940, Loss: 1.1980795860290527\n",
      "Epoch 8, Batch 950, Loss: 1.3620657920837402\n",
      "Epoch 8, Batch 960, Loss: 1.3050211668014526\n",
      "Epoch 8, Batch 970, Loss: 1.3679819107055664\n",
      "Epoch 8, Batch 980, Loss: 1.0378224849700928\n",
      "Epoch 8, Batch 990, Loss: 0.9460567235946655\n",
      "Epoch 8, Batch 1000, Loss: 0.9736610651016235\n",
      "Epoch 8, Batch 1010, Loss: 0.9783440828323364\n",
      "Epoch 8, Batch 1020, Loss: 1.232474684715271\n",
      "Epoch 8, Batch 1030, Loss: 1.0673373937606812\n",
      "Epoch 8, Batch 1040, Loss: 1.220144271850586\n",
      "Epoch 8, Batch 1050, Loss: 1.068981647491455\n",
      "Epoch 8, Batch 1060, Loss: 1.6004024744033813\n",
      "Epoch 8, Batch 1070, Loss: 1.0758931636810303\n",
      "Epoch 8, Batch 1080, Loss: 0.9679976105690002\n",
      "Epoch 8, Batch 1090, Loss: 1.1736055612564087\n",
      "Epoch 8, Batch 1100, Loss: 1.3188905715942383\n",
      "Epoch 8, Batch 1110, Loss: 1.162795066833496\n",
      "Epoch 8, Batch 1120, Loss: 1.2650636434555054\n",
      "Epoch 8, Batch 1130, Loss: 1.0102730989456177\n",
      "Epoch 8, Batch 1140, Loss: 1.1314921379089355\n",
      "Epoch 8, Batch 1150, Loss: 1.1427475214004517\n",
      "Epoch 8, Batch 1160, Loss: 1.162505030632019\n",
      "Epoch 8, Batch 1170, Loss: 1.0648961067199707\n",
      "Epoch 8, Batch 1180, Loss: 1.4540590047836304\n",
      "Epoch 8, Batch 1190, Loss: 1.5022209882736206\n",
      "Epoch 8, Batch 1200, Loss: 1.180821180343628\n",
      "Epoch 8, Batch 1210, Loss: 1.163149356842041\n",
      "Epoch 8, Batch 1220, Loss: 1.1365489959716797\n",
      "Epoch 8, Batch 1230, Loss: 1.0387707948684692\n",
      "Epoch 8, Batch 1240, Loss: 1.237802505493164\n",
      "Epoch 8, Batch 1250, Loss: 1.0535213947296143\n",
      "Epoch 8, Batch 1260, Loss: 0.9155123233795166\n",
      "Epoch 8, Batch 1270, Loss: 1.3445537090301514\n",
      "Epoch 8, Batch 1280, Loss: 1.2676293849945068\n",
      "Epoch 8, Batch 1290, Loss: 1.2206813097000122\n",
      "Epoch 8, Batch 1300, Loss: 1.1388866901397705\n",
      "Epoch 8, Batch 1310, Loss: 1.0575429201126099\n",
      "Epoch 8, Batch 1320, Loss: 1.2492244243621826\n",
      "Epoch 8, Batch 1330, Loss: 1.1037278175354004\n",
      "Epoch 8, Batch 1340, Loss: 1.2621588706970215\n",
      "Epoch 8, Batch 1350, Loss: 1.1307146549224854\n",
      "Epoch 8, Batch 1360, Loss: 1.2777020931243896\n",
      "Epoch 8, Batch 1370, Loss: 1.170220136642456\n",
      "Epoch 8, Batch 1380, Loss: 1.084375262260437\n",
      "Epoch 8, Batch 1390, Loss: 1.3328813314437866\n",
      "Epoch 8, Batch 1400, Loss: 1.0694100856781006\n",
      "Epoch 8, Train Loss: 1.2001406730873498, Validation Accuracy: 56.157536985205915%\n",
      "Epoch 9, Batch 0, Loss: 0.8511528372764587\n",
      "Epoch 9, Batch 10, Loss: 1.0306925773620605\n",
      "Epoch 9, Batch 20, Loss: 1.2541450262069702\n",
      "Epoch 9, Batch 30, Loss: 1.380839228630066\n",
      "Epoch 9, Batch 40, Loss: 1.064988374710083\n",
      "Epoch 9, Batch 50, Loss: 1.2092477083206177\n",
      "Epoch 9, Batch 60, Loss: 1.3457763195037842\n",
      "Epoch 9, Batch 70, Loss: 1.0573495626449585\n",
      "Epoch 9, Batch 80, Loss: 1.2478890419006348\n",
      "Epoch 9, Batch 90, Loss: 1.282629132270813\n",
      "Epoch 9, Batch 100, Loss: 1.1538151502609253\n",
      "Epoch 9, Batch 110, Loss: 1.3083815574645996\n",
      "Epoch 9, Batch 120, Loss: 1.1702300310134888\n",
      "Epoch 9, Batch 130, Loss: 1.0757551193237305\n",
      "Epoch 9, Batch 140, Loss: 1.184175729751587\n",
      "Epoch 9, Batch 150, Loss: 1.2308481931686401\n",
      "Epoch 9, Batch 160, Loss: 1.1859054565429688\n",
      "Epoch 9, Batch 170, Loss: 1.2961974143981934\n",
      "Epoch 9, Batch 180, Loss: 1.0293858051300049\n",
      "Epoch 9, Batch 190, Loss: 1.2792260646820068\n",
      "Epoch 9, Batch 200, Loss: 1.3607659339904785\n",
      "Epoch 9, Batch 210, Loss: 1.3780897855758667\n",
      "Epoch 9, Batch 220, Loss: 1.3045926094055176\n",
      "Epoch 9, Batch 230, Loss: 1.2740765810012817\n",
      "Epoch 9, Batch 240, Loss: 1.184920310974121\n",
      "Epoch 9, Batch 250, Loss: 1.3069802522659302\n",
      "Epoch 9, Batch 260, Loss: 1.0454909801483154\n",
      "Epoch 9, Batch 270, Loss: 1.2195262908935547\n",
      "Epoch 9, Batch 280, Loss: 1.3435282707214355\n",
      "Epoch 9, Batch 290, Loss: 0.9241231679916382\n",
      "Epoch 9, Batch 300, Loss: 1.2621229887008667\n",
      "Epoch 9, Batch 310, Loss: 1.1870955228805542\n",
      "Epoch 9, Batch 320, Loss: 1.3509838581085205\n",
      "Epoch 9, Batch 330, Loss: 1.2060593366622925\n",
      "Epoch 9, Batch 340, Loss: 1.2691506147384644\n",
      "Epoch 9, Batch 350, Loss: 1.151009440422058\n",
      "Epoch 9, Batch 360, Loss: 1.3338117599487305\n",
      "Epoch 9, Batch 370, Loss: 1.0609281063079834\n",
      "Epoch 9, Batch 380, Loss: 1.1066815853118896\n",
      "Epoch 9, Batch 390, Loss: 1.1176965236663818\n",
      "Epoch 9, Batch 400, Loss: 1.118703842163086\n",
      "Epoch 9, Batch 410, Loss: 1.2870659828186035\n",
      "Epoch 9, Batch 420, Loss: 1.1566376686096191\n",
      "Epoch 9, Batch 430, Loss: 1.2027090787887573\n",
      "Epoch 9, Batch 440, Loss: 1.2057747840881348\n",
      "Epoch 9, Batch 450, Loss: 1.1247512102127075\n",
      "Epoch 9, Batch 460, Loss: 1.0721275806427002\n",
      "Epoch 9, Batch 470, Loss: 1.3020864725112915\n",
      "Epoch 9, Batch 480, Loss: 1.3631107807159424\n",
      "Epoch 9, Batch 490, Loss: 1.206751823425293\n",
      "Epoch 9, Batch 500, Loss: 1.1223323345184326\n",
      "Epoch 9, Batch 510, Loss: 1.2474114894866943\n",
      "Epoch 9, Batch 520, Loss: 1.2753244638442993\n",
      "Epoch 9, Batch 530, Loss: 1.117221713066101\n",
      "Epoch 9, Batch 540, Loss: 1.2196944952011108\n",
      "Epoch 9, Batch 550, Loss: 0.9320279955863953\n",
      "Epoch 9, Batch 560, Loss: 1.0439064502716064\n",
      "Epoch 9, Batch 570, Loss: 0.9740340113639832\n",
      "Epoch 9, Batch 580, Loss: 1.4288235902786255\n",
      "Epoch 9, Batch 590, Loss: 1.2044939994812012\n",
      "Epoch 9, Batch 600, Loss: 1.2708922624588013\n",
      "Epoch 9, Batch 610, Loss: 1.248020887374878\n",
      "Epoch 9, Batch 620, Loss: 1.1059517860412598\n",
      "Epoch 9, Batch 630, Loss: 1.1586310863494873\n",
      "Epoch 9, Batch 640, Loss: 0.9653434753417969\n",
      "Epoch 9, Batch 650, Loss: 1.0510109663009644\n",
      "Epoch 9, Batch 660, Loss: 1.2552876472473145\n",
      "Epoch 9, Batch 670, Loss: 1.0675220489501953\n",
      "Epoch 9, Batch 680, Loss: 1.3523917198181152\n",
      "Epoch 9, Batch 690, Loss: 0.951654851436615\n",
      "Epoch 9, Batch 700, Loss: 1.0240927934646606\n",
      "Epoch 9, Batch 710, Loss: 1.396309494972229\n",
      "Epoch 9, Batch 720, Loss: 1.0271352529525757\n",
      "Epoch 9, Batch 730, Loss: 1.107877492904663\n",
      "Epoch 9, Batch 740, Loss: 1.2516857385635376\n",
      "Epoch 9, Batch 750, Loss: 1.1306523084640503\n",
      "Epoch 9, Batch 760, Loss: 1.0489901304244995\n",
      "Epoch 9, Batch 770, Loss: 1.145850419998169\n",
      "Epoch 9, Batch 780, Loss: 1.0256125926971436\n",
      "Epoch 9, Batch 790, Loss: 1.0007579326629639\n",
      "Epoch 9, Batch 800, Loss: 1.269255518913269\n",
      "Epoch 9, Batch 810, Loss: 1.3978848457336426\n",
      "Epoch 9, Batch 820, Loss: 1.241265058517456\n",
      "Epoch 9, Batch 830, Loss: 1.0529446601867676\n",
      "Epoch 9, Batch 840, Loss: 1.1284265518188477\n",
      "Epoch 9, Batch 850, Loss: 1.4312955141067505\n",
      "Epoch 9, Batch 860, Loss: 1.1624811887741089\n",
      "Epoch 9, Batch 870, Loss: 1.259643793106079\n",
      "Epoch 9, Batch 880, Loss: 1.2063567638397217\n",
      "Epoch 9, Batch 890, Loss: 1.1821147203445435\n",
      "Epoch 9, Batch 900, Loss: 1.1222856044769287\n",
      "Epoch 9, Batch 910, Loss: 1.2512584924697876\n",
      "Epoch 9, Batch 920, Loss: 1.4594491720199585\n",
      "Epoch 9, Batch 930, Loss: 1.3647092580795288\n",
      "Epoch 9, Batch 940, Loss: 1.313295602798462\n",
      "Epoch 9, Batch 950, Loss: 1.0341607332229614\n",
      "Epoch 9, Batch 960, Loss: 1.1079822778701782\n",
      "Epoch 9, Batch 970, Loss: 1.271477222442627\n",
      "Epoch 9, Batch 980, Loss: 1.1960469484329224\n",
      "Epoch 9, Batch 990, Loss: 1.1690170764923096\n",
      "Epoch 9, Batch 1000, Loss: 1.1422441005706787\n",
      "Epoch 9, Batch 1010, Loss: 1.321647047996521\n",
      "Epoch 9, Batch 1020, Loss: 1.2552558183670044\n",
      "Epoch 9, Batch 1030, Loss: 0.9482762813568115\n",
      "Epoch 9, Batch 1040, Loss: 1.178955316543579\n",
      "Epoch 9, Batch 1050, Loss: 1.2481340169906616\n",
      "Epoch 9, Batch 1060, Loss: 1.1763631105422974\n",
      "Epoch 9, Batch 1070, Loss: 1.306233286857605\n",
      "Epoch 9, Batch 1080, Loss: 1.0344172716140747\n",
      "Epoch 9, Batch 1090, Loss: 1.2601066827774048\n",
      "Epoch 9, Batch 1100, Loss: 1.3276195526123047\n",
      "Epoch 9, Batch 1110, Loss: 1.3879534006118774\n",
      "Epoch 9, Batch 1120, Loss: 1.3040485382080078\n",
      "Epoch 9, Batch 1130, Loss: 1.120386004447937\n",
      "Epoch 9, Batch 1140, Loss: 1.847307801246643\n",
      "Epoch 9, Batch 1150, Loss: 1.5712486505508423\n",
      "Epoch 9, Batch 1160, Loss: 0.9360324740409851\n",
      "Epoch 9, Batch 1170, Loss: 1.0939713716506958\n",
      "Epoch 9, Batch 1180, Loss: 1.4007996320724487\n",
      "Epoch 9, Batch 1190, Loss: 1.1823792457580566\n",
      "Epoch 9, Batch 1200, Loss: 1.1863653659820557\n",
      "Epoch 9, Batch 1210, Loss: 1.208103895187378\n",
      "Epoch 9, Batch 1220, Loss: 0.9599952101707458\n",
      "Epoch 9, Batch 1230, Loss: 1.3447321653366089\n",
      "Epoch 9, Batch 1240, Loss: 1.216304898262024\n",
      "Epoch 9, Batch 1250, Loss: 1.1551132202148438\n",
      "Epoch 9, Batch 1260, Loss: 1.272916555404663\n",
      "Epoch 9, Batch 1270, Loss: 1.2691280841827393\n",
      "Epoch 9, Batch 1280, Loss: 1.3081765174865723\n",
      "Epoch 9, Batch 1290, Loss: 0.9770864248275757\n",
      "Epoch 9, Batch 1300, Loss: 1.1690154075622559\n",
      "Epoch 9, Batch 1310, Loss: 0.9798557162284851\n",
      "Epoch 9, Batch 1320, Loss: 1.3184306621551514\n",
      "Epoch 9, Batch 1330, Loss: 1.334991693496704\n",
      "Epoch 9, Batch 1340, Loss: 1.0275838375091553\n",
      "Epoch 9, Batch 1350, Loss: 1.3672502040863037\n",
      "Epoch 9, Batch 1360, Loss: 1.2071837186813354\n",
      "Epoch 9, Batch 1370, Loss: 1.0494283437728882\n",
      "Epoch 9, Batch 1380, Loss: 1.3529655933380127\n",
      "Epoch 9, Batch 1390, Loss: 0.9041395783424377\n",
      "Epoch 9, Batch 1400, Loss: 1.2519603967666626\n",
      "Epoch 9, Train Loss: 1.203048827805753, Validation Accuracy: 57.35705717712915%\n",
      "Best Validation Accuracy: 57.35705717712915%\n",
      "Test Accuracy: 63.899721448467965%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from medmnist import PathMNIST\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = self.get_resnet(base_model)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def get_resnet(self, base_model):\n",
    "        model = models.__dict__[base_model](weights=None)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        z = self.projector(h)\n",
    "        return h, z\n",
    "\n",
    "\n",
    "model = SimCLR(base_model='resnet18', out_dim=128).cuda()\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./simclr_pathmnist_pretrained_epoch_25.pth'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, base_encoder):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.classifier = nn.Linear(512, 9)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        logits = self.classifier(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "linear_probe_model = LinearProbe(model.encoder).cuda()\n",
    "\n",
    "PathMNIST_MEAN = [0.73765225, 0.53090023, 0.70307171]\n",
    "PathMNIST_STD = [0.12319908, 0.17607205, 0.12394462]\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=PathMNIST_MEAN, std=PathMNIST_STD)\n",
    "])\n",
    "\n",
    "train_dataset = PathMNIST(split='train', transform=data_transform, download=True, size=64)\n",
    "val_dataset = PathMNIST(split='val', transform=data_transform, download=True, size=64)\n",
    "test_dataset = PathMNIST(split='test', transform=data_transform, download=True, size=64)\n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "subset_indices = list(range(num_samples))\n",
    "np.random.shuffle(subset_indices)\n",
    "subset_indices = subset_indices[:int(1 * num_samples)]\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe_model.classifier.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "def train_linear_probe(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.cuda(), labels.squeeze().long().cuda() \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.squeeze().long().cuda()  # 确保 labels 是 1D 张量\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_linear_probe(linear_probe_model, train_loader, criterion, optimizer, epoch)\n",
    "    val_accuracy = evaluate(linear_probe_model, val_loader)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    \n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = linear_probe_model.state_dict()\n",
    "\n",
    "linear_probe_model.load_state_dict(best_model_state)\n",
    "\n",
    "test_accuracy = evaluate(linear_probe_model, test_loader)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}%')\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4ad27",
   "metadata": {},
   "source": [
    "# 50 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd30bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Epoch 0, Batch 0, Loss: 2.2523465156555176\n",
      "Epoch 0, Batch 10, Loss: 2.102701425552368\n",
      "Epoch 0, Train Loss: 2.125023873647054, Validation Accuracy: 18.422630947620952%\n",
      "Epoch 1, Batch 0, Loss: 2.0268633365631104\n",
      "Epoch 1, Batch 10, Loss: 1.8630499839782715\n",
      "Epoch 1, Train Loss: 1.9863341490427653, Validation Accuracy: 24.840063974410235%\n",
      "Epoch 2, Batch 0, Loss: 1.8402773141860962\n",
      "Epoch 2, Batch 10, Loss: 1.7296574115753174\n",
      "Epoch 2, Train Loss: 1.902810033162435, Validation Accuracy: 31.827269092363053%\n",
      "Epoch 3, Batch 0, Loss: 1.8165563344955444\n",
      "Epoch 3, Batch 10, Loss: 1.8288967609405518\n",
      "Epoch 3, Train Loss: 1.806272013982137, Validation Accuracy: 32.62694922031188%\n",
      "Epoch 4, Batch 0, Loss: 1.8543851375579834\n",
      "Epoch 4, Batch 10, Loss: 1.6831387281417847\n",
      "Epoch 4, Train Loss: 1.7589614152908326, Validation Accuracy: 35.34586165533786%\n",
      "Epoch 5, Batch 0, Loss: 1.7872724533081055\n",
      "Epoch 5, Batch 10, Loss: 1.6574586629867554\n",
      "Epoch 5, Train Loss: 1.7376868963241576, Validation Accuracy: 37.75489804078369%\n",
      "Epoch 6, Batch 0, Loss: 1.6356326341629028\n",
      "Epoch 6, Batch 10, Loss: 1.6414917707443237\n",
      "Epoch 6, Train Loss: 1.706555461883545, Validation Accuracy: 36.91523390643742%\n",
      "Epoch 7, Batch 0, Loss: 1.6849069595336914\n",
      "Epoch 7, Batch 10, Loss: 1.6907267570495605\n",
      "Epoch 7, Train Loss: 1.6613490104675293, Validation Accuracy: 35.98560575769692%\n",
      "Epoch 8, Batch 0, Loss: 1.602851152420044\n",
      "Epoch 8, Batch 10, Loss: 1.4744850397109985\n",
      "Epoch 8, Train Loss: 1.6700666268666586, Validation Accuracy: 38.76449420231907%\n",
      "Epoch 9, Batch 0, Loss: 1.5925335884094238\n",
      "Epoch 9, Batch 10, Loss: 1.6527162790298462\n",
      "Epoch 9, Train Loss: 1.6033220211664836, Validation Accuracy: 37.475009996001596%\n",
      "Best Validation Accuracy: 38.76449420231907%\n",
      "Test Accuracy: 41.183844011142064%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from medmnist import PathMNIST\n",
    "import numpy as np\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = self.get_resnet(base_model)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def get_resnet(self, base_model):\n",
    "        model = models.__dict__[base_model](weights=None)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        z = self.projector(h)\n",
    "        return h, z\n",
    "\n",
    "\n",
    "model = SimCLR(base_model='resnet18', out_dim=128).cuda()\n",
    "\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./simclr_pathmnist_pretrained_epoch_50.pth'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, base_encoder):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.classifier = nn.Linear(512, 9)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        logits = self.classifier(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "linear_probe_model = LinearProbe(model.encoder).cuda()\n",
    "\n",
    "PathMNIST_MEAN = [0.73765225, 0.53090023, 0.70307171]\n",
    "PathMNIST_STD = [0.12319908, 0.17607205, 0.12394462]\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=PathMNIST_MEAN, std=PathMNIST_STD)\n",
    "])\n",
    "\n",
    "train_dataset = PathMNIST(split='train', transform=data_transform, download=True, size=64)\n",
    "val_dataset = PathMNIST(split='val', transform=data_transform, download=True, size=64)\n",
    "test_dataset = PathMNIST(split='test', transform=data_transform, download=True, size=64)\n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "subset_indices = list(range(num_samples))\n",
    "np.random.shuffle(subset_indices)\n",
    "subset_indices = subset_indices[:int(0.01 * num_samples)]\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe_model.classifier.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "def train_linear_probe(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.cuda(), labels.squeeze().long().cuda()  # make sure labels is 1D tensor\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.squeeze().long().cuda() \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_linear_probe(linear_probe_model, train_loader, criterion, optimizer, epoch)\n",
    "    val_accuracy = evaluate(linear_probe_model, val_loader)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    \n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = linear_probe_model.state_dict()\n",
    "\n",
    "linear_probe_model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "test_accuracy = evaluate(linear_probe_model, test_loader)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}%')\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a93557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Epoch 0, Batch 0, Loss: 2.3063788414001465\n",
      "Epoch 0, Batch 10, Loss: 2.1800320148468018\n",
      "Epoch 0, Batch 20, Loss: 1.9225044250488281\n",
      "Epoch 0, Batch 30, Loss: 1.964726448059082\n",
      "Epoch 0, Batch 40, Loss: 1.9487274885177612\n",
      "Epoch 0, Batch 50, Loss: 1.8286046981811523\n",
      "Epoch 0, Batch 60, Loss: 1.811076283454895\n",
      "Epoch 0, Batch 70, Loss: 1.6507103443145752\n",
      "Epoch 0, Batch 80, Loss: 1.8722964525222778\n",
      "Epoch 0, Batch 90, Loss: 1.7462856769561768\n",
      "Epoch 0, Batch 100, Loss: 1.7835569381713867\n",
      "Epoch 0, Batch 110, Loss: 1.8792753219604492\n",
      "Epoch 0, Batch 120, Loss: 1.6946680545806885\n",
      "Epoch 0, Batch 130, Loss: 1.7640433311462402\n",
      "Epoch 0, Batch 140, Loss: 1.6019096374511719\n",
      "Epoch 0, Train Loss: 1.8541685680971078, Validation Accuracy: 42.87285085965614%\n",
      "Epoch 1, Batch 0, Loss: 1.631035566329956\n",
      "Epoch 1, Batch 10, Loss: 1.6526954174041748\n",
      "Epoch 1, Batch 20, Loss: 1.6740224361419678\n",
      "Epoch 1, Batch 30, Loss: 1.5421184301376343\n",
      "Epoch 1, Batch 40, Loss: 1.6439929008483887\n",
      "Epoch 1, Batch 50, Loss: 1.6112830638885498\n",
      "Epoch 1, Batch 60, Loss: 1.583754062652588\n",
      "Epoch 1, Batch 70, Loss: 1.496037244796753\n",
      "Epoch 1, Batch 80, Loss: 1.6293249130249023\n",
      "Epoch 1, Batch 90, Loss: 1.6378310918807983\n",
      "Epoch 1, Batch 100, Loss: 1.5275936126708984\n",
      "Epoch 1, Batch 110, Loss: 1.5349316596984863\n",
      "Epoch 1, Batch 120, Loss: 1.642457365989685\n",
      "Epoch 1, Batch 130, Loss: 1.5420578718185425\n",
      "Epoch 1, Batch 140, Loss: 1.7770334482192993\n",
      "Epoch 1, Train Loss: 1.578424777544982, Validation Accuracy: 48.04078368652539%\n",
      "Epoch 2, Batch 0, Loss: 1.3653429746627808\n",
      "Epoch 2, Batch 10, Loss: 1.4314219951629639\n",
      "Epoch 2, Batch 20, Loss: 1.3511416912078857\n",
      "Epoch 2, Batch 30, Loss: 1.386518120765686\n",
      "Epoch 2, Batch 40, Loss: 1.4349333047866821\n",
      "Epoch 2, Batch 50, Loss: 1.554190754890442\n",
      "Epoch 2, Batch 60, Loss: 1.4721392393112183\n",
      "Epoch 2, Batch 70, Loss: 1.5236690044403076\n",
      "Epoch 2, Batch 80, Loss: 1.568118929862976\n",
      "Epoch 2, Batch 90, Loss: 1.3638843297958374\n",
      "Epoch 2, Batch 100, Loss: 1.4719231128692627\n",
      "Epoch 2, Batch 110, Loss: 1.3771395683288574\n",
      "Epoch 2, Batch 120, Loss: 1.5447496175765991\n",
      "Epoch 2, Batch 130, Loss: 1.372648000717163\n",
      "Epoch 2, Batch 140, Loss: 1.5012927055358887\n",
      "Epoch 2, Train Loss: 1.456264077348912, Validation Accuracy: 50.81967213114754%\n",
      "Epoch 3, Batch 0, Loss: 1.425073266029358\n",
      "Epoch 3, Batch 10, Loss: 1.5157805681228638\n",
      "Epoch 3, Batch 20, Loss: 1.4016786813735962\n",
      "Epoch 3, Batch 30, Loss: 1.3501890897750854\n",
      "Epoch 3, Batch 40, Loss: 1.3120830059051514\n",
      "Epoch 3, Batch 50, Loss: 1.5392889976501465\n",
      "Epoch 3, Batch 60, Loss: 1.279012680053711\n",
      "Epoch 3, Batch 70, Loss: 1.4739019870758057\n",
      "Epoch 3, Batch 80, Loss: 1.4539991617202759\n",
      "Epoch 3, Batch 90, Loss: 1.193410038948059\n",
      "Epoch 3, Batch 100, Loss: 1.2196038961410522\n",
      "Epoch 3, Batch 110, Loss: 1.4315648078918457\n",
      "Epoch 3, Batch 120, Loss: 1.4282132387161255\n",
      "Epoch 3, Batch 130, Loss: 1.3592007160186768\n",
      "Epoch 3, Batch 140, Loss: 1.2488691806793213\n",
      "Epoch 3, Train Loss: 1.3953468605136194, Validation Accuracy: 51.29948020791684%\n",
      "Epoch 4, Batch 0, Loss: 1.3365576267242432\n",
      "Epoch 4, Batch 10, Loss: 1.2783780097961426\n",
      "Epoch 4, Batch 20, Loss: 1.4115500450134277\n",
      "Epoch 4, Batch 30, Loss: 1.250606894493103\n",
      "Epoch 4, Batch 40, Loss: 1.3510314226150513\n",
      "Epoch 4, Batch 50, Loss: 1.1648763418197632\n",
      "Epoch 4, Batch 60, Loss: 1.2755987644195557\n",
      "Epoch 4, Batch 70, Loss: 1.5002193450927734\n",
      "Epoch 4, Batch 80, Loss: 1.3088693618774414\n",
      "Epoch 4, Batch 90, Loss: 1.5786445140838623\n",
      "Epoch 4, Batch 100, Loss: 1.4861849546432495\n",
      "Epoch 4, Batch 110, Loss: 1.401992917060852\n",
      "Epoch 4, Batch 120, Loss: 1.202405571937561\n",
      "Epoch 4, Batch 130, Loss: 1.4143764972686768\n",
      "Epoch 4, Batch 140, Loss: 1.418821096420288\n",
      "Epoch 4, Train Loss: 1.347517209695586, Validation Accuracy: 50.529788084766096%\n",
      "Epoch 5, Batch 0, Loss: 1.4917104244232178\n",
      "Epoch 5, Batch 10, Loss: 1.0379095077514648\n",
      "Epoch 5, Batch 20, Loss: 1.3833450078964233\n",
      "Epoch 5, Batch 30, Loss: 1.25213623046875\n",
      "Epoch 5, Batch 40, Loss: 1.0471383333206177\n",
      "Epoch 5, Batch 50, Loss: 1.2516913414001465\n",
      "Epoch 5, Batch 60, Loss: 1.2812949419021606\n",
      "Epoch 5, Batch 70, Loss: 1.1291764974594116\n",
      "Epoch 5, Batch 80, Loss: 1.661678671836853\n",
      "Epoch 5, Batch 90, Loss: 1.1814839839935303\n",
      "Epoch 5, Batch 100, Loss: 1.053288221359253\n",
      "Epoch 5, Batch 110, Loss: 1.3377704620361328\n",
      "Epoch 5, Batch 120, Loss: 1.3674176931381226\n",
      "Epoch 5, Batch 130, Loss: 1.508346438407898\n",
      "Epoch 5, Batch 140, Loss: 1.1024463176727295\n",
      "Epoch 5, Train Loss: 1.3208062750227907, Validation Accuracy: 51.83926429428229%\n",
      "Epoch 6, Batch 0, Loss: 1.187458872795105\n",
      "Epoch 6, Batch 10, Loss: 1.365061640739441\n",
      "Epoch 6, Batch 20, Loss: 1.2880024909973145\n",
      "Epoch 6, Batch 30, Loss: 1.1177899837493896\n",
      "Epoch 6, Batch 40, Loss: 1.3306478261947632\n",
      "Epoch 6, Batch 50, Loss: 1.3530734777450562\n",
      "Epoch 6, Batch 60, Loss: 1.432855248451233\n",
      "Epoch 6, Batch 70, Loss: 1.362553358078003\n",
      "Epoch 6, Batch 80, Loss: 1.2626416683197021\n",
      "Epoch 6, Batch 90, Loss: 1.394156575202942\n",
      "Epoch 6, Batch 100, Loss: 1.1984955072402954\n",
      "Epoch 6, Batch 110, Loss: 1.4514873027801514\n",
      "Epoch 6, Batch 120, Loss: 1.2978489398956299\n",
      "Epoch 6, Batch 130, Loss: 1.2876224517822266\n",
      "Epoch 6, Batch 140, Loss: 1.2399420738220215\n",
      "Epoch 6, Train Loss: 1.2844083140082394, Validation Accuracy: 52.109156337465016%\n",
      "Epoch 7, Batch 0, Loss: 1.050148844718933\n",
      "Epoch 7, Batch 10, Loss: 1.323431134223938\n",
      "Epoch 7, Batch 20, Loss: 1.2720158100128174\n",
      "Epoch 7, Batch 30, Loss: 1.1853909492492676\n",
      "Epoch 7, Batch 40, Loss: 1.234300971031189\n",
      "Epoch 7, Batch 50, Loss: 1.407546043395996\n",
      "Epoch 7, Batch 60, Loss: 1.3283398151397705\n",
      "Epoch 7, Batch 70, Loss: 1.2097443342208862\n",
      "Epoch 7, Batch 80, Loss: 1.1771793365478516\n",
      "Epoch 7, Batch 90, Loss: 1.2656960487365723\n",
      "Epoch 7, Batch 100, Loss: 1.100982904434204\n",
      "Epoch 7, Batch 110, Loss: 1.332289695739746\n",
      "Epoch 7, Batch 120, Loss: 1.1872096061706543\n",
      "Epoch 7, Batch 130, Loss: 1.260432243347168\n",
      "Epoch 7, Batch 140, Loss: 1.4233052730560303\n",
      "Epoch 7, Train Loss: 1.2606624212670834, Validation Accuracy: 52.21911235505798%\n",
      "Epoch 8, Batch 0, Loss: 1.1993228197097778\n",
      "Epoch 8, Batch 10, Loss: 1.3061949014663696\n",
      "Epoch 8, Batch 20, Loss: 1.1548625230789185\n",
      "Epoch 8, Batch 30, Loss: 1.3771288394927979\n",
      "Epoch 8, Batch 40, Loss: 1.1607024669647217\n",
      "Epoch 8, Batch 50, Loss: 1.1222362518310547\n",
      "Epoch 8, Batch 60, Loss: 1.3925049304962158\n",
      "Epoch 8, Batch 70, Loss: 1.1567822694778442\n",
      "Epoch 8, Batch 80, Loss: 1.4678905010223389\n",
      "Epoch 8, Batch 90, Loss: 1.2860076427459717\n",
      "Epoch 8, Batch 100, Loss: 1.0973985195159912\n",
      "Epoch 8, Batch 110, Loss: 1.4331586360931396\n",
      "Epoch 8, Batch 120, Loss: 1.2405171394348145\n",
      "Epoch 8, Batch 130, Loss: 1.4625695943832397\n",
      "Epoch 8, Batch 140, Loss: 0.9758256077766418\n",
      "Epoch 8, Train Loss: 1.2413306198221572, Validation Accuracy: 53.038784486205515%\n",
      "Epoch 9, Batch 0, Loss: 1.1878095865249634\n",
      "Epoch 9, Batch 10, Loss: 1.4008210897445679\n",
      "Epoch 9, Batch 20, Loss: 1.2388278245925903\n",
      "Epoch 9, Batch 30, Loss: 1.2907615900039673\n",
      "Epoch 9, Batch 40, Loss: 1.4015331268310547\n",
      "Epoch 9, Batch 50, Loss: 1.1444482803344727\n",
      "Epoch 9, Batch 60, Loss: 1.1350867748260498\n",
      "Epoch 9, Batch 70, Loss: 1.2424113750457764\n",
      "Epoch 9, Batch 80, Loss: 1.1053602695465088\n",
      "Epoch 9, Batch 90, Loss: 1.299898386001587\n",
      "Epoch 9, Batch 100, Loss: 1.0112719535827637\n",
      "Epoch 9, Batch 110, Loss: 1.3051116466522217\n",
      "Epoch 9, Batch 120, Loss: 1.3711183071136475\n",
      "Epoch 9, Batch 130, Loss: 1.1876498460769653\n",
      "Epoch 9, Batch 140, Loss: 1.2697445154190063\n",
      "Epoch 9, Train Loss: 1.2347794263920886, Validation Accuracy: 52.279088364654136%\n",
      "Best Validation Accuracy: 53.038784486205515%\n",
      "Test Accuracy: 58.45403899721448%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from medmnist import PathMNIST\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = self.get_resnet(base_model)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def get_resnet(self, base_model):\n",
    "        model = models.__dict__[base_model](weights=None)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        z = self.projector(h)\n",
    "        return h, z\n",
    "\n",
    "\n",
    "model = SimCLR(base_model='resnet18', out_dim=128).cuda()\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./simclr_pathmnist_pretrained_epoch_50.pth'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, base_encoder):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.classifier = nn.Linear(512, 9)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        logits = self.classifier(h)\n",
    "        return logits\n",
    "\n",
    "linear_probe_model = LinearProbe(model.encoder).cuda()\n",
    "\n",
    "PathMNIST_MEAN = [0.73765225, 0.53090023, 0.70307171]\n",
    "PathMNIST_STD = [0.12319908, 0.17607205, 0.12394462]\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=PathMNIST_MEAN, std=PathMNIST_STD)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = PathMNIST(split='train', transform=data_transform, download=True, size=64)\n",
    "val_dataset = PathMNIST(split='val', transform=data_transform, download=True, size=64)\n",
    "test_dataset = PathMNIST(split='test', transform=data_transform, download=True, size=64)\n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "subset_indices = list(range(num_samples))\n",
    "np.random.shuffle(subset_indices)\n",
    "subset_indices = subset_indices[:int(0.1 * num_samples)]\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe_model.classifier.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "def train_linear_probe(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.cuda(), labels.squeeze().long().cuda()  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.squeeze().long().cuda()  \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_linear_probe(linear_probe_model, train_loader, criterion, optimizer, epoch)\n",
    "    val_accuracy = evaluate(linear_probe_model, val_loader)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = linear_probe_model.state_dict()\n",
    "linear_probe_model.load_state_dict(best_model_state)\n",
    "\n",
    "test_accuracy = evaluate(linear_probe_model, test_loader)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}%')\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55283a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Using downloaded and verified file: C:\\Users\\xie\\.medmnist\\pathmnist_64.npz\n",
      "Epoch 0, Batch 0, Loss: 2.3683950901031494\n",
      "Epoch 0, Batch 10, Loss: 2.126798152923584\n",
      "Epoch 0, Batch 20, Loss: 2.0432913303375244\n",
      "Epoch 0, Batch 30, Loss: 1.920710802078247\n",
      "Epoch 0, Batch 40, Loss: 1.8767074346542358\n",
      "Epoch 0, Batch 50, Loss: 1.9641681909561157\n",
      "Epoch 0, Batch 60, Loss: 1.799139380455017\n",
      "Epoch 0, Batch 70, Loss: 1.8516727685928345\n",
      "Epoch 0, Batch 80, Loss: 1.8467769622802734\n",
      "Epoch 0, Batch 90, Loss: 1.7184334993362427\n",
      "Epoch 0, Batch 100, Loss: 1.8639758825302124\n",
      "Epoch 0, Batch 110, Loss: 1.7580262422561646\n",
      "Epoch 0, Batch 120, Loss: 1.8155076503753662\n",
      "Epoch 0, Batch 130, Loss: 1.8071348667144775\n",
      "Epoch 0, Batch 140, Loss: 1.7139227390289307\n",
      "Epoch 0, Batch 150, Loss: 1.6342320442199707\n",
      "Epoch 0, Batch 160, Loss: 1.526282787322998\n",
      "Epoch 0, Batch 170, Loss: 1.6600713729858398\n",
      "Epoch 0, Batch 180, Loss: 1.6657612323760986\n",
      "Epoch 0, Batch 190, Loss: 1.6241933107376099\n",
      "Epoch 0, Batch 200, Loss: 1.6125339269638062\n",
      "Epoch 0, Batch 210, Loss: 1.6830750703811646\n",
      "Epoch 0, Batch 220, Loss: 1.4531108140945435\n",
      "Epoch 0, Batch 230, Loss: 1.5549992322921753\n",
      "Epoch 0, Batch 240, Loss: 1.5361735820770264\n",
      "Epoch 0, Batch 250, Loss: 1.5975672006607056\n",
      "Epoch 0, Batch 260, Loss: 1.5308727025985718\n",
      "Epoch 0, Batch 270, Loss: 1.5320132970809937\n",
      "Epoch 0, Batch 280, Loss: 1.462764859199524\n",
      "Epoch 0, Batch 290, Loss: 1.639810562133789\n",
      "Epoch 0, Batch 300, Loss: 1.672471284866333\n",
      "Epoch 0, Batch 310, Loss: 1.481244683265686\n",
      "Epoch 0, Batch 320, Loss: 1.4211722612380981\n",
      "Epoch 0, Batch 330, Loss: 1.4095721244812012\n",
      "Epoch 0, Batch 340, Loss: 1.6294046640396118\n",
      "Epoch 0, Batch 350, Loss: 1.6450319290161133\n",
      "Epoch 0, Batch 360, Loss: 1.575535535812378\n",
      "Epoch 0, Batch 370, Loss: 1.423972487449646\n",
      "Epoch 0, Batch 380, Loss: 1.5508383512496948\n",
      "Epoch 0, Batch 390, Loss: 1.5060862302780151\n",
      "Epoch 0, Batch 400, Loss: 1.3076820373535156\n",
      "Epoch 0, Batch 410, Loss: 1.4795749187469482\n",
      "Epoch 0, Batch 420, Loss: 1.5597680807113647\n",
      "Epoch 0, Batch 430, Loss: 1.4256622791290283\n",
      "Epoch 0, Batch 440, Loss: 1.5078668594360352\n",
      "Epoch 0, Batch 450, Loss: 1.3273463249206543\n",
      "Epoch 0, Batch 460, Loss: 1.3641304969787598\n",
      "Epoch 0, Batch 470, Loss: 1.3372840881347656\n",
      "Epoch 0, Batch 480, Loss: 1.454969048500061\n",
      "Epoch 0, Batch 490, Loss: 1.3883490562438965\n",
      "Epoch 0, Batch 500, Loss: 1.5219526290893555\n",
      "Epoch 0, Batch 510, Loss: 1.4122763872146606\n",
      "Epoch 0, Batch 520, Loss: 1.4207649230957031\n",
      "Epoch 0, Batch 530, Loss: 1.2500566244125366\n",
      "Epoch 0, Batch 540, Loss: 1.5225926637649536\n",
      "Epoch 0, Batch 550, Loss: 1.4276845455169678\n",
      "Epoch 0, Batch 560, Loss: 1.5511844158172607\n",
      "Epoch 0, Batch 570, Loss: 1.4288522005081177\n",
      "Epoch 0, Batch 580, Loss: 1.476358413696289\n",
      "Epoch 0, Batch 590, Loss: 1.3493982553482056\n",
      "Epoch 0, Batch 600, Loss: 1.2637839317321777\n",
      "Epoch 0, Batch 610, Loss: 1.4331064224243164\n",
      "Epoch 0, Batch 620, Loss: 1.3557177782058716\n",
      "Epoch 0, Batch 630, Loss: 1.2765741348266602\n",
      "Epoch 0, Batch 640, Loss: 1.3503470420837402\n",
      "Epoch 0, Batch 650, Loss: 1.3309526443481445\n",
      "Epoch 0, Batch 660, Loss: 1.316644549369812\n",
      "Epoch 0, Batch 670, Loss: 1.2786709070205688\n",
      "Epoch 0, Batch 680, Loss: 1.429489016532898\n",
      "Epoch 0, Batch 690, Loss: 1.4279940128326416\n",
      "Epoch 0, Batch 700, Loss: 1.290364146232605\n",
      "Epoch 0, Batch 710, Loss: 1.3157964944839478\n",
      "Epoch 0, Batch 720, Loss: 1.3796048164367676\n",
      "Epoch 0, Batch 730, Loss: 1.4173593521118164\n",
      "Epoch 0, Batch 740, Loss: 1.2198394536972046\n",
      "Epoch 0, Batch 750, Loss: 1.2322086095809937\n",
      "Epoch 0, Batch 760, Loss: 1.341732144355774\n",
      "Epoch 0, Batch 770, Loss: 1.2685389518737793\n",
      "Epoch 0, Batch 780, Loss: 1.2687864303588867\n",
      "Epoch 0, Batch 790, Loss: 1.3888710737228394\n",
      "Epoch 0, Batch 800, Loss: 1.3175694942474365\n",
      "Epoch 0, Batch 810, Loss: 1.1679844856262207\n",
      "Epoch 0, Batch 820, Loss: 1.491926670074463\n",
      "Epoch 0, Batch 830, Loss: 1.4984673261642456\n",
      "Epoch 0, Batch 840, Loss: 1.4050387144088745\n",
      "Epoch 0, Batch 850, Loss: 1.751343011856079\n",
      "Epoch 0, Batch 860, Loss: 1.4678064584732056\n",
      "Epoch 0, Batch 870, Loss: 1.2363336086273193\n",
      "Epoch 0, Batch 880, Loss: 1.2634742259979248\n",
      "Epoch 0, Batch 890, Loss: 1.260913372039795\n",
      "Epoch 0, Batch 900, Loss: 1.4807136058807373\n",
      "Epoch 0, Batch 910, Loss: 1.2297604084014893\n",
      "Epoch 0, Batch 920, Loss: 1.439378023147583\n",
      "Epoch 0, Batch 930, Loss: 1.2818434238433838\n",
      "Epoch 0, Batch 940, Loss: 1.4188655614852905\n",
      "Epoch 0, Batch 950, Loss: 1.2712255716323853\n",
      "Epoch 0, Batch 960, Loss: 1.343241810798645\n",
      "Epoch 0, Batch 970, Loss: 1.232177734375\n",
      "Epoch 0, Batch 980, Loss: 1.2479315996170044\n",
      "Epoch 0, Batch 990, Loss: 1.4373550415039062\n",
      "Epoch 0, Batch 1000, Loss: 1.3592981100082397\n",
      "Epoch 0, Batch 1010, Loss: 1.2482647895812988\n",
      "Epoch 0, Batch 1020, Loss: 1.394867181777954\n",
      "Epoch 0, Batch 1030, Loss: 1.217939019203186\n",
      "Epoch 0, Batch 1040, Loss: 1.3256019353866577\n",
      "Epoch 0, Batch 1050, Loss: 1.4535815715789795\n",
      "Epoch 0, Batch 1060, Loss: 1.482437014579773\n",
      "Epoch 0, Batch 1070, Loss: 1.2693456411361694\n",
      "Epoch 0, Batch 1080, Loss: 1.0950933694839478\n",
      "Epoch 0, Batch 1090, Loss: 1.223846673965454\n",
      "Epoch 0, Batch 1100, Loss: 1.2626090049743652\n",
      "Epoch 0, Batch 1110, Loss: 1.2122383117675781\n",
      "Epoch 0, Batch 1120, Loss: 1.1836639642715454\n",
      "Epoch 0, Batch 1130, Loss: 1.2534315586090088\n",
      "Epoch 0, Batch 1140, Loss: 1.5149614810943604\n",
      "Epoch 0, Batch 1150, Loss: 1.4580848217010498\n",
      "Epoch 0, Batch 1160, Loss: 1.355039358139038\n",
      "Epoch 0, Batch 1170, Loss: 1.209609031677246\n",
      "Epoch 0, Batch 1180, Loss: 1.2446109056472778\n",
      "Epoch 0, Batch 1190, Loss: 1.27949857711792\n",
      "Epoch 0, Batch 1200, Loss: 1.1853816509246826\n",
      "Epoch 0, Batch 1210, Loss: 1.3583203554153442\n",
      "Epoch 0, Batch 1220, Loss: 1.3287545442581177\n",
      "Epoch 0, Batch 1230, Loss: 1.2950869798660278\n",
      "Epoch 0, Batch 1240, Loss: 1.3601170778274536\n",
      "Epoch 0, Batch 1250, Loss: 1.1044796705245972\n",
      "Epoch 0, Batch 1260, Loss: 1.3879917860031128\n",
      "Epoch 0, Batch 1270, Loss: 1.125125527381897\n",
      "Epoch 0, Batch 1280, Loss: 1.2349107265472412\n",
      "Epoch 0, Batch 1290, Loss: 1.2982985973358154\n",
      "Epoch 0, Batch 1300, Loss: 1.3031928539276123\n",
      "Epoch 0, Batch 1310, Loss: 1.2709294557571411\n",
      "Epoch 0, Batch 1320, Loss: 1.1714049577713013\n",
      "Epoch 0, Batch 1330, Loss: 1.4871848821640015\n",
      "Epoch 0, Batch 1340, Loss: 1.1034114360809326\n",
      "Epoch 0, Batch 1350, Loss: 1.1996036767959595\n",
      "Epoch 0, Batch 1360, Loss: 1.2192116975784302\n",
      "Epoch 0, Batch 1370, Loss: 1.2930985689163208\n",
      "Epoch 0, Batch 1380, Loss: 1.1278934478759766\n",
      "Epoch 0, Batch 1390, Loss: 1.2958959341049194\n",
      "Epoch 0, Batch 1400, Loss: 1.4199719429016113\n",
      "Epoch 0, Train Loss: 1.4374224073766562, Validation Accuracy: 53.218712514994%\n",
      "Epoch 1, Batch 0, Loss: 1.4554795026779175\n",
      "Epoch 1, Batch 10, Loss: 1.3165578842163086\n",
      "Epoch 1, Batch 20, Loss: 1.422387957572937\n",
      "Epoch 1, Batch 30, Loss: 1.3833184242248535\n",
      "Epoch 1, Batch 40, Loss: 1.2570840120315552\n",
      "Epoch 1, Batch 50, Loss: 1.268503189086914\n",
      "Epoch 1, Batch 60, Loss: 1.3657727241516113\n",
      "Epoch 1, Batch 70, Loss: 1.1347674131393433\n",
      "Epoch 1, Batch 80, Loss: 1.2575386762619019\n",
      "Epoch 1, Batch 90, Loss: 1.6320658922195435\n",
      "Epoch 1, Batch 100, Loss: 1.369266152381897\n",
      "Epoch 1, Batch 110, Loss: 1.27005934715271\n",
      "Epoch 1, Batch 120, Loss: 1.434569001197815\n",
      "Epoch 1, Batch 130, Loss: 1.1424081325531006\n",
      "Epoch 1, Batch 140, Loss: 1.211378812789917\n",
      "Epoch 1, Batch 150, Loss: 1.1784476041793823\n",
      "Epoch 1, Batch 160, Loss: 1.0864297151565552\n",
      "Epoch 1, Batch 170, Loss: 1.3290834426879883\n",
      "Epoch 1, Batch 180, Loss: 1.0369040966033936\n",
      "Epoch 1, Batch 190, Loss: 1.3911463022232056\n",
      "Epoch 1, Batch 200, Loss: 1.295911431312561\n",
      "Epoch 1, Batch 210, Loss: 1.3240584135055542\n",
      "Epoch 1, Batch 220, Loss: 1.354809045791626\n",
      "Epoch 1, Batch 230, Loss: 1.4184660911560059\n",
      "Epoch 1, Batch 240, Loss: 1.2137510776519775\n",
      "Epoch 1, Batch 250, Loss: 1.3468009233474731\n",
      "Epoch 1, Batch 260, Loss: 1.2062305212020874\n",
      "Epoch 1, Batch 270, Loss: 1.356793999671936\n",
      "Epoch 1, Batch 280, Loss: 1.1597448587417603\n",
      "Epoch 1, Batch 290, Loss: 1.2896909713745117\n",
      "Epoch 1, Batch 300, Loss: 1.2708408832550049\n",
      "Epoch 1, Batch 310, Loss: 1.3518069982528687\n",
      "Epoch 1, Batch 320, Loss: 1.0048878192901611\n",
      "Epoch 1, Batch 330, Loss: 1.316406488418579\n",
      "Epoch 1, Batch 340, Loss: 1.206744909286499\n",
      "Epoch 1, Batch 350, Loss: 1.3251923322677612\n",
      "Epoch 1, Batch 360, Loss: 0.9945294857025146\n",
      "Epoch 1, Batch 370, Loss: 1.377515196800232\n",
      "Epoch 1, Batch 380, Loss: 1.5620155334472656\n",
      "Epoch 1, Batch 390, Loss: 1.4004161357879639\n",
      "Epoch 1, Batch 400, Loss: 1.428442358970642\n",
      "Epoch 1, Batch 410, Loss: 1.2431716918945312\n",
      "Epoch 1, Batch 420, Loss: 1.0994657278060913\n",
      "Epoch 1, Batch 430, Loss: 1.2661771774291992\n",
      "Epoch 1, Batch 440, Loss: 1.3482860326766968\n",
      "Epoch 1, Batch 450, Loss: 1.299312949180603\n",
      "Epoch 1, Batch 460, Loss: 1.2334181070327759\n",
      "Epoch 1, Batch 470, Loss: 1.1854114532470703\n",
      "Epoch 1, Batch 480, Loss: 1.345145344734192\n",
      "Epoch 1, Batch 490, Loss: 1.2704188823699951\n",
      "Epoch 1, Batch 500, Loss: 1.222345232963562\n",
      "Epoch 1, Batch 510, Loss: 1.1795252561569214\n",
      "Epoch 1, Batch 520, Loss: 1.465396523475647\n",
      "Epoch 1, Batch 530, Loss: 1.2131487131118774\n",
      "Epoch 1, Batch 540, Loss: 1.2209532260894775\n",
      "Epoch 1, Batch 550, Loss: 1.4332078695297241\n",
      "Epoch 1, Batch 560, Loss: 1.3135030269622803\n",
      "Epoch 1, Batch 570, Loss: 1.3859050273895264\n",
      "Epoch 1, Batch 580, Loss: 1.2930781841278076\n",
      "Epoch 1, Batch 590, Loss: 1.3736807107925415\n",
      "Epoch 1, Batch 600, Loss: 1.0849871635437012\n",
      "Epoch 1, Batch 610, Loss: 1.2550468444824219\n",
      "Epoch 1, Batch 620, Loss: 1.3812098503112793\n",
      "Epoch 1, Batch 630, Loss: 1.3586896657943726\n",
      "Epoch 1, Batch 640, Loss: 1.1354151964187622\n",
      "Epoch 1, Batch 650, Loss: 1.1904349327087402\n",
      "Epoch 1, Batch 660, Loss: 1.2212109565734863\n",
      "Epoch 1, Batch 670, Loss: 1.4810906648635864\n",
      "Epoch 1, Batch 680, Loss: 1.2827174663543701\n",
      "Epoch 1, Batch 690, Loss: 1.2031598091125488\n",
      "Epoch 1, Batch 700, Loss: 1.3915181159973145\n",
      "Epoch 1, Batch 710, Loss: 1.3347434997558594\n",
      "Epoch 1, Batch 720, Loss: 1.0483524799346924\n",
      "Epoch 1, Batch 730, Loss: 1.4212993383407593\n",
      "Epoch 1, Batch 740, Loss: 1.1501392126083374\n",
      "Epoch 1, Batch 750, Loss: 0.9212119579315186\n",
      "Epoch 1, Batch 760, Loss: 1.4280847311019897\n",
      "Epoch 1, Batch 770, Loss: 1.3546655178070068\n",
      "Epoch 1, Batch 780, Loss: 1.3604170083999634\n",
      "Epoch 1, Batch 790, Loss: 1.4057705402374268\n",
      "Epoch 1, Batch 800, Loss: 1.4681099653244019\n",
      "Epoch 1, Batch 810, Loss: 1.2386547327041626\n",
      "Epoch 1, Batch 820, Loss: 0.9478220343589783\n",
      "Epoch 1, Batch 830, Loss: 1.1691302061080933\n",
      "Epoch 1, Batch 840, Loss: 1.0109227895736694\n",
      "Epoch 1, Batch 850, Loss: 1.21280038356781\n",
      "Epoch 1, Batch 860, Loss: 1.2957375049591064\n",
      "Epoch 1, Batch 870, Loss: 1.2029776573181152\n",
      "Epoch 1, Batch 880, Loss: 1.2029714584350586\n",
      "Epoch 1, Batch 890, Loss: 1.1479171514511108\n",
      "Epoch 1, Batch 900, Loss: 1.214117169380188\n",
      "Epoch 1, Batch 910, Loss: 1.2522821426391602\n",
      "Epoch 1, Batch 920, Loss: 1.2550066709518433\n",
      "Epoch 1, Batch 930, Loss: 1.3639825582504272\n",
      "Epoch 1, Batch 940, Loss: 1.3630143404006958\n",
      "Epoch 1, Batch 950, Loss: 1.196937918663025\n",
      "Epoch 1, Batch 960, Loss: 1.220689296722412\n",
      "Epoch 1, Batch 970, Loss: 1.2909808158874512\n",
      "Epoch 1, Batch 980, Loss: 1.1833322048187256\n",
      "Epoch 1, Batch 990, Loss: 1.368085503578186\n",
      "Epoch 1, Batch 1000, Loss: 1.2615357637405396\n",
      "Epoch 1, Batch 1010, Loss: 1.2415225505828857\n",
      "Epoch 1, Batch 1020, Loss: 1.2077223062515259\n",
      "Epoch 1, Batch 1030, Loss: 1.0647211074829102\n",
      "Epoch 1, Batch 1040, Loss: 1.2687005996704102\n",
      "Epoch 1, Batch 1050, Loss: 1.2442933320999146\n",
      "Epoch 1, Batch 1060, Loss: 1.1973228454589844\n",
      "Epoch 1, Batch 1070, Loss: 1.1711726188659668\n",
      "Epoch 1, Batch 1080, Loss: 1.2430015802383423\n",
      "Epoch 1, Batch 1090, Loss: 1.2444161176681519\n",
      "Epoch 1, Batch 1100, Loss: 0.9806156754493713\n",
      "Epoch 1, Batch 1110, Loss: 1.3375059366226196\n",
      "Epoch 1, Batch 1120, Loss: 1.2087498903274536\n",
      "Epoch 1, Batch 1130, Loss: 1.3883111476898193\n",
      "Epoch 1, Batch 1140, Loss: 1.2646400928497314\n",
      "Epoch 1, Batch 1150, Loss: 1.0666865110397339\n",
      "Epoch 1, Batch 1160, Loss: 1.0692579746246338\n",
      "Epoch 1, Batch 1170, Loss: 1.7048555612564087\n",
      "Epoch 1, Batch 1180, Loss: 1.2413281202316284\n",
      "Epoch 1, Batch 1190, Loss: 1.2725205421447754\n",
      "Epoch 1, Batch 1200, Loss: 1.228001356124878\n",
      "Epoch 1, Batch 1210, Loss: 1.1817355155944824\n",
      "Epoch 1, Batch 1220, Loss: 1.3272087574005127\n",
      "Epoch 1, Batch 1230, Loss: 1.1737980842590332\n",
      "Epoch 1, Batch 1240, Loss: 1.4035917520523071\n",
      "Epoch 1, Batch 1250, Loss: 1.4344478845596313\n",
      "Epoch 1, Batch 1260, Loss: 1.5262242555618286\n",
      "Epoch 1, Batch 1270, Loss: 1.0635586977005005\n",
      "Epoch 1, Batch 1280, Loss: 1.252325415611267\n",
      "Epoch 1, Batch 1290, Loss: 1.2178795337677002\n",
      "Epoch 1, Batch 1300, Loss: 1.0840342044830322\n",
      "Epoch 1, Batch 1310, Loss: 1.304034948348999\n",
      "Epoch 1, Batch 1320, Loss: 1.2222330570220947\n",
      "Epoch 1, Batch 1330, Loss: 1.355049729347229\n",
      "Epoch 1, Batch 1340, Loss: 1.1825052499771118\n",
      "Epoch 1, Batch 1350, Loss: 1.180298924446106\n",
      "Epoch 1, Batch 1360, Loss: 1.2383460998535156\n",
      "Epoch 1, Batch 1370, Loss: 1.3789578676223755\n",
      "Epoch 1, Batch 1380, Loss: 1.0691025257110596\n",
      "Epoch 1, Batch 1390, Loss: 1.3598296642303467\n",
      "Epoch 1, Batch 1400, Loss: 1.1313245296478271\n",
      "Epoch 1, Train Loss: 1.2564938298844242, Validation Accuracy: 54.58816473410636%\n",
      "Epoch 2, Batch 0, Loss: 1.2360618114471436\n",
      "Epoch 2, Batch 10, Loss: 1.054059386253357\n",
      "Epoch 2, Batch 20, Loss: 1.4144225120544434\n",
      "Epoch 2, Batch 30, Loss: 1.396551489830017\n",
      "Epoch 2, Batch 40, Loss: 1.393951654434204\n",
      "Epoch 2, Batch 50, Loss: 1.0298110246658325\n",
      "Epoch 2, Batch 60, Loss: 1.111650824546814\n",
      "Epoch 2, Batch 70, Loss: 0.9478178024291992\n",
      "Epoch 2, Batch 80, Loss: 1.1067867279052734\n",
      "Epoch 2, Batch 90, Loss: 1.2894045114517212\n",
      "Epoch 2, Batch 100, Loss: 1.1875360012054443\n",
      "Epoch 2, Batch 110, Loss: 1.2398937940597534\n",
      "Epoch 2, Batch 120, Loss: 1.2214627265930176\n",
      "Epoch 2, Batch 130, Loss: 1.306221604347229\n",
      "Epoch 2, Batch 140, Loss: 1.0760711431503296\n",
      "Epoch 2, Batch 150, Loss: 1.3100841045379639\n",
      "Epoch 2, Batch 160, Loss: 1.2164093255996704\n",
      "Epoch 2, Batch 170, Loss: 1.2540355920791626\n",
      "Epoch 2, Batch 180, Loss: 1.2098438739776611\n",
      "Epoch 2, Batch 190, Loss: 1.0378873348236084\n",
      "Epoch 2, Batch 200, Loss: 1.3809142112731934\n",
      "Epoch 2, Batch 210, Loss: 1.127910852432251\n",
      "Epoch 2, Batch 220, Loss: 1.162168264389038\n",
      "Epoch 2, Batch 230, Loss: 1.4188281297683716\n",
      "Epoch 2, Batch 240, Loss: 1.356497049331665\n",
      "Epoch 2, Batch 250, Loss: 1.075925588607788\n",
      "Epoch 2, Batch 260, Loss: 1.1422321796417236\n",
      "Epoch 2, Batch 270, Loss: 1.337200403213501\n",
      "Epoch 2, Batch 280, Loss: 1.4904201030731201\n",
      "Epoch 2, Batch 290, Loss: 1.0734248161315918\n",
      "Epoch 2, Batch 300, Loss: 1.3382781744003296\n",
      "Epoch 2, Batch 310, Loss: 1.2277933359146118\n",
      "Epoch 2, Batch 320, Loss: 1.1503849029541016\n",
      "Epoch 2, Batch 330, Loss: 1.2020418643951416\n",
      "Epoch 2, Batch 340, Loss: 1.3164819478988647\n",
      "Epoch 2, Batch 350, Loss: 1.154714822769165\n",
      "Epoch 2, Batch 360, Loss: 1.188774824142456\n",
      "Epoch 2, Batch 370, Loss: 1.0846410989761353\n",
      "Epoch 2, Batch 380, Loss: 1.2617474794387817\n",
      "Epoch 2, Batch 390, Loss: 1.3640408515930176\n",
      "Epoch 2, Batch 400, Loss: 1.0296039581298828\n",
      "Epoch 2, Batch 410, Loss: 1.2038252353668213\n",
      "Epoch 2, Batch 420, Loss: 1.194400429725647\n",
      "Epoch 2, Batch 430, Loss: 1.1955212354660034\n",
      "Epoch 2, Batch 440, Loss: 1.1983011960983276\n",
      "Epoch 2, Batch 450, Loss: 1.1406819820404053\n",
      "Epoch 2, Batch 460, Loss: 1.2369530200958252\n",
      "Epoch 2, Batch 470, Loss: 1.1414389610290527\n",
      "Epoch 2, Batch 480, Loss: 1.334375023841858\n",
      "Epoch 2, Batch 490, Loss: 1.0297634601593018\n",
      "Epoch 2, Batch 500, Loss: 1.2794946432113647\n",
      "Epoch 2, Batch 510, Loss: 1.4707509279251099\n",
      "Epoch 2, Batch 520, Loss: 1.318994402885437\n",
      "Epoch 2, Batch 530, Loss: 1.1187102794647217\n",
      "Epoch 2, Batch 540, Loss: 1.1175631284713745\n",
      "Epoch 2, Batch 550, Loss: 1.2762519121170044\n",
      "Epoch 2, Batch 560, Loss: 1.1774320602416992\n",
      "Epoch 2, Batch 570, Loss: 1.2520489692687988\n",
      "Epoch 2, Batch 580, Loss: 1.1447222232818604\n",
      "Epoch 2, Batch 590, Loss: 1.289961576461792\n",
      "Epoch 2, Batch 600, Loss: 1.2214882373809814\n",
      "Epoch 2, Batch 610, Loss: 1.244828462600708\n",
      "Epoch 2, Batch 620, Loss: 1.0735392570495605\n",
      "Epoch 2, Batch 630, Loss: 1.3714526891708374\n",
      "Epoch 2, Batch 640, Loss: 1.2403720617294312\n",
      "Epoch 2, Batch 650, Loss: 1.4135713577270508\n",
      "Epoch 2, Batch 660, Loss: 1.218464970588684\n",
      "Epoch 2, Batch 670, Loss: 1.069096326828003\n",
      "Epoch 2, Batch 680, Loss: 1.0747127532958984\n",
      "Epoch 2, Batch 690, Loss: 1.3457696437835693\n",
      "Epoch 2, Batch 700, Loss: 1.1824281215667725\n",
      "Epoch 2, Batch 710, Loss: 1.2754764556884766\n",
      "Epoch 2, Batch 720, Loss: 1.2060468196868896\n",
      "Epoch 2, Batch 730, Loss: 1.4243249893188477\n",
      "Epoch 2, Batch 740, Loss: 1.0955935716629028\n",
      "Epoch 2, Batch 750, Loss: 1.1235295534133911\n",
      "Epoch 2, Batch 760, Loss: 1.4337482452392578\n",
      "Epoch 2, Batch 770, Loss: 1.098097324371338\n",
      "Epoch 2, Batch 780, Loss: 1.1131426095962524\n",
      "Epoch 2, Batch 790, Loss: 1.2090559005737305\n",
      "Epoch 2, Batch 800, Loss: 1.100816249847412\n",
      "Epoch 2, Batch 810, Loss: 1.2132047414779663\n",
      "Epoch 2, Batch 820, Loss: 1.0737069845199585\n",
      "Epoch 2, Batch 830, Loss: 1.21566641330719\n",
      "Epoch 2, Batch 840, Loss: 0.9261994957923889\n",
      "Epoch 2, Batch 850, Loss: 1.2169253826141357\n",
      "Epoch 2, Batch 860, Loss: 1.1200897693634033\n",
      "Epoch 2, Batch 870, Loss: 1.235021710395813\n",
      "Epoch 2, Batch 880, Loss: 1.090564489364624\n",
      "Epoch 2, Batch 890, Loss: 1.2347928285598755\n",
      "Epoch 2, Batch 900, Loss: 1.3302092552185059\n",
      "Epoch 2, Batch 910, Loss: 1.0784587860107422\n",
      "Epoch 2, Batch 920, Loss: 1.4594323635101318\n",
      "Epoch 2, Batch 930, Loss: 1.23453950881958\n",
      "Epoch 2, Batch 940, Loss: 1.298097014427185\n",
      "Epoch 2, Batch 950, Loss: 1.242264986038208\n",
      "Epoch 2, Batch 960, Loss: 1.2957878112792969\n",
      "Epoch 2, Batch 970, Loss: 1.4624426364898682\n",
      "Epoch 2, Batch 980, Loss: 1.1906033754348755\n",
      "Epoch 2, Batch 990, Loss: 1.3785991668701172\n",
      "Epoch 2, Batch 1000, Loss: 1.3456919193267822\n",
      "Epoch 2, Batch 1010, Loss: 1.1763074398040771\n",
      "Epoch 2, Batch 1020, Loss: 1.2400199174880981\n",
      "Epoch 2, Batch 1030, Loss: 1.0849791765213013\n",
      "Epoch 2, Batch 1040, Loss: 1.206976294517517\n",
      "Epoch 2, Batch 1050, Loss: 0.9982611536979675\n",
      "Epoch 2, Batch 1060, Loss: 1.3645788431167603\n",
      "Epoch 2, Batch 1070, Loss: 1.2396185398101807\n",
      "Epoch 2, Batch 1080, Loss: 1.4178016185760498\n",
      "Epoch 2, Batch 1090, Loss: 1.2777358293533325\n",
      "Epoch 2, Batch 1100, Loss: 1.2977572679519653\n",
      "Epoch 2, Batch 1110, Loss: 1.342678427696228\n",
      "Epoch 2, Batch 1120, Loss: 1.3180115222930908\n",
      "Epoch 2, Batch 1130, Loss: 1.1176810264587402\n",
      "Epoch 2, Batch 1140, Loss: 1.1109747886657715\n",
      "Epoch 2, Batch 1150, Loss: 1.3456811904907227\n",
      "Epoch 2, Batch 1160, Loss: 1.0995765924453735\n",
      "Epoch 2, Batch 1170, Loss: 1.3199944496154785\n",
      "Epoch 2, Batch 1180, Loss: 1.295715093612671\n",
      "Epoch 2, Batch 1190, Loss: 1.3050663471221924\n",
      "Epoch 2, Batch 1200, Loss: 1.0526000261306763\n",
      "Epoch 2, Batch 1210, Loss: 1.250610589981079\n",
      "Epoch 2, Batch 1220, Loss: 1.12320077419281\n",
      "Epoch 2, Batch 1230, Loss: 1.1485264301300049\n",
      "Epoch 2, Batch 1240, Loss: 1.1324999332427979\n",
      "Epoch 2, Batch 1250, Loss: 1.2002990245819092\n",
      "Epoch 2, Batch 1260, Loss: 1.2660609483718872\n",
      "Epoch 2, Batch 1270, Loss: 1.2537509202957153\n",
      "Epoch 2, Batch 1280, Loss: 1.2048254013061523\n",
      "Epoch 2, Batch 1290, Loss: 1.0967581272125244\n",
      "Epoch 2, Batch 1300, Loss: 1.196946144104004\n",
      "Epoch 2, Batch 1310, Loss: 1.1773481369018555\n",
      "Epoch 2, Batch 1320, Loss: 1.0787065029144287\n",
      "Epoch 2, Batch 1330, Loss: 1.107059359550476\n",
      "Epoch 2, Batch 1340, Loss: 1.080749750137329\n",
      "Epoch 2, Batch 1350, Loss: 1.1252093315124512\n",
      "Epoch 2, Batch 1360, Loss: 1.0506079196929932\n",
      "Epoch 2, Batch 1370, Loss: 1.4396469593048096\n",
      "Epoch 2, Batch 1380, Loss: 1.2095012664794922\n",
      "Epoch 2, Batch 1390, Loss: 1.2765214443206787\n",
      "Epoch 2, Batch 1400, Loss: 1.3123629093170166\n",
      "Epoch 2, Train Loss: 1.2310930627749672, Validation Accuracy: 55.28788484606157%\n",
      "Epoch 3, Batch 0, Loss: 1.0672730207443237\n",
      "Epoch 3, Batch 10, Loss: 1.196045994758606\n",
      "Epoch 3, Batch 20, Loss: 0.9597722887992859\n",
      "Epoch 3, Batch 30, Loss: 1.2127479314804077\n",
      "Epoch 3, Batch 40, Loss: 1.2025136947631836\n",
      "Epoch 3, Batch 50, Loss: 1.2919963598251343\n",
      "Epoch 3, Batch 60, Loss: 0.9526002407073975\n",
      "Epoch 3, Batch 70, Loss: 1.1225457191467285\n",
      "Epoch 3, Batch 80, Loss: 1.4530905485153198\n",
      "Epoch 3, Batch 90, Loss: 1.2733439207077026\n",
      "Epoch 3, Batch 100, Loss: 1.3492162227630615\n",
      "Epoch 3, Batch 110, Loss: 1.2307252883911133\n",
      "Epoch 3, Batch 120, Loss: 0.9117509126663208\n",
      "Epoch 3, Batch 130, Loss: 1.0835267305374146\n",
      "Epoch 3, Batch 140, Loss: 1.2936607599258423\n",
      "Epoch 3, Batch 150, Loss: 1.1698893308639526\n",
      "Epoch 3, Batch 160, Loss: 1.394673466682434\n",
      "Epoch 3, Batch 170, Loss: 1.2547814846038818\n",
      "Epoch 3, Batch 180, Loss: 1.3731273412704468\n",
      "Epoch 3, Batch 190, Loss: 1.030367136001587\n",
      "Epoch 3, Batch 200, Loss: 1.2488797903060913\n",
      "Epoch 3, Batch 210, Loss: 1.2849208116531372\n",
      "Epoch 3, Batch 220, Loss: 1.4774458408355713\n",
      "Epoch 3, Batch 230, Loss: 1.2824726104736328\n",
      "Epoch 3, Batch 240, Loss: 1.2556852102279663\n",
      "Epoch 3, Batch 250, Loss: 1.0589776039123535\n",
      "Epoch 3, Batch 260, Loss: 1.1129111051559448\n",
      "Epoch 3, Batch 270, Loss: 1.1850194931030273\n",
      "Epoch 3, Batch 280, Loss: 1.232256531715393\n",
      "Epoch 3, Batch 290, Loss: 1.2370871305465698\n",
      "Epoch 3, Batch 300, Loss: 1.1158827543258667\n",
      "Epoch 3, Batch 310, Loss: 1.3575928211212158\n",
      "Epoch 3, Batch 320, Loss: 1.0919452905654907\n",
      "Epoch 3, Batch 330, Loss: 1.0772279500961304\n",
      "Epoch 3, Batch 340, Loss: 1.283644676208496\n",
      "Epoch 3, Batch 350, Loss: 1.236937165260315\n",
      "Epoch 3, Batch 360, Loss: 1.2000834941864014\n",
      "Epoch 3, Batch 370, Loss: 1.3795902729034424\n",
      "Epoch 3, Batch 380, Loss: 1.219845175743103\n",
      "Epoch 3, Batch 390, Loss: 1.186130166053772\n",
      "Epoch 3, Batch 400, Loss: 1.311949610710144\n",
      "Epoch 3, Batch 410, Loss: 1.226981520652771\n",
      "Epoch 3, Batch 420, Loss: 1.29358971118927\n",
      "Epoch 3, Batch 430, Loss: 1.0148884057998657\n",
      "Epoch 3, Batch 440, Loss: 1.1344237327575684\n",
      "Epoch 3, Batch 450, Loss: 1.3052632808685303\n",
      "Epoch 3, Batch 460, Loss: 0.9907053709030151\n",
      "Epoch 3, Batch 470, Loss: 1.310521125793457\n",
      "Epoch 3, Batch 480, Loss: 1.0623410940170288\n",
      "Epoch 3, Batch 490, Loss: 1.2306132316589355\n",
      "Epoch 3, Batch 500, Loss: 1.2308664321899414\n",
      "Epoch 3, Batch 510, Loss: 1.3305139541625977\n",
      "Epoch 3, Batch 520, Loss: 1.1127876043319702\n",
      "Epoch 3, Batch 530, Loss: 1.0502963066101074\n",
      "Epoch 3, Batch 540, Loss: 1.2197691202163696\n",
      "Epoch 3, Batch 550, Loss: 1.250098466873169\n",
      "Epoch 3, Batch 560, Loss: 1.3414816856384277\n",
      "Epoch 3, Batch 570, Loss: 1.4950215816497803\n",
      "Epoch 3, Batch 580, Loss: 1.3011025190353394\n",
      "Epoch 3, Batch 590, Loss: 1.1434599161148071\n",
      "Epoch 3, Batch 600, Loss: 1.127313494682312\n",
      "Epoch 3, Batch 610, Loss: 1.1270183324813843\n",
      "Epoch 3, Batch 620, Loss: 1.3410950899124146\n",
      "Epoch 3, Batch 630, Loss: 1.060492992401123\n",
      "Epoch 3, Batch 640, Loss: 1.3377220630645752\n",
      "Epoch 3, Batch 650, Loss: 1.294538140296936\n",
      "Epoch 3, Batch 660, Loss: 1.0405464172363281\n",
      "Epoch 3, Batch 670, Loss: 1.0246636867523193\n",
      "Epoch 3, Batch 680, Loss: 1.2019855976104736\n",
      "Epoch 3, Batch 690, Loss: 1.2700905799865723\n",
      "Epoch 3, Batch 700, Loss: 1.118370532989502\n",
      "Epoch 3, Batch 710, Loss: 1.3433113098144531\n",
      "Epoch 3, Batch 720, Loss: 1.2111154794692993\n",
      "Epoch 3, Batch 730, Loss: 1.126080870628357\n",
      "Epoch 3, Batch 740, Loss: 1.1283683776855469\n",
      "Epoch 3, Batch 750, Loss: 1.3207299709320068\n",
      "Epoch 3, Batch 760, Loss: 1.0262696743011475\n",
      "Epoch 3, Batch 770, Loss: 1.3336052894592285\n",
      "Epoch 3, Batch 780, Loss: 1.3308024406433105\n",
      "Epoch 3, Batch 790, Loss: 1.1686707735061646\n",
      "Epoch 3, Batch 800, Loss: 1.3040406703948975\n",
      "Epoch 3, Batch 810, Loss: 1.0947380065917969\n",
      "Epoch 3, Batch 820, Loss: 1.195339322090149\n",
      "Epoch 3, Batch 830, Loss: 1.2686893939971924\n",
      "Epoch 3, Batch 840, Loss: 1.2443318367004395\n",
      "Epoch 3, Batch 850, Loss: 1.1762090921401978\n",
      "Epoch 3, Batch 860, Loss: 1.2308769226074219\n",
      "Epoch 3, Batch 870, Loss: 1.1282682418823242\n",
      "Epoch 3, Batch 880, Loss: 1.340590476989746\n",
      "Epoch 3, Batch 890, Loss: 1.3048219680786133\n",
      "Epoch 3, Batch 900, Loss: 1.311004400253296\n",
      "Epoch 3, Batch 910, Loss: 1.313869833946228\n",
      "Epoch 3, Batch 920, Loss: 1.440109133720398\n",
      "Epoch 3, Batch 930, Loss: 0.8657868504524231\n",
      "Epoch 3, Batch 940, Loss: 1.1241178512573242\n",
      "Epoch 3, Batch 950, Loss: 1.4966049194335938\n",
      "Epoch 3, Batch 960, Loss: 1.2350765466690063\n",
      "Epoch 3, Batch 970, Loss: 1.347123384475708\n",
      "Epoch 3, Batch 980, Loss: 1.070251703262329\n",
      "Epoch 3, Batch 990, Loss: 1.0779469013214111\n",
      "Epoch 3, Batch 1000, Loss: 1.13912034034729\n",
      "Epoch 3, Batch 1010, Loss: 1.096295714378357\n",
      "Epoch 3, Batch 1020, Loss: 1.1844518184661865\n",
      "Epoch 3, Batch 1030, Loss: 1.1759413480758667\n",
      "Epoch 3, Batch 1040, Loss: 1.06710946559906\n",
      "Epoch 3, Batch 1050, Loss: 1.365700125694275\n",
      "Epoch 3, Batch 1060, Loss: 1.532306432723999\n",
      "Epoch 3, Batch 1070, Loss: 1.381839632987976\n",
      "Epoch 3, Batch 1080, Loss: 1.1854842901229858\n",
      "Epoch 3, Batch 1090, Loss: 1.3007491827011108\n",
      "Epoch 3, Batch 1100, Loss: 1.1654995679855347\n",
      "Epoch 3, Batch 1110, Loss: 1.1727383136749268\n",
      "Epoch 3, Batch 1120, Loss: 1.3127328157424927\n",
      "Epoch 3, Batch 1130, Loss: 1.257715106010437\n",
      "Epoch 3, Batch 1140, Loss: 1.1669306755065918\n",
      "Epoch 3, Batch 1150, Loss: 1.2545058727264404\n",
      "Epoch 3, Batch 1160, Loss: 1.3889373540878296\n",
      "Epoch 3, Batch 1170, Loss: 1.1165623664855957\n",
      "Epoch 3, Batch 1180, Loss: 1.2753326892852783\n",
      "Epoch 3, Batch 1190, Loss: 1.196699857711792\n",
      "Epoch 3, Batch 1200, Loss: 1.2123740911483765\n",
      "Epoch 3, Batch 1210, Loss: 1.0279464721679688\n",
      "Epoch 3, Batch 1220, Loss: 1.319198727607727\n",
      "Epoch 3, Batch 1230, Loss: 1.1347222328186035\n",
      "Epoch 3, Batch 1240, Loss: 1.5093493461608887\n",
      "Epoch 3, Batch 1250, Loss: 1.4682947397232056\n",
      "Epoch 3, Batch 1260, Loss: 1.0476075410842896\n",
      "Epoch 3, Batch 1270, Loss: 1.2003068923950195\n",
      "Epoch 3, Batch 1280, Loss: 1.278279185295105\n",
      "Epoch 3, Batch 1290, Loss: 0.8719170093536377\n",
      "Epoch 3, Batch 1300, Loss: 0.9583115577697754\n",
      "Epoch 3, Batch 1310, Loss: 1.0849077701568604\n",
      "Epoch 3, Batch 1320, Loss: 1.0172392129898071\n",
      "Epoch 3, Batch 1330, Loss: 1.2097487449645996\n",
      "Epoch 3, Batch 1340, Loss: 1.1421914100646973\n",
      "Epoch 3, Batch 1350, Loss: 1.3289432525634766\n",
      "Epoch 3, Batch 1360, Loss: 1.1543376445770264\n",
      "Epoch 3, Batch 1370, Loss: 1.3271992206573486\n",
      "Epoch 3, Batch 1380, Loss: 1.1766772270202637\n",
      "Epoch 3, Batch 1390, Loss: 1.2917579412460327\n",
      "Epoch 3, Batch 1400, Loss: 0.9797868132591248\n",
      "Epoch 3, Train Loss: 1.2164886886970152, Validation Accuracy: 56.427429028388644%\n",
      "Epoch 4, Batch 0, Loss: 1.3587608337402344\n",
      "Epoch 4, Batch 10, Loss: 1.0868107080459595\n",
      "Epoch 4, Batch 20, Loss: 1.0286693572998047\n",
      "Epoch 4, Batch 30, Loss: 1.0881612300872803\n",
      "Epoch 4, Batch 40, Loss: 0.9209914207458496\n",
      "Epoch 4, Batch 50, Loss: 1.2827574014663696\n",
      "Epoch 4, Batch 60, Loss: 1.3530189990997314\n",
      "Epoch 4, Batch 70, Loss: 1.26947820186615\n",
      "Epoch 4, Batch 80, Loss: 1.2818971872329712\n",
      "Epoch 4, Batch 90, Loss: 1.1087238788604736\n",
      "Epoch 4, Batch 100, Loss: 1.0956403017044067\n",
      "Epoch 4, Batch 110, Loss: 1.3592334985733032\n",
      "Epoch 4, Batch 120, Loss: 0.9767357110977173\n",
      "Epoch 4, Batch 130, Loss: 1.683658242225647\n",
      "Epoch 4, Batch 140, Loss: 1.0778156518936157\n",
      "Epoch 4, Batch 150, Loss: 1.2916501760482788\n",
      "Epoch 4, Batch 160, Loss: 1.1446830034255981\n",
      "Epoch 4, Batch 170, Loss: 1.181236743927002\n",
      "Epoch 4, Batch 180, Loss: 1.408098816871643\n",
      "Epoch 4, Batch 190, Loss: 1.2835168838500977\n",
      "Epoch 4, Batch 200, Loss: 1.182417392730713\n",
      "Epoch 4, Batch 210, Loss: 1.3494876623153687\n",
      "Epoch 4, Batch 220, Loss: 1.192861795425415\n",
      "Epoch 4, Batch 230, Loss: 1.3586513996124268\n",
      "Epoch 4, Batch 240, Loss: 1.0629535913467407\n",
      "Epoch 4, Batch 250, Loss: 1.0536160469055176\n",
      "Epoch 4, Batch 260, Loss: 1.4707869291305542\n",
      "Epoch 4, Batch 270, Loss: 1.3307163715362549\n",
      "Epoch 4, Batch 280, Loss: 1.190245270729065\n",
      "Epoch 4, Batch 290, Loss: 1.3256572484970093\n",
      "Epoch 4, Batch 300, Loss: 1.4295531511306763\n",
      "Epoch 4, Batch 310, Loss: 1.3675017356872559\n",
      "Epoch 4, Batch 320, Loss: 1.1806068420410156\n",
      "Epoch 4, Batch 330, Loss: 1.1041120290756226\n",
      "Epoch 4, Batch 340, Loss: 1.3277534246444702\n",
      "Epoch 4, Batch 350, Loss: 1.2738844156265259\n",
      "Epoch 4, Batch 360, Loss: 1.0787749290466309\n",
      "Epoch 4, Batch 370, Loss: 1.035635232925415\n",
      "Epoch 4, Batch 380, Loss: 1.27115797996521\n",
      "Epoch 4, Batch 390, Loss: 1.1687822341918945\n",
      "Epoch 4, Batch 400, Loss: 1.3792595863342285\n",
      "Epoch 4, Batch 410, Loss: 1.2204090356826782\n",
      "Epoch 4, Batch 420, Loss: 1.381527066230774\n",
      "Epoch 4, Batch 430, Loss: 1.2711491584777832\n",
      "Epoch 4, Batch 440, Loss: 1.2247246503829956\n",
      "Epoch 4, Batch 450, Loss: 1.0028092861175537\n",
      "Epoch 4, Batch 460, Loss: 1.159010648727417\n",
      "Epoch 4, Batch 470, Loss: 1.3134163618087769\n",
      "Epoch 4, Batch 480, Loss: 1.2914079427719116\n",
      "Epoch 4, Batch 490, Loss: 1.3081077337265015\n",
      "Epoch 4, Batch 500, Loss: 1.1695681810379028\n",
      "Epoch 4, Batch 510, Loss: 1.075188159942627\n",
      "Epoch 4, Batch 520, Loss: 1.0982483625411987\n",
      "Epoch 4, Batch 530, Loss: 1.3217793703079224\n",
      "Epoch 4, Batch 540, Loss: 0.9679571986198425\n",
      "Epoch 4, Batch 550, Loss: 1.1722742319107056\n",
      "Epoch 4, Batch 560, Loss: 1.1367210149765015\n",
      "Epoch 4, Batch 570, Loss: 1.1244747638702393\n",
      "Epoch 4, Batch 580, Loss: 1.364660620689392\n",
      "Epoch 4, Batch 590, Loss: 1.0859830379486084\n",
      "Epoch 4, Batch 600, Loss: 1.1338729858398438\n",
      "Epoch 4, Batch 610, Loss: 1.2163467407226562\n",
      "Epoch 4, Batch 620, Loss: 1.0176477432250977\n",
      "Epoch 4, Batch 630, Loss: 1.0287903547286987\n",
      "Epoch 4, Batch 640, Loss: 1.1339695453643799\n",
      "Epoch 4, Batch 650, Loss: 1.2104110717773438\n",
      "Epoch 4, Batch 660, Loss: 1.1690223217010498\n",
      "Epoch 4, Batch 670, Loss: 1.1706470251083374\n",
      "Epoch 4, Batch 680, Loss: 1.262392520904541\n",
      "Epoch 4, Batch 690, Loss: 1.4655643701553345\n",
      "Epoch 4, Batch 700, Loss: 1.2171214818954468\n",
      "Epoch 4, Batch 710, Loss: 1.1866044998168945\n",
      "Epoch 4, Batch 720, Loss: 1.074254035949707\n",
      "Epoch 4, Batch 730, Loss: 1.300050139427185\n",
      "Epoch 4, Batch 740, Loss: 0.954979419708252\n",
      "Epoch 4, Batch 750, Loss: 1.1988019943237305\n",
      "Epoch 4, Batch 760, Loss: 1.1403917074203491\n",
      "Epoch 4, Batch 770, Loss: 1.1330498456954956\n",
      "Epoch 4, Batch 780, Loss: 1.2138781547546387\n",
      "Epoch 4, Batch 790, Loss: 1.1462528705596924\n",
      "Epoch 4, Batch 800, Loss: 1.1699116230010986\n",
      "Epoch 4, Batch 810, Loss: 1.0147426128387451\n",
      "Epoch 4, Batch 820, Loss: 1.0907011032104492\n",
      "Epoch 4, Batch 830, Loss: 1.448910117149353\n",
      "Epoch 4, Batch 840, Loss: 1.4425855875015259\n",
      "Epoch 4, Batch 850, Loss: 1.3061819076538086\n",
      "Epoch 4, Batch 860, Loss: 1.031279444694519\n",
      "Epoch 4, Batch 870, Loss: 1.2168896198272705\n",
      "Epoch 4, Batch 880, Loss: 1.1949076652526855\n",
      "Epoch 4, Batch 890, Loss: 1.224408507347107\n",
      "Epoch 4, Batch 900, Loss: 1.134021520614624\n",
      "Epoch 4, Batch 910, Loss: 1.1533541679382324\n",
      "Epoch 4, Batch 920, Loss: 1.2686301469802856\n",
      "Epoch 4, Batch 930, Loss: 1.1366409063339233\n",
      "Epoch 4, Batch 940, Loss: 1.1913220882415771\n",
      "Epoch 4, Batch 950, Loss: 1.0147892236709595\n",
      "Epoch 4, Batch 960, Loss: 1.36798894405365\n",
      "Epoch 4, Batch 970, Loss: 1.2555476427078247\n",
      "Epoch 4, Batch 980, Loss: 1.2834314107894897\n",
      "Epoch 4, Batch 990, Loss: 1.1577280759811401\n",
      "Epoch 4, Batch 1000, Loss: 1.2925395965576172\n",
      "Epoch 4, Batch 1010, Loss: 0.9070352911949158\n",
      "Epoch 4, Batch 1020, Loss: 1.2926024198532104\n",
      "Epoch 4, Batch 1030, Loss: 1.3270998001098633\n",
      "Epoch 4, Batch 1040, Loss: 1.1780849695205688\n",
      "Epoch 4, Batch 1050, Loss: 1.18963623046875\n",
      "Epoch 4, Batch 1060, Loss: 1.4124699831008911\n",
      "Epoch 4, Batch 1070, Loss: 0.9702621102333069\n",
      "Epoch 4, Batch 1080, Loss: 1.039109468460083\n",
      "Epoch 4, Batch 1090, Loss: 1.2048213481903076\n",
      "Epoch 4, Batch 1100, Loss: 1.351912021636963\n",
      "Epoch 4, Batch 1110, Loss: 0.8811258673667908\n",
      "Epoch 4, Batch 1120, Loss: 1.2825565338134766\n",
      "Epoch 4, Batch 1130, Loss: 1.2930108308792114\n",
      "Epoch 4, Batch 1140, Loss: 1.1709914207458496\n",
      "Epoch 4, Batch 1150, Loss: 1.140445590019226\n",
      "Epoch 4, Batch 1160, Loss: 1.2501260042190552\n",
      "Epoch 4, Batch 1170, Loss: 1.461429476737976\n",
      "Epoch 4, Batch 1180, Loss: 1.3158445358276367\n",
      "Epoch 4, Batch 1190, Loss: 1.34894597530365\n",
      "Epoch 4, Batch 1200, Loss: 1.1619001626968384\n",
      "Epoch 4, Batch 1210, Loss: 1.2228484153747559\n",
      "Epoch 4, Batch 1220, Loss: 1.2530102729797363\n",
      "Epoch 4, Batch 1230, Loss: 1.1517411470413208\n",
      "Epoch 4, Batch 1240, Loss: 1.0499979257583618\n",
      "Epoch 4, Batch 1250, Loss: 1.374827265739441\n",
      "Epoch 4, Batch 1260, Loss: 1.1187708377838135\n",
      "Epoch 4, Batch 1270, Loss: 1.1849400997161865\n",
      "Epoch 4, Batch 1280, Loss: 1.3911291360855103\n",
      "Epoch 4, Batch 1290, Loss: 1.190706491470337\n",
      "Epoch 4, Batch 1300, Loss: 1.2096978425979614\n",
      "Epoch 4, Batch 1310, Loss: 0.977694571018219\n",
      "Epoch 4, Batch 1320, Loss: 1.0001134872436523\n",
      "Epoch 4, Batch 1330, Loss: 0.9082515835762024\n",
      "Epoch 4, Batch 1340, Loss: 1.4909632205963135\n",
      "Epoch 4, Batch 1350, Loss: 1.2445317506790161\n",
      "Epoch 4, Batch 1360, Loss: 1.3594422340393066\n",
      "Epoch 4, Batch 1370, Loss: 1.023083209991455\n",
      "Epoch 4, Batch 1380, Loss: 1.1646690368652344\n",
      "Epoch 4, Batch 1390, Loss: 1.0454596281051636\n",
      "Epoch 4, Batch 1400, Loss: 1.2260804176330566\n",
      "Epoch 4, Train Loss: 1.2061976325706802, Validation Accuracy: 56.16753298680528%\n",
      "Epoch 5, Batch 0, Loss: 1.0646910667419434\n",
      "Epoch 5, Batch 10, Loss: 1.0142110586166382\n",
      "Epoch 5, Batch 20, Loss: 1.2088004350662231\n",
      "Epoch 5, Batch 30, Loss: 1.2538868188858032\n",
      "Epoch 5, Batch 40, Loss: 0.9728355407714844\n",
      "Epoch 5, Batch 50, Loss: 1.0828379392623901\n",
      "Epoch 5, Batch 60, Loss: 1.2135775089263916\n",
      "Epoch 5, Batch 70, Loss: 1.1547176837921143\n",
      "Epoch 5, Batch 80, Loss: 1.2283486127853394\n",
      "Epoch 5, Batch 90, Loss: 1.439502477645874\n",
      "Epoch 5, Batch 100, Loss: 1.1329339742660522\n",
      "Epoch 5, Batch 110, Loss: 1.2945693731307983\n",
      "Epoch 5, Batch 120, Loss: 1.1734230518341064\n",
      "Epoch 5, Batch 130, Loss: 1.3023877143859863\n",
      "Epoch 5, Batch 140, Loss: 1.050546407699585\n",
      "Epoch 5, Batch 150, Loss: 1.4088199138641357\n",
      "Epoch 5, Batch 160, Loss: 1.2704596519470215\n",
      "Epoch 5, Batch 170, Loss: 1.2380313873291016\n",
      "Epoch 5, Batch 180, Loss: 1.3366175889968872\n",
      "Epoch 5, Batch 190, Loss: 1.1627100706100464\n",
      "Epoch 5, Batch 200, Loss: 0.9923097491264343\n",
      "Epoch 5, Batch 210, Loss: 1.1387392282485962\n",
      "Epoch 5, Batch 220, Loss: 1.396131157875061\n",
      "Epoch 5, Batch 230, Loss: 1.022385597229004\n",
      "Epoch 5, Batch 240, Loss: 1.4200701713562012\n",
      "Epoch 5, Batch 250, Loss: 1.1162233352661133\n",
      "Epoch 5, Batch 260, Loss: 1.1688734292984009\n",
      "Epoch 5, Batch 270, Loss: 1.265579342842102\n",
      "Epoch 5, Batch 280, Loss: 1.372748851776123\n",
      "Epoch 5, Batch 290, Loss: 1.27726411819458\n",
      "Epoch 5, Batch 300, Loss: 1.2180919647216797\n",
      "Epoch 5, Batch 310, Loss: 1.2690048217773438\n",
      "Epoch 5, Batch 320, Loss: 0.9824863076210022\n",
      "Epoch 5, Batch 330, Loss: 1.082364559173584\n",
      "Epoch 5, Batch 340, Loss: 1.1422661542892456\n",
      "Epoch 5, Batch 350, Loss: 1.1123331785202026\n",
      "Epoch 5, Batch 360, Loss: 1.3568609952926636\n",
      "Epoch 5, Batch 370, Loss: 1.1104062795639038\n",
      "Epoch 5, Batch 380, Loss: 1.1531167030334473\n",
      "Epoch 5, Batch 390, Loss: 1.1917058229446411\n",
      "Epoch 5, Batch 400, Loss: 1.1209213733673096\n",
      "Epoch 5, Batch 410, Loss: 1.2899893522262573\n",
      "Epoch 5, Batch 420, Loss: 1.184224009513855\n",
      "Epoch 5, Batch 430, Loss: 1.0500776767730713\n",
      "Epoch 5, Batch 440, Loss: 1.1664443016052246\n",
      "Epoch 5, Batch 450, Loss: 1.4492284059524536\n",
      "Epoch 5, Batch 460, Loss: 1.1570299863815308\n",
      "Epoch 5, Batch 470, Loss: 1.008226752281189\n",
      "Epoch 5, Batch 480, Loss: 1.2265336513519287\n",
      "Epoch 5, Batch 490, Loss: 1.534820318222046\n",
      "Epoch 5, Batch 500, Loss: 1.1398212909698486\n",
      "Epoch 5, Batch 510, Loss: 1.3015166521072388\n",
      "Epoch 5, Batch 520, Loss: 1.3749051094055176\n",
      "Epoch 5, Batch 530, Loss: 1.4807242155075073\n",
      "Epoch 5, Batch 540, Loss: 1.2286591529846191\n",
      "Epoch 5, Batch 550, Loss: 1.340571641921997\n",
      "Epoch 5, Batch 560, Loss: 1.3390822410583496\n",
      "Epoch 5, Batch 570, Loss: 1.141347885131836\n",
      "Epoch 5, Batch 580, Loss: 1.0126339197158813\n",
      "Epoch 5, Batch 590, Loss: 1.1110970973968506\n",
      "Epoch 5, Batch 600, Loss: 1.4569640159606934\n",
      "Epoch 5, Batch 610, Loss: 1.0814006328582764\n",
      "Epoch 5, Batch 620, Loss: 1.1276739835739136\n",
      "Epoch 5, Batch 630, Loss: 1.3186815977096558\n",
      "Epoch 5, Batch 640, Loss: 1.2483224868774414\n",
      "Epoch 5, Batch 650, Loss: 1.2313122749328613\n",
      "Epoch 5, Batch 660, Loss: 1.1714993715286255\n",
      "Epoch 5, Batch 670, Loss: 1.134474754333496\n",
      "Epoch 5, Batch 680, Loss: 1.250755786895752\n",
      "Epoch 5, Batch 690, Loss: 1.2746058702468872\n",
      "Epoch 5, Batch 700, Loss: 1.2482081651687622\n",
      "Epoch 5, Batch 710, Loss: 1.2111918926239014\n",
      "Epoch 5, Batch 720, Loss: 1.2314014434814453\n",
      "Epoch 5, Batch 730, Loss: 1.1621670722961426\n",
      "Epoch 5, Batch 740, Loss: 1.1985816955566406\n",
      "Epoch 5, Batch 750, Loss: 1.1116247177124023\n",
      "Epoch 5, Batch 760, Loss: 1.2518023252487183\n",
      "Epoch 5, Batch 770, Loss: 1.1655064821243286\n",
      "Epoch 5, Batch 780, Loss: 1.2648351192474365\n",
      "Epoch 5, Batch 790, Loss: 1.1592066287994385\n",
      "Epoch 5, Batch 800, Loss: 1.1882481575012207\n",
      "Epoch 5, Batch 810, Loss: 1.1952183246612549\n",
      "Epoch 5, Batch 820, Loss: 1.5233330726623535\n",
      "Epoch 5, Batch 830, Loss: 1.157559871673584\n",
      "Epoch 5, Batch 840, Loss: 1.302382230758667\n",
      "Epoch 5, Batch 850, Loss: 0.9363725781440735\n",
      "Epoch 5, Batch 860, Loss: 1.1016559600830078\n",
      "Epoch 5, Batch 870, Loss: 1.1305128335952759\n",
      "Epoch 5, Batch 880, Loss: 1.157450795173645\n",
      "Epoch 5, Batch 890, Loss: 1.0212175846099854\n",
      "Epoch 5, Batch 900, Loss: 1.2586965560913086\n",
      "Epoch 5, Batch 910, Loss: 1.4173792600631714\n",
      "Epoch 5, Batch 920, Loss: 1.382727026939392\n",
      "Epoch 5, Batch 930, Loss: 0.923904538154602\n",
      "Epoch 5, Batch 940, Loss: 1.2200721502304077\n",
      "Epoch 5, Batch 950, Loss: 1.291370153427124\n",
      "Epoch 5, Batch 960, Loss: 1.1324646472930908\n",
      "Epoch 5, Batch 970, Loss: 1.0951675176620483\n",
      "Epoch 5, Batch 980, Loss: 0.9719988703727722\n",
      "Epoch 5, Batch 990, Loss: 1.162132978439331\n",
      "Epoch 5, Batch 1000, Loss: 1.1909217834472656\n",
      "Epoch 5, Batch 1010, Loss: 1.270598292350769\n",
      "Epoch 5, Batch 1020, Loss: 1.2364563941955566\n",
      "Epoch 5, Batch 1030, Loss: 1.4171111583709717\n",
      "Epoch 5, Batch 1040, Loss: 1.1529093980789185\n",
      "Epoch 5, Batch 1050, Loss: 1.2899551391601562\n",
      "Epoch 5, Batch 1060, Loss: 1.0895447731018066\n",
      "Epoch 5, Batch 1070, Loss: 1.2606301307678223\n",
      "Epoch 5, Batch 1080, Loss: 0.9928711652755737\n",
      "Epoch 5, Batch 1090, Loss: 1.217353343963623\n",
      "Epoch 5, Batch 1100, Loss: 1.0644304752349854\n",
      "Epoch 5, Batch 1110, Loss: 1.3739140033721924\n",
      "Epoch 5, Batch 1120, Loss: 1.070144772529602\n",
      "Epoch 5, Batch 1130, Loss: 1.2073651552200317\n",
      "Epoch 5, Batch 1140, Loss: 1.0660878419876099\n",
      "Epoch 5, Batch 1150, Loss: 1.4148329496383667\n",
      "Epoch 5, Batch 1160, Loss: 1.294523000717163\n",
      "Epoch 5, Batch 1170, Loss: 0.9453889727592468\n",
      "Epoch 5, Batch 1180, Loss: 1.093671441078186\n",
      "Epoch 5, Batch 1190, Loss: 1.3695180416107178\n",
      "Epoch 5, Batch 1200, Loss: 0.9746624827384949\n",
      "Epoch 5, Batch 1210, Loss: 1.22515869140625\n",
      "Epoch 5, Batch 1220, Loss: 1.18099045753479\n",
      "Epoch 5, Batch 1230, Loss: 1.1731092929840088\n",
      "Epoch 5, Batch 1240, Loss: 1.03685462474823\n",
      "Epoch 5, Batch 1250, Loss: 1.1805520057678223\n",
      "Epoch 5, Batch 1260, Loss: 1.3417505025863647\n",
      "Epoch 5, Batch 1270, Loss: 1.409356713294983\n",
      "Epoch 5, Batch 1280, Loss: 1.2089080810546875\n",
      "Epoch 5, Batch 1290, Loss: 1.1669625043869019\n",
      "Epoch 5, Batch 1300, Loss: 1.3244686126708984\n",
      "Epoch 5, Batch 1310, Loss: 1.1802595853805542\n",
      "Epoch 5, Batch 1320, Loss: 1.336350679397583\n",
      "Epoch 5, Batch 1330, Loss: 1.1681017875671387\n",
      "Epoch 5, Batch 1340, Loss: 1.3767895698547363\n",
      "Epoch 5, Batch 1350, Loss: 1.1420947313308716\n",
      "Epoch 5, Batch 1360, Loss: 1.1905347108840942\n",
      "Epoch 5, Batch 1370, Loss: 1.3503538370132446\n",
      "Epoch 5, Batch 1380, Loss: 1.455964207649231\n",
      "Epoch 5, Batch 1390, Loss: 1.3126908540725708\n",
      "Epoch 5, Batch 1400, Loss: 1.1639915704727173\n",
      "Epoch 5, Train Loss: 1.207269644093327, Validation Accuracy: 55.82766893242703%\n",
      "Epoch 6, Batch 0, Loss: 1.0637624263763428\n",
      "Epoch 6, Batch 10, Loss: 1.132389783859253\n",
      "Epoch 6, Batch 20, Loss: 1.426641583442688\n",
      "Epoch 6, Batch 30, Loss: 1.1156805753707886\n",
      "Epoch 6, Batch 40, Loss: 1.2202914953231812\n",
      "Epoch 6, Batch 50, Loss: 1.1246005296707153\n",
      "Epoch 6, Batch 60, Loss: 0.9458296895027161\n",
      "Epoch 6, Batch 70, Loss: 1.10884690284729\n",
      "Epoch 6, Batch 80, Loss: 1.3827364444732666\n",
      "Epoch 6, Batch 90, Loss: 1.2099525928497314\n",
      "Epoch 6, Batch 100, Loss: 0.9963348507881165\n",
      "Epoch 6, Batch 110, Loss: 1.147468090057373\n",
      "Epoch 6, Batch 120, Loss: 1.194011926651001\n",
      "Epoch 6, Batch 130, Loss: 1.0118335485458374\n",
      "Epoch 6, Batch 140, Loss: 1.0133291482925415\n",
      "Epoch 6, Batch 150, Loss: 1.2176893949508667\n",
      "Epoch 6, Batch 160, Loss: 1.279952883720398\n",
      "Epoch 6, Batch 170, Loss: 1.1657345294952393\n",
      "Epoch 6, Batch 180, Loss: 1.486484169960022\n",
      "Epoch 6, Batch 190, Loss: 1.198515772819519\n",
      "Epoch 6, Batch 200, Loss: 1.1088992357254028\n",
      "Epoch 6, Batch 210, Loss: 1.152969241142273\n",
      "Epoch 6, Batch 220, Loss: 0.9877687096595764\n",
      "Epoch 6, Batch 230, Loss: 1.241273045539856\n",
      "Epoch 6, Batch 240, Loss: 1.4025907516479492\n",
      "Epoch 6, Batch 250, Loss: 1.228523850440979\n",
      "Epoch 6, Batch 260, Loss: 1.2645533084869385\n",
      "Epoch 6, Batch 270, Loss: 1.305866003036499\n",
      "Epoch 6, Batch 280, Loss: 1.0126314163208008\n",
      "Epoch 6, Batch 290, Loss: 1.2082630395889282\n",
      "Epoch 6, Batch 300, Loss: 1.1943835020065308\n",
      "Epoch 6, Batch 310, Loss: 1.40755295753479\n",
      "Epoch 6, Batch 320, Loss: 1.0370049476623535\n",
      "Epoch 6, Batch 330, Loss: 1.0625452995300293\n",
      "Epoch 6, Batch 340, Loss: 1.3224763870239258\n",
      "Epoch 6, Batch 350, Loss: 1.2931506633758545\n",
      "Epoch 6, Batch 360, Loss: 1.1799380779266357\n",
      "Epoch 6, Batch 370, Loss: 1.2284759283065796\n",
      "Epoch 6, Batch 380, Loss: 1.3815157413482666\n",
      "Epoch 6, Batch 390, Loss: 1.148908019065857\n",
      "Epoch 6, Batch 400, Loss: 1.081878900527954\n",
      "Epoch 6, Batch 410, Loss: 1.1133240461349487\n",
      "Epoch 6, Batch 420, Loss: 1.2689881324768066\n",
      "Epoch 6, Batch 430, Loss: 1.04868745803833\n",
      "Epoch 6, Batch 440, Loss: 1.3306798934936523\n",
      "Epoch 6, Batch 450, Loss: 1.2444994449615479\n",
      "Epoch 6, Batch 460, Loss: 1.3561652898788452\n",
      "Epoch 6, Batch 470, Loss: 1.4425804615020752\n",
      "Epoch 6, Batch 480, Loss: 1.1503052711486816\n",
      "Epoch 6, Batch 490, Loss: 1.216403841972351\n",
      "Epoch 6, Batch 500, Loss: 1.2037665843963623\n",
      "Epoch 6, Batch 510, Loss: 1.2118281126022339\n",
      "Epoch 6, Batch 520, Loss: 1.2278519868850708\n",
      "Epoch 6, Batch 530, Loss: 1.3606165647506714\n",
      "Epoch 6, Batch 540, Loss: 1.2346491813659668\n",
      "Epoch 6, Batch 550, Loss: 0.9619658589363098\n",
      "Epoch 6, Batch 560, Loss: 1.4389207363128662\n",
      "Epoch 6, Batch 570, Loss: 1.1604118347167969\n",
      "Epoch 6, Batch 580, Loss: 1.0940724611282349\n",
      "Epoch 6, Batch 590, Loss: 1.0796971321105957\n",
      "Epoch 6, Batch 600, Loss: 0.9567421674728394\n",
      "Epoch 6, Batch 610, Loss: 1.2515310049057007\n",
      "Epoch 6, Batch 620, Loss: 1.099008321762085\n",
      "Epoch 6, Batch 630, Loss: 1.0600179433822632\n",
      "Epoch 6, Batch 640, Loss: 1.188606858253479\n",
      "Epoch 6, Batch 650, Loss: 1.2636536359786987\n",
      "Epoch 6, Batch 660, Loss: 1.1229114532470703\n",
      "Epoch 6, Batch 670, Loss: 1.3216184377670288\n",
      "Epoch 6, Batch 680, Loss: 1.0536699295043945\n",
      "Epoch 6, Batch 690, Loss: 1.0145058631896973\n",
      "Epoch 6, Batch 700, Loss: 1.1901533603668213\n",
      "Epoch 6, Batch 710, Loss: 1.0137131214141846\n",
      "Epoch 6, Batch 720, Loss: 1.4503800868988037\n",
      "Epoch 6, Batch 730, Loss: 1.1254786252975464\n",
      "Epoch 6, Batch 740, Loss: 1.2563257217407227\n",
      "Epoch 6, Batch 750, Loss: 1.055992603302002\n",
      "Epoch 6, Batch 760, Loss: 1.295142412185669\n",
      "Epoch 6, Batch 770, Loss: 1.1139419078826904\n",
      "Epoch 6, Batch 780, Loss: 0.9825399518013\n",
      "Epoch 6, Batch 790, Loss: 1.04079270362854\n",
      "Epoch 6, Batch 800, Loss: 1.2058861255645752\n",
      "Epoch 6, Batch 810, Loss: 1.2209075689315796\n",
      "Epoch 6, Batch 820, Loss: 1.3490086793899536\n",
      "Epoch 6, Batch 830, Loss: 1.401339054107666\n",
      "Epoch 6, Batch 840, Loss: 1.2123655080795288\n",
      "Epoch 6, Batch 850, Loss: 1.2186987400054932\n",
      "Epoch 6, Batch 860, Loss: 1.380861520767212\n",
      "Epoch 6, Batch 870, Loss: 1.0785448551177979\n",
      "Epoch 6, Batch 880, Loss: 1.2085723876953125\n",
      "Epoch 6, Batch 890, Loss: 1.3632820844650269\n",
      "Epoch 6, Batch 900, Loss: 1.1594865322113037\n",
      "Epoch 6, Batch 910, Loss: 1.18660569190979\n",
      "Epoch 6, Batch 920, Loss: 0.9176160097122192\n",
      "Epoch 6, Batch 930, Loss: 1.0727934837341309\n",
      "Epoch 6, Batch 940, Loss: 1.3683258295059204\n",
      "Epoch 6, Batch 950, Loss: 1.188462734222412\n",
      "Epoch 6, Batch 960, Loss: 1.4426437616348267\n",
      "Epoch 6, Batch 970, Loss: 1.3681493997573853\n",
      "Epoch 6, Batch 980, Loss: 1.1024543046951294\n",
      "Epoch 6, Batch 990, Loss: 1.214028000831604\n",
      "Epoch 6, Batch 1000, Loss: 1.159881353378296\n",
      "Epoch 6, Batch 1010, Loss: 1.2447636127471924\n",
      "Epoch 6, Batch 1020, Loss: 1.5353589057922363\n",
      "Epoch 6, Batch 1030, Loss: 1.2176587581634521\n",
      "Epoch 6, Batch 1040, Loss: 1.046125054359436\n",
      "Epoch 6, Batch 1050, Loss: 1.1533198356628418\n",
      "Epoch 6, Batch 1060, Loss: 1.440889596939087\n",
      "Epoch 6, Batch 1070, Loss: 1.2540885210037231\n",
      "Epoch 6, Batch 1080, Loss: 1.478577971458435\n",
      "Epoch 6, Batch 1090, Loss: 1.1443742513656616\n",
      "Epoch 6, Batch 1100, Loss: 1.0264054536819458\n",
      "Epoch 6, Batch 1110, Loss: 1.1676586866378784\n",
      "Epoch 6, Batch 1120, Loss: 1.3106032609939575\n",
      "Epoch 6, Batch 1130, Loss: 1.3370047807693481\n",
      "Epoch 6, Batch 1140, Loss: 1.1155651807785034\n",
      "Epoch 6, Batch 1150, Loss: 1.0550308227539062\n",
      "Epoch 6, Batch 1160, Loss: 1.1376513242721558\n",
      "Epoch 6, Batch 1170, Loss: 1.2060151100158691\n",
      "Epoch 6, Batch 1180, Loss: 1.374991536140442\n",
      "Epoch 6, Batch 1190, Loss: 1.0321084260940552\n",
      "Epoch 6, Batch 1200, Loss: 1.323749303817749\n",
      "Epoch 6, Batch 1210, Loss: 1.210083246231079\n",
      "Epoch 6, Batch 1220, Loss: 1.2611424922943115\n",
      "Epoch 6, Batch 1230, Loss: 1.3177860975265503\n",
      "Epoch 6, Batch 1240, Loss: 1.36570143699646\n",
      "Epoch 6, Batch 1250, Loss: 1.1464861631393433\n",
      "Epoch 6, Batch 1260, Loss: 1.247278094291687\n",
      "Epoch 6, Batch 1270, Loss: 1.0927282571792603\n",
      "Epoch 6, Batch 1280, Loss: 1.1816092729568481\n",
      "Epoch 6, Batch 1290, Loss: 1.4384440183639526\n",
      "Epoch 6, Batch 1300, Loss: 1.090404748916626\n",
      "Epoch 6, Batch 1310, Loss: 1.1219395399093628\n",
      "Epoch 6, Batch 1320, Loss: 1.156299352645874\n",
      "Epoch 6, Batch 1330, Loss: 1.1893936395645142\n",
      "Epoch 6, Batch 1340, Loss: 1.17911958694458\n",
      "Epoch 6, Batch 1350, Loss: 0.9214333295822144\n",
      "Epoch 6, Batch 1360, Loss: 1.1837116479873657\n",
      "Epoch 6, Batch 1370, Loss: 1.2434537410736084\n",
      "Epoch 6, Batch 1380, Loss: 1.023699164390564\n",
      "Epoch 6, Batch 1390, Loss: 1.044065237045288\n",
      "Epoch 6, Batch 1400, Loss: 1.162958025932312\n",
      "Epoch 6, Train Loss: 1.200901204080724, Validation Accuracy: 56.27748900439824%\n",
      "Epoch 7, Batch 0, Loss: 1.1903637647628784\n",
      "Epoch 7, Batch 10, Loss: 1.1344194412231445\n",
      "Epoch 7, Batch 20, Loss: 1.3771843910217285\n",
      "Epoch 7, Batch 30, Loss: 1.0636223554611206\n",
      "Epoch 7, Batch 40, Loss: 1.002703309059143\n",
      "Epoch 7, Batch 50, Loss: 1.2659982442855835\n",
      "Epoch 7, Batch 60, Loss: 1.3320422172546387\n",
      "Epoch 7, Batch 70, Loss: 1.08444344997406\n",
      "Epoch 7, Batch 80, Loss: 1.122753381729126\n",
      "Epoch 7, Batch 90, Loss: 1.3884739875793457\n",
      "Epoch 7, Batch 100, Loss: 1.2695538997650146\n",
      "Epoch 7, Batch 110, Loss: 1.2081880569458008\n",
      "Epoch 7, Batch 120, Loss: 1.1100319623947144\n",
      "Epoch 7, Batch 130, Loss: 1.2543644905090332\n",
      "Epoch 7, Batch 140, Loss: 1.1148290634155273\n",
      "Epoch 7, Batch 150, Loss: 1.0168222188949585\n",
      "Epoch 7, Batch 160, Loss: 1.2592514753341675\n",
      "Epoch 7, Batch 170, Loss: 1.1187547445297241\n",
      "Epoch 7, Batch 180, Loss: 1.331485390663147\n",
      "Epoch 7, Batch 190, Loss: 1.2224986553192139\n",
      "Epoch 7, Batch 200, Loss: 0.9217826128005981\n",
      "Epoch 7, Batch 210, Loss: 1.4293674230575562\n",
      "Epoch 7, Batch 220, Loss: 1.32127845287323\n",
      "Epoch 7, Batch 230, Loss: 1.0569984912872314\n",
      "Epoch 7, Batch 240, Loss: 1.3575680255889893\n",
      "Epoch 7, Batch 250, Loss: 1.3500850200653076\n",
      "Epoch 7, Batch 260, Loss: 1.3122224807739258\n",
      "Epoch 7, Batch 270, Loss: 1.3181066513061523\n",
      "Epoch 7, Batch 280, Loss: 1.3014034032821655\n",
      "Epoch 7, Batch 290, Loss: 1.4174284934997559\n",
      "Epoch 7, Batch 300, Loss: 1.0768226385116577\n",
      "Epoch 7, Batch 310, Loss: 1.2837822437286377\n",
      "Epoch 7, Batch 320, Loss: 1.0928117036819458\n",
      "Epoch 7, Batch 330, Loss: 1.2881038188934326\n",
      "Epoch 7, Batch 340, Loss: 1.243828535079956\n",
      "Epoch 7, Batch 350, Loss: 1.1960445642471313\n",
      "Epoch 7, Batch 360, Loss: 1.3141340017318726\n",
      "Epoch 7, Batch 370, Loss: 1.4941327571868896\n",
      "Epoch 7, Batch 380, Loss: 1.3783619403839111\n",
      "Epoch 7, Batch 390, Loss: 1.2858668565750122\n",
      "Epoch 7, Batch 400, Loss: 1.0611432790756226\n",
      "Epoch 7, Batch 410, Loss: 1.1896592378616333\n",
      "Epoch 7, Batch 420, Loss: 1.2955262660980225\n",
      "Epoch 7, Batch 430, Loss: 1.3725560903549194\n",
      "Epoch 7, Batch 440, Loss: 1.2702457904815674\n",
      "Epoch 7, Batch 450, Loss: 1.2312126159667969\n",
      "Epoch 7, Batch 460, Loss: 1.3261651992797852\n",
      "Epoch 7, Batch 470, Loss: 1.1431328058242798\n",
      "Epoch 7, Batch 480, Loss: 1.0487453937530518\n",
      "Epoch 7, Batch 490, Loss: 1.0734323263168335\n",
      "Epoch 7, Batch 500, Loss: 1.1818792819976807\n",
      "Epoch 7, Batch 510, Loss: 1.2352606058120728\n",
      "Epoch 7, Batch 520, Loss: 1.4098742008209229\n",
      "Epoch 7, Batch 530, Loss: 1.210409164428711\n",
      "Epoch 7, Batch 540, Loss: 1.4039627313613892\n",
      "Epoch 7, Batch 550, Loss: 1.0915292501449585\n",
      "Epoch 7, Batch 560, Loss: 1.1642259359359741\n",
      "Epoch 7, Batch 570, Loss: 1.4382023811340332\n",
      "Epoch 7, Batch 580, Loss: 1.056844711303711\n",
      "Epoch 7, Batch 590, Loss: 1.3122942447662354\n",
      "Epoch 7, Batch 600, Loss: 1.1196938753128052\n",
      "Epoch 7, Batch 610, Loss: 1.4105193614959717\n",
      "Epoch 7, Batch 620, Loss: 0.9997879862785339\n",
      "Epoch 7, Batch 630, Loss: 1.1467808485031128\n",
      "Epoch 7, Batch 640, Loss: 1.3806132078170776\n",
      "Epoch 7, Batch 650, Loss: 1.1153826713562012\n",
      "Epoch 7, Batch 660, Loss: 1.083565354347229\n",
      "Epoch 7, Batch 670, Loss: 1.5745288133621216\n",
      "Epoch 7, Batch 680, Loss: 1.103533148765564\n",
      "Epoch 7, Batch 690, Loss: 1.2933716773986816\n",
      "Epoch 7, Batch 700, Loss: 1.2384092807769775\n",
      "Epoch 7, Batch 710, Loss: 1.0673707723617554\n",
      "Epoch 7, Batch 720, Loss: 1.320142388343811\n",
      "Epoch 7, Batch 730, Loss: 1.031923770904541\n",
      "Epoch 7, Batch 740, Loss: 1.0996202230453491\n",
      "Epoch 7, Batch 750, Loss: 1.503475308418274\n",
      "Epoch 7, Batch 760, Loss: 1.055046796798706\n",
      "Epoch 7, Batch 770, Loss: 1.0245946645736694\n",
      "Epoch 7, Batch 780, Loss: 1.2732707262039185\n",
      "Epoch 7, Batch 790, Loss: 1.097276210784912\n",
      "Epoch 7, Batch 800, Loss: 1.2876709699630737\n",
      "Epoch 7, Batch 810, Loss: 1.0092175006866455\n",
      "Epoch 7, Batch 820, Loss: 1.1241778135299683\n",
      "Epoch 7, Batch 830, Loss: 1.4539543390274048\n",
      "Epoch 7, Batch 840, Loss: 1.1834986209869385\n",
      "Epoch 7, Batch 850, Loss: 1.2623344659805298\n",
      "Epoch 7, Batch 860, Loss: 1.188058853149414\n",
      "Epoch 7, Batch 870, Loss: 1.1378200054168701\n",
      "Epoch 7, Batch 880, Loss: 1.15994131565094\n",
      "Epoch 7, Batch 890, Loss: 1.310584545135498\n",
      "Epoch 7, Batch 900, Loss: 1.200822353363037\n",
      "Epoch 7, Batch 910, Loss: 1.3277784585952759\n",
      "Epoch 7, Batch 920, Loss: 1.1504390239715576\n",
      "Epoch 7, Batch 930, Loss: 1.3713383674621582\n",
      "Epoch 7, Batch 940, Loss: 1.3689336776733398\n",
      "Epoch 7, Batch 950, Loss: 1.2150599956512451\n",
      "Epoch 7, Batch 960, Loss: 0.971514880657196\n",
      "Epoch 7, Batch 970, Loss: 1.1060463190078735\n",
      "Epoch 7, Batch 980, Loss: 1.2184923887252808\n",
      "Epoch 7, Batch 990, Loss: 1.1809426546096802\n",
      "Epoch 7, Batch 1000, Loss: 1.057857632637024\n",
      "Epoch 7, Batch 1010, Loss: 1.480074405670166\n",
      "Epoch 7, Batch 1020, Loss: 1.0519773960113525\n",
      "Epoch 7, Batch 1030, Loss: 1.1703071594238281\n",
      "Epoch 7, Batch 1040, Loss: 1.3540557622909546\n",
      "Epoch 7, Batch 1050, Loss: 1.1788630485534668\n",
      "Epoch 7, Batch 1060, Loss: 0.9786719083786011\n",
      "Epoch 7, Batch 1070, Loss: 1.0992368459701538\n",
      "Epoch 7, Batch 1080, Loss: 1.053653597831726\n",
      "Epoch 7, Batch 1090, Loss: 1.2751848697662354\n",
      "Epoch 7, Batch 1100, Loss: 0.9554183483123779\n",
      "Epoch 7, Batch 1110, Loss: 1.0151264667510986\n",
      "Epoch 7, Batch 1120, Loss: 1.2970298528671265\n",
      "Epoch 7, Batch 1130, Loss: 1.192819356918335\n",
      "Epoch 7, Batch 1140, Loss: 1.3029950857162476\n",
      "Epoch 7, Batch 1150, Loss: 1.1882437467575073\n",
      "Epoch 7, Batch 1160, Loss: 1.392091989517212\n",
      "Epoch 7, Batch 1170, Loss: 1.0415303707122803\n",
      "Epoch 7, Batch 1180, Loss: 1.1445934772491455\n",
      "Epoch 7, Batch 1190, Loss: 1.1682299375534058\n",
      "Epoch 7, Batch 1200, Loss: 1.3867908716201782\n",
      "Epoch 7, Batch 1210, Loss: 1.284760594367981\n",
      "Epoch 7, Batch 1220, Loss: 1.1019694805145264\n",
      "Epoch 7, Batch 1230, Loss: 0.9935929775238037\n",
      "Epoch 7, Batch 1240, Loss: 1.0862810611724854\n",
      "Epoch 7, Batch 1250, Loss: 1.227415680885315\n",
      "Epoch 7, Batch 1260, Loss: 1.1171294450759888\n",
      "Epoch 7, Batch 1270, Loss: 0.9357473850250244\n",
      "Epoch 7, Batch 1280, Loss: 1.319305181503296\n",
      "Epoch 7, Batch 1290, Loss: 1.0637857913970947\n",
      "Epoch 7, Batch 1300, Loss: 1.1703143119812012\n",
      "Epoch 7, Batch 1310, Loss: 1.2077956199645996\n",
      "Epoch 7, Batch 1320, Loss: 1.32919442653656\n",
      "Epoch 7, Batch 1330, Loss: 0.8853410482406616\n",
      "Epoch 7, Batch 1340, Loss: 1.6452499628067017\n",
      "Epoch 7, Batch 1350, Loss: 1.2792165279388428\n",
      "Epoch 7, Batch 1360, Loss: 1.1018743515014648\n",
      "Epoch 7, Batch 1370, Loss: 1.0327070951461792\n",
      "Epoch 7, Batch 1380, Loss: 1.2760848999023438\n",
      "Epoch 7, Batch 1390, Loss: 1.3157685995101929\n",
      "Epoch 7, Batch 1400, Loss: 1.1843433380126953\n",
      "Epoch 7, Train Loss: 1.1964566306434121, Validation Accuracy: 56.93722510995602%\n",
      "Epoch 8, Batch 0, Loss: 1.140830159187317\n",
      "Epoch 8, Batch 10, Loss: 1.1944900751113892\n",
      "Epoch 8, Batch 20, Loss: 1.272304892539978\n",
      "Epoch 8, Batch 30, Loss: 1.0099005699157715\n",
      "Epoch 8, Batch 40, Loss: 1.199771523475647\n",
      "Epoch 8, Batch 50, Loss: 1.1302180290222168\n",
      "Epoch 8, Batch 60, Loss: 1.1334625482559204\n",
      "Epoch 8, Batch 70, Loss: 1.2714194059371948\n",
      "Epoch 8, Batch 80, Loss: 1.2437869310379028\n",
      "Epoch 8, Batch 90, Loss: 1.2958842515945435\n",
      "Epoch 8, Batch 100, Loss: 1.247243046760559\n",
      "Epoch 8, Batch 110, Loss: 1.5291883945465088\n",
      "Epoch 8, Batch 120, Loss: 1.1897425651550293\n",
      "Epoch 8, Batch 130, Loss: 1.2601659297943115\n",
      "Epoch 8, Batch 140, Loss: 1.0665113925933838\n",
      "Epoch 8, Batch 150, Loss: 1.0884599685668945\n",
      "Epoch 8, Batch 160, Loss: 1.011573076248169\n",
      "Epoch 8, Batch 170, Loss: 1.1758025884628296\n",
      "Epoch 8, Batch 180, Loss: 1.2536001205444336\n",
      "Epoch 8, Batch 190, Loss: 1.3086066246032715\n",
      "Epoch 8, Batch 200, Loss: 1.5156049728393555\n",
      "Epoch 8, Batch 210, Loss: 1.2449004650115967\n",
      "Epoch 8, Batch 220, Loss: 1.2408549785614014\n",
      "Epoch 8, Batch 230, Loss: 1.2627791166305542\n",
      "Epoch 8, Batch 240, Loss: 1.2532449960708618\n",
      "Epoch 8, Batch 250, Loss: 1.0880024433135986\n",
      "Epoch 8, Batch 260, Loss: 1.232011318206787\n",
      "Epoch 8, Batch 270, Loss: 1.278700828552246\n",
      "Epoch 8, Batch 280, Loss: 1.1194803714752197\n",
      "Epoch 8, Batch 290, Loss: 1.3445597887039185\n",
      "Epoch 8, Batch 300, Loss: 1.174017071723938\n",
      "Epoch 8, Batch 310, Loss: 1.3434385061264038\n",
      "Epoch 8, Batch 320, Loss: 1.2404907941818237\n",
      "Epoch 8, Batch 330, Loss: 1.2256040573120117\n",
      "Epoch 8, Batch 340, Loss: 1.310692310333252\n",
      "Epoch 8, Batch 350, Loss: 1.1776440143585205\n",
      "Epoch 8, Batch 360, Loss: 0.9935694336891174\n",
      "Epoch 8, Batch 370, Loss: 1.1763242483139038\n",
      "Epoch 8, Batch 380, Loss: 1.1715306043624878\n",
      "Epoch 8, Batch 390, Loss: 1.2263078689575195\n",
      "Epoch 8, Batch 400, Loss: 1.050504446029663\n",
      "Epoch 8, Batch 410, Loss: 1.1378463506698608\n",
      "Epoch 8, Batch 420, Loss: 1.1406058073043823\n",
      "Epoch 8, Batch 430, Loss: 1.322253704071045\n",
      "Epoch 8, Batch 440, Loss: 1.2907750606536865\n",
      "Epoch 8, Batch 450, Loss: 1.141984462738037\n",
      "Epoch 8, Batch 460, Loss: 1.1542468070983887\n",
      "Epoch 8, Batch 470, Loss: 1.361424446105957\n",
      "Epoch 8, Batch 480, Loss: 1.0565311908721924\n",
      "Epoch 8, Batch 490, Loss: 1.386974811553955\n",
      "Epoch 8, Batch 500, Loss: 1.052456021308899\n",
      "Epoch 8, Batch 510, Loss: 1.459431529045105\n",
      "Epoch 8, Batch 520, Loss: 1.0499979257583618\n",
      "Epoch 8, Batch 530, Loss: 1.1466972827911377\n",
      "Epoch 8, Batch 540, Loss: 1.0747524499893188\n",
      "Epoch 8, Batch 550, Loss: 1.6053708791732788\n",
      "Epoch 8, Batch 560, Loss: 1.1630923748016357\n",
      "Epoch 8, Batch 570, Loss: 1.2804368734359741\n",
      "Epoch 8, Batch 580, Loss: 1.3151135444641113\n",
      "Epoch 8, Batch 590, Loss: 1.4050928354263306\n",
      "Epoch 8, Batch 600, Loss: 1.0947531461715698\n",
      "Epoch 8, Batch 610, Loss: 1.23835027217865\n",
      "Epoch 8, Batch 620, Loss: 0.9114372730255127\n",
      "Epoch 8, Batch 630, Loss: 1.1480772495269775\n",
      "Epoch 8, Batch 640, Loss: 1.2082984447479248\n",
      "Epoch 8, Batch 650, Loss: 1.3938329219818115\n",
      "Epoch 8, Batch 660, Loss: 1.5120049715042114\n",
      "Epoch 8, Batch 670, Loss: 1.1956539154052734\n",
      "Epoch 8, Batch 680, Loss: 1.2937297821044922\n",
      "Epoch 8, Batch 690, Loss: 1.2036569118499756\n",
      "Epoch 8, Batch 700, Loss: 1.1247204542160034\n",
      "Epoch 8, Batch 710, Loss: 1.339347004890442\n",
      "Epoch 8, Batch 720, Loss: 1.0396872758865356\n",
      "Epoch 8, Batch 730, Loss: 1.0250979661941528\n",
      "Epoch 8, Batch 740, Loss: 1.041140079498291\n",
      "Epoch 8, Batch 750, Loss: 1.2295467853546143\n",
      "Epoch 8, Batch 760, Loss: 1.134735107421875\n",
      "Epoch 8, Batch 770, Loss: 1.1418217420578003\n",
      "Epoch 8, Batch 780, Loss: 1.272149682044983\n",
      "Epoch 8, Batch 790, Loss: 1.2010923624038696\n",
      "Epoch 8, Batch 800, Loss: 1.2866274118423462\n",
      "Epoch 8, Batch 810, Loss: 1.07527494430542\n",
      "Epoch 8, Batch 820, Loss: 1.0784499645233154\n",
      "Epoch 8, Batch 830, Loss: 1.2599036693572998\n",
      "Epoch 8, Batch 840, Loss: 1.225144386291504\n",
      "Epoch 8, Batch 850, Loss: 1.2912888526916504\n",
      "Epoch 8, Batch 860, Loss: 1.234302043914795\n",
      "Epoch 8, Batch 870, Loss: 1.179328203201294\n",
      "Epoch 8, Batch 880, Loss: 1.338930368423462\n",
      "Epoch 8, Batch 890, Loss: 1.1971440315246582\n",
      "Epoch 8, Batch 900, Loss: 1.1331143379211426\n",
      "Epoch 8, Batch 910, Loss: 1.4069676399230957\n",
      "Epoch 8, Batch 920, Loss: 1.308592677116394\n",
      "Epoch 8, Batch 930, Loss: 1.2944072484970093\n",
      "Epoch 8, Batch 940, Loss: 1.215040922164917\n",
      "Epoch 8, Batch 950, Loss: 1.1896002292633057\n",
      "Epoch 8, Batch 960, Loss: 1.2659422159194946\n",
      "Epoch 8, Batch 970, Loss: 0.8909041285514832\n",
      "Epoch 8, Batch 980, Loss: 1.359086275100708\n",
      "Epoch 8, Batch 990, Loss: 1.160953164100647\n",
      "Epoch 8, Batch 1000, Loss: 1.2804497480392456\n",
      "Epoch 8, Batch 1010, Loss: 1.1634151935577393\n",
      "Epoch 8, Batch 1020, Loss: 1.3159257173538208\n",
      "Epoch 8, Batch 1030, Loss: 1.349030613899231\n",
      "Epoch 8, Batch 1040, Loss: 1.0984598398208618\n",
      "Epoch 8, Batch 1050, Loss: 1.1650787591934204\n",
      "Epoch 8, Batch 1060, Loss: 1.4264858961105347\n",
      "Epoch 8, Batch 1070, Loss: 1.2904703617095947\n",
      "Epoch 8, Batch 1080, Loss: 1.4650331735610962\n",
      "Epoch 8, Batch 1090, Loss: 1.2037816047668457\n",
      "Epoch 8, Batch 1100, Loss: 1.2445076704025269\n",
      "Epoch 8, Batch 1110, Loss: 1.200613021850586\n",
      "Epoch 8, Batch 1120, Loss: 1.2886149883270264\n",
      "Epoch 8, Batch 1130, Loss: 1.3737742900848389\n",
      "Epoch 8, Batch 1140, Loss: 1.174208641052246\n",
      "Epoch 8, Batch 1150, Loss: 1.1882823705673218\n",
      "Epoch 8, Batch 1160, Loss: 1.309417724609375\n",
      "Epoch 8, Batch 1170, Loss: 1.2963597774505615\n",
      "Epoch 8, Batch 1180, Loss: 1.2310680150985718\n",
      "Epoch 8, Batch 1190, Loss: 1.097555160522461\n",
      "Epoch 8, Batch 1200, Loss: 1.2745773792266846\n",
      "Epoch 8, Batch 1210, Loss: 1.1523483991622925\n",
      "Epoch 8, Batch 1220, Loss: 1.1672956943511963\n",
      "Epoch 8, Batch 1230, Loss: 1.0627750158309937\n",
      "Epoch 8, Batch 1240, Loss: 1.1687217950820923\n",
      "Epoch 8, Batch 1250, Loss: 1.1329026222229004\n",
      "Epoch 8, Batch 1260, Loss: 1.066074252128601\n",
      "Epoch 8, Batch 1270, Loss: 1.2500509023666382\n",
      "Epoch 8, Batch 1280, Loss: 1.3391705751419067\n",
      "Epoch 8, Batch 1290, Loss: 1.2592253684997559\n",
      "Epoch 8, Batch 1300, Loss: 1.2471882104873657\n",
      "Epoch 8, Batch 1310, Loss: 1.2317034006118774\n",
      "Epoch 8, Batch 1320, Loss: 1.3353853225708008\n",
      "Epoch 8, Batch 1330, Loss: 1.1253937482833862\n",
      "Epoch 8, Batch 1340, Loss: 1.2035166025161743\n",
      "Epoch 8, Batch 1350, Loss: 1.2779114246368408\n",
      "Epoch 8, Batch 1360, Loss: 1.3072409629821777\n",
      "Epoch 8, Batch 1370, Loss: 1.0734742879867554\n",
      "Epoch 8, Batch 1380, Loss: 1.2608259916305542\n",
      "Epoch 8, Batch 1390, Loss: 1.1139750480651855\n",
      "Epoch 8, Batch 1400, Loss: 1.1734395027160645\n",
      "Epoch 8, Train Loss: 1.1957421571800027, Validation Accuracy: 55.91763294682127%\n",
      "Epoch 9, Batch 0, Loss: 1.3879867792129517\n",
      "Epoch 9, Batch 10, Loss: 1.2008752822875977\n",
      "Epoch 9, Batch 20, Loss: 1.4992878437042236\n",
      "Epoch 9, Batch 30, Loss: 1.1365540027618408\n",
      "Epoch 9, Batch 40, Loss: 1.305225133895874\n",
      "Epoch 9, Batch 50, Loss: 1.3324049711227417\n",
      "Epoch 9, Batch 60, Loss: 1.139078974723816\n",
      "Epoch 9, Batch 70, Loss: 1.156420350074768\n",
      "Epoch 9, Batch 80, Loss: 1.184914469718933\n",
      "Epoch 9, Batch 90, Loss: 1.048816442489624\n",
      "Epoch 9, Batch 100, Loss: 1.0979784727096558\n",
      "Epoch 9, Batch 110, Loss: 1.1646069288253784\n",
      "Epoch 9, Batch 120, Loss: 1.1139357089996338\n",
      "Epoch 9, Batch 130, Loss: 1.0777231454849243\n",
      "Epoch 9, Batch 140, Loss: 0.9652865529060364\n",
      "Epoch 9, Batch 150, Loss: 1.2009111642837524\n",
      "Epoch 9, Batch 160, Loss: 1.1550309658050537\n",
      "Epoch 9, Batch 170, Loss: 1.1445434093475342\n",
      "Epoch 9, Batch 180, Loss: 1.2569060325622559\n",
      "Epoch 9, Batch 190, Loss: 1.2311375141143799\n",
      "Epoch 9, Batch 200, Loss: 1.1730036735534668\n",
      "Epoch 9, Batch 210, Loss: 1.2947174310684204\n",
      "Epoch 9, Batch 220, Loss: 1.3806554079055786\n",
      "Epoch 9, Batch 230, Loss: 1.2223557233810425\n",
      "Epoch 9, Batch 240, Loss: 1.0859038829803467\n",
      "Epoch 9, Batch 250, Loss: 1.09080970287323\n",
      "Epoch 9, Batch 260, Loss: 1.032829999923706\n",
      "Epoch 9, Batch 270, Loss: 1.0151158571243286\n",
      "Epoch 9, Batch 280, Loss: 1.1956194639205933\n",
      "Epoch 9, Batch 290, Loss: 1.015870213508606\n",
      "Epoch 9, Batch 300, Loss: 1.2123147249221802\n",
      "Epoch 9, Batch 310, Loss: 1.3298077583312988\n",
      "Epoch 9, Batch 320, Loss: 0.863324761390686\n",
      "Epoch 9, Batch 330, Loss: 1.2746843099594116\n",
      "Epoch 9, Batch 340, Loss: 0.995930552482605\n",
      "Epoch 9, Batch 350, Loss: 1.1768995523452759\n",
      "Epoch 9, Batch 360, Loss: 1.3822098970413208\n",
      "Epoch 9, Batch 370, Loss: 1.2469972372055054\n",
      "Epoch 9, Batch 380, Loss: 1.2640106678009033\n",
      "Epoch 9, Batch 390, Loss: 1.3774464130401611\n",
      "Epoch 9, Batch 400, Loss: 1.1623047590255737\n",
      "Epoch 9, Batch 410, Loss: 1.211090087890625\n",
      "Epoch 9, Batch 420, Loss: 1.32248055934906\n",
      "Epoch 9, Batch 430, Loss: 1.0131462812423706\n",
      "Epoch 9, Batch 440, Loss: 1.1831066608428955\n",
      "Epoch 9, Batch 450, Loss: 1.1411914825439453\n",
      "Epoch 9, Batch 460, Loss: 1.210180401802063\n",
      "Epoch 9, Batch 470, Loss: 1.4005118608474731\n",
      "Epoch 9, Batch 480, Loss: 1.1393234729766846\n",
      "Epoch 9, Batch 490, Loss: 1.239453911781311\n",
      "Epoch 9, Batch 500, Loss: 1.0944973230361938\n",
      "Epoch 9, Batch 510, Loss: 1.046651840209961\n",
      "Epoch 9, Batch 520, Loss: 1.0791672468185425\n",
      "Epoch 9, Batch 530, Loss: 1.1015304327011108\n",
      "Epoch 9, Batch 540, Loss: 1.2045451402664185\n",
      "Epoch 9, Batch 550, Loss: 1.0511610507965088\n",
      "Epoch 9, Batch 560, Loss: 1.3123136758804321\n",
      "Epoch 9, Batch 570, Loss: 0.975470244884491\n",
      "Epoch 9, Batch 580, Loss: 1.0605018138885498\n",
      "Epoch 9, Batch 590, Loss: 1.1463135480880737\n",
      "Epoch 9, Batch 600, Loss: 1.0461487770080566\n",
      "Epoch 9, Batch 610, Loss: 1.2107473611831665\n",
      "Epoch 9, Batch 620, Loss: 1.1157991886138916\n",
      "Epoch 9, Batch 630, Loss: 0.9662179946899414\n",
      "Epoch 9, Batch 640, Loss: 1.0424509048461914\n",
      "Epoch 9, Batch 650, Loss: 1.1781948804855347\n",
      "Epoch 9, Batch 660, Loss: 1.3350857496261597\n",
      "Epoch 9, Batch 670, Loss: 1.176040768623352\n",
      "Epoch 9, Batch 680, Loss: 0.9288744330406189\n",
      "Epoch 9, Batch 690, Loss: 1.1507947444915771\n",
      "Epoch 9, Batch 700, Loss: 1.193562626838684\n",
      "Epoch 9, Batch 710, Loss: 1.1828596591949463\n",
      "Epoch 9, Batch 720, Loss: 1.358056664466858\n",
      "Epoch 9, Batch 730, Loss: 1.0985758304595947\n",
      "Epoch 9, Batch 740, Loss: 1.0831934213638306\n",
      "Epoch 9, Batch 750, Loss: 1.3506780862808228\n",
      "Epoch 9, Batch 760, Loss: 1.1927847862243652\n",
      "Epoch 9, Batch 770, Loss: 1.1194870471954346\n",
      "Epoch 9, Batch 780, Loss: 1.3101530075073242\n",
      "Epoch 9, Batch 790, Loss: 1.0665987730026245\n",
      "Epoch 9, Batch 800, Loss: 1.1479068994522095\n",
      "Epoch 9, Batch 810, Loss: 0.8322595357894897\n",
      "Epoch 9, Batch 820, Loss: 1.1788280010223389\n",
      "Epoch 9, Batch 830, Loss: 1.3454264402389526\n",
      "Epoch 9, Batch 840, Loss: 1.0308035612106323\n",
      "Epoch 9, Batch 850, Loss: 1.3082633018493652\n",
      "Epoch 9, Batch 860, Loss: 1.2198177576065063\n",
      "Epoch 9, Batch 870, Loss: 1.1895993947982788\n",
      "Epoch 9, Batch 880, Loss: 1.2208179235458374\n",
      "Epoch 9, Batch 890, Loss: 1.2060493230819702\n",
      "Epoch 9, Batch 900, Loss: 1.3594427108764648\n",
      "Epoch 9, Batch 910, Loss: 1.1184297800064087\n",
      "Epoch 9, Batch 920, Loss: 1.1874245405197144\n",
      "Epoch 9, Batch 930, Loss: 0.9683951735496521\n",
      "Epoch 9, Batch 940, Loss: 1.1012037992477417\n",
      "Epoch 9, Batch 950, Loss: 1.1915040016174316\n",
      "Epoch 9, Batch 960, Loss: 1.1455087661743164\n",
      "Epoch 9, Batch 970, Loss: 1.166877269744873\n",
      "Epoch 9, Batch 980, Loss: 1.348029375076294\n",
      "Epoch 9, Batch 990, Loss: 1.1044071912765503\n",
      "Epoch 9, Batch 1000, Loss: 1.3646653890609741\n",
      "Epoch 9, Batch 1010, Loss: 1.312618613243103\n",
      "Epoch 9, Batch 1020, Loss: 1.256272554397583\n",
      "Epoch 9, Batch 1030, Loss: 1.0296396017074585\n",
      "Epoch 9, Batch 1040, Loss: 1.1617134809494019\n",
      "Epoch 9, Batch 1050, Loss: 1.1947327852249146\n",
      "Epoch 9, Batch 1060, Loss: 1.2316302061080933\n",
      "Epoch 9, Batch 1070, Loss: 1.313236951828003\n",
      "Epoch 9, Batch 1080, Loss: 0.9154557585716248\n",
      "Epoch 9, Batch 1090, Loss: 1.3401243686676025\n",
      "Epoch 9, Batch 1100, Loss: 1.4268053770065308\n",
      "Epoch 9, Batch 1110, Loss: 1.5595864057540894\n",
      "Epoch 9, Batch 1120, Loss: 1.2804219722747803\n",
      "Epoch 9, Batch 1130, Loss: 1.1337013244628906\n",
      "Epoch 9, Batch 1140, Loss: 1.198140263557434\n",
      "Epoch 9, Batch 1150, Loss: 1.1778500080108643\n",
      "Epoch 9, Batch 1160, Loss: 1.083526611328125\n",
      "Epoch 9, Batch 1170, Loss: 1.0333672761917114\n",
      "Epoch 9, Batch 1180, Loss: 1.0669139623641968\n",
      "Epoch 9, Batch 1190, Loss: 1.1624553203582764\n",
      "Epoch 9, Batch 1200, Loss: 1.2328041791915894\n",
      "Epoch 9, Batch 1210, Loss: 1.2872673273086548\n",
      "Epoch 9, Batch 1220, Loss: 1.2446027994155884\n",
      "Epoch 9, Batch 1230, Loss: 1.1245197057724\n",
      "Epoch 9, Batch 1240, Loss: 1.404744267463684\n",
      "Epoch 9, Batch 1250, Loss: 1.1283292770385742\n",
      "Epoch 9, Batch 1260, Loss: 1.1565494537353516\n",
      "Epoch 9, Batch 1270, Loss: 1.2497600317001343\n",
      "Epoch 9, Batch 1280, Loss: 1.4007680416107178\n",
      "Epoch 9, Batch 1290, Loss: 1.303971290588379\n",
      "Epoch 9, Batch 1300, Loss: 1.0777095556259155\n",
      "Epoch 9, Batch 1310, Loss: 1.2014753818511963\n",
      "Epoch 9, Batch 1320, Loss: 1.1325243711471558\n",
      "Epoch 9, Batch 1330, Loss: 1.1243432760238647\n",
      "Epoch 9, Batch 1340, Loss: 1.0797728300094604\n",
      "Epoch 9, Batch 1350, Loss: 1.227258324623108\n",
      "Epoch 9, Batch 1360, Loss: 1.1053491830825806\n",
      "Epoch 9, Batch 1370, Loss: 1.1226619482040405\n",
      "Epoch 9, Batch 1380, Loss: 1.1314386129379272\n",
      "Epoch 9, Batch 1390, Loss: 1.1237598657608032\n",
      "Epoch 9, Batch 1400, Loss: 1.3843286037445068\n",
      "Epoch 9, Train Loss: 1.194220441872123, Validation Accuracy: 56.917233106757294%\n",
      "Best Validation Accuracy: 56.93722510995602%\n",
      "Test Accuracy: 65.0974930362117%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from medmnist import PathMNIST\n",
    "import numpy as np\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_model, out_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = self.get_resnet(base_model)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, out_dim)\n",
    "        )\n",
    "\n",
    "    def get_resnet(self, base_model):\n",
    "        model = models.__dict__[base_model](weights=None)\n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        return model\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        z = self.projector(h)\n",
    "        return h, z\n",
    "\n",
    "model = SimCLR(base_model='resnet18', out_dim=128).cuda()\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load('./simclr_pathmnist_pretrained_epoch_50.pth'))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, base_encoder):\n",
    "        super(LinearProbe, self).__init__()\n",
    "        self.encoder = base_encoder\n",
    "        self.classifier = nn.Linear(512, 9)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            h = self.encoder(x)\n",
    "        h = h.squeeze()\n",
    "        logits = self.classifier(h)\n",
    "        return logits\n",
    "\n",
    "linear_probe_model = LinearProbe(model.encoder).cuda()\n",
    "\n",
    "PathMNIST_MEAN = [0.73765225, 0.53090023, 0.70307171]\n",
    "PathMNIST_STD = [0.12319908, 0.17607205, 0.12394462]\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=PathMNIST_MEAN, std=PathMNIST_STD)\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = PathMNIST(split='train', transform=data_transform, download=True, size=64)\n",
    "val_dataset = PathMNIST(split='val', transform=data_transform, download=True, size=64)\n",
    "test_dataset = PathMNIST(split='test', transform=data_transform, download=True, size=64)\n",
    "\n",
    "\n",
    "num_samples = len(train_dataset)\n",
    "subset_indices = list(range(num_samples))\n",
    "np.random.shuffle(subset_indices)\n",
    "subset_indices = subset_indices[:int(1 * num_samples)]\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_probe_model.classifier.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "def train_linear_probe(model, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.cuda(), labels.squeeze().long().cuda()  \n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.squeeze().long().cuda()  \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_accuracy = 0\n",
    "best_model_state = None\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_linear_probe(linear_probe_model, train_loader, criterion, optimizer, epoch)\n",
    "    val_accuracy = evaluate(linear_probe_model, val_loader)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Accuracy: {val_accuracy}%')\n",
    "    \n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model_state = linear_probe_model.state_dict()\n",
    "\n",
    "linear_probe_model.load_state_dict(best_model_state)\n",
    "\n",
    "test_accuracy = evaluate(linear_probe_model, test_loader)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy}%')\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
